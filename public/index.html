<!DOCTYPE  html>
<html>
  <head>
    <meta charset="utf-8">
    
    <title>doc</title>
    <style>
      /*github.com style (c) Vasily Polovnyov <vast@whiteants.net>*/
      pre code {
        display: block; padding: 0.5em;
        color: #333;
        background: #f8f8ff
      }
      pre .comment,
      pre .template_comment,
      pre .diff .header,
      pre .javadoc {
        color: #998;
        font-style: italic
      }
      pre .keyword,
      pre .css .rule .keyword,
      pre .winutils,
      pre .javascript .title,
      pre .nginx .title,
      pre .subst,
      pre .request,
      pre .status {
        color: #333;
        font-weight: bold
      }
      pre .number,
      pre .hexcolor,
      pre .ruby .constant {
        color: #099;
      }
      pre .string,
      pre .tag .value,
      pre .phpdoc,
      pre .tex .formula {
        color: #d14
      }
      pre .title,
      pre .id {
        color: #900;
        font-weight: bold
      }
      pre .javascript .title,
      pre .lisp .title,
      pre .clojure .title,
      pre .subst {
        font-weight: normal
      }
      pre .class .title,
      pre .haskell .type,
      pre .vhdl .literal,
      pre .tex .command {
        color: #458;
        font-weight: bold
      }
      pre .tag,
      pre .tag .title,
      pre .rules .property,
      pre .django .tag .keyword {
        color: #000080;
        font-weight: normal
      }
      pre .attribute,
      pre .variable,
      pre .lisp .body {
        color: #008080
      }
      pre .regexp {
        color: #009926
      }
      pre .class {
        color: #458;
        font-weight: bold
      }
      pre .symbol,
      pre .ruby .symbol .string,
      pre .lisp .keyword,
      pre .tex .special,
      pre .prompt {
        color: #990073
      }
      pre .built_in,
      pre .lisp .title,
      pre .clojure .built_in {
        color: #0086b3
      }
      pre .preprocessor,
      pre .pi,
      pre .doctype,
      pre .shebang,
      pre .cdata {
        color: #999;
        font-weight: bold
      }
      pre .deletion {
        background: #fdd
      }
      pre .addition {
        background: #dfd
      }
      pre .diff .change {
        background: #0086b3
      }
      pre .chunk {
        color: #aaa
      }
    </style>
  </head>
  <body>  
    <h1 id="title">Title</h1>
<h2 id="by-alasdair-smith">by Alasdair Smith</h2>
<p>Submitted in partial fulfilment of the requirements for the award of degree of Your Degree of the University of Portsmouth</p>
<p>April 2014</p>
<h3 id="copyright">Copyright</h3>
<p>Copyright &copy; Alasdair Smith. All rights reserved.
The copyright of this thesis rests with the Author. Copies (by any means) either in full, or of extracts, may not be made without prior written consent from the Author.</p>
<h3 id="preface">Preface</h3>
<p>If you have people to thank, do so on this page, otherwise, delete it.<br></p>
<h3 id="table-of-contents">Table of contents</h3>
<!-- Adjust these page numbers as necessary -->

<p>Copyright            i
Preface                ii
Table of Contents    iii
List of Tables        vi
List of Figures        v
Nomenclature        vi
Abstract            vii</p>
<p><strong>Chaper 1 Introduction...........................................................1</strong></p>
<p>1.1 A Major Section...............................................................1
1.1.1 A Major Subsection..........................................................1
1.2 Another Major Subsection......................................................1
1.3 Report Outline................................................................1</p>
<p><strong>Chapter 2 A Review of your subject and topics...................................2</strong></p>
<p>2.1 Foo...........................................................................2
2.2 Bar...........................................................................2
2.3 Chapter Summary...............................................................2</p>
<p><strong>Chapter 3 Artefact Design.......................................................3</strong></p>
<p>3.1 Introduction..................................................................3
3.2 Methodology...................................................................3
3.3 Requirements..................................................................3
3.4 Proposed Solution.............................................................3
3.5 Summary.......................................................................3</p>
<p><strong>Chapter 4 System Implementation................................................4</strong></p>
<p>4.1 Introduction.................................................................4
4.2 Foo..........................................................................4
4.3 Interesting Problems.........................................................4
4.4 Summary......................................................................4</p>
<p><strong>Chapter 5 Testing and Evaluation...............................................5</strong></p>
<p>5.1 Introduction.................................................................5
5.2 Testing Summary..............................................................5
5.3 Evaluation...................................................................5
5.3.1 Requirements Review........................................................5
5.3.2 Artefact Review............................................................5
5.4 Testing and Evaluation Summary...............................................5</p>
<p><strong>Chapter 6 Conclusion...........................................................6</strong></p>
<p>6.1 Introduction.................................................................6
6.2 Summary......................................................................6
6.3 Conclusions..................................................................6
6.3.1 Key Points.................................................................6</p>
<p><strong>Chapter 7 References...........................................................7</strong></p>
<p><strong>Appendices</strong></p>
<p>Something interesting
Something else<br></p>
<h3 id="list-of-tables">List of Tables</h3>
<p>Table 1: An interesting table</p>
<h3 id="list-of-figures">List of Figures</h3>
<!-- Adjust page numbers as necessary -->
Figure 1: An example diagram that shows four connected things        1

### Nomenclature

A number of key terms are used throughout this document and are defined here:

<!-- A nomenclature section can be useful way of defining common terms without having to include a lot of bracketed terms in your document. If you prefer to introduce terms when they are first used, remove this section -->

<h3 id="abstract">Abstract</h3>
<!-- The abstract should be the last thing you write, it is a summary of the introduction that is only a few sentences long. At most it should be a couple of paragraphs. It must be very succinct. --><br>
### Chapter 1 Introduction

This chapter introduces the overarching themes of this report and places the motivation for the work in context. Thereafter, the rationale and goals defined for the investigation of the project are discussed. followed by a summary of the overall project. Finally, an overview of the dissertation is given on a per-chapter basis.

<!-- Your words begin here at the high level, then dive into detail in sections and subsections... --><br>
#### 1.1 A Major Section

<!-- Your words go here -->

<h5 id="1-1-1-a-major-subsection">1.1.1 A Major Subsection</h5>
<!-- Your words go here -->

<h6 id="a-minor-subsection">A Minor Subsection</h6>
<!-- Your words go here-->

<p><img src="../../src/img/figure1.png" alt="Figure 1: An example diagram that shows four connected things"><br></p>
<h4 id="1-2-another-major-section">1.2 Another Major Section</h4>
<p>This section shows how the numbering works<br></p>
<h3 id="chapter-2-a-review-of-altmetrics">Chapter 2 A Review of Altmetrics</h3>
<p>This chapter discusses the state-of-the-art of ... considered during the analysis and design phase of this project. The investigation served x purposes: firstly, we wished to identify ... (see section 2.1); and secondly we wished to establish ... (see section 2.2). Finally, section 2.3 summarises the chapter.<br></p>
<h4 id="2-1-history-of-bibliometrics">2.1 History of Bibliometrics</h4>
<h5 id="2-1-1-introduction-to-bibliometrics">2.1.1 Introduction to Bibliometrics</h5>
<p>The field of bibliometrics, sometimes scientometrics, has existed for many years and has created a set of methods to quantitatively analyse scientific and technological literature (De Bellis, 2009). These metrics are most commonly used to measure the impact, or value, of the research in question. Fenner (2013) defines impact as follows: &quot;The scientific impact of a particular piece of research is reflected in how this work is taken up by the scientific community&quot;. This impact ranking has a diverse set of applications, including assessment of an author&#39;s work. Godin &amp; Dore (2004) find that there is &quot;there is huge demand for quantitative studies and indicators on the impact of science&quot;, with most aimed at the economic impact of research. They state that research has, and continues to be, funded on the basis that it has outcomes in society. One of the most widely used methods is the Impact Factor, sometimes known as the Journal Impact Factor.  It was first proposed by Eugene Garfield in 1972, in his paper, Citation Analysis As A Tool in Journal Evaluation. The impact factor is calculated using the following algorithm:</p>
<p><img src="../../src/img/figure2-1.png" alt="Impact Factor algorithm"></p>
<p>The Journal Citation Reports is published annually by Thomson Reuters, listing all known journals and giving their impact factor, and other metrics for the current year. Neylon and Wu (2009) state that &quot;most scientists ... will point to the Thomson ISI Journal Impact Factor as an external and &#39;objective&#39; measure for ranking the impact of specific journals and the individual articles within them&quot;.</p>
<p>Usage of the impact factor in ranking research other than journals has become more widespread. Increasingly, impact factor has become a proxy for measuring many diverse research outputs. These range from comparisons of international impact to individual article value (Fuyuno &amp; Cyranoski, 2006) (Arnold &amp; Fowler, 2010) (Garfield, 2006). Article value is calculated by proxy, by simply taking the impact factor from the journal it was published in. This has lead to ranking author value, by totalling the impact factor of each paper published.</p>
<p>Because of it&#39;s wide-ranging use, the impact factor has a strong influence on the scientific community. This has affected decisions on where to publish, whom to promote or hire, the success of grant applications, library decisions to purchase and renew journal subscriptions, researchers deciding where to publish, researchers choice on what to read and even salary bonuses (Van Epps &amp; Hill, 2007) (Arnold &amp; Fowler, 2010). The Public Library of Science (PLOS) Medicine editors (2006) report that &quot;in some countries, government funding of entire institutions is dependent on the number of publications in journals with high impact factors&quot;.</p>
<p>In the UK, governmental assessment of Higher Education institutions have been conducted by the Research Assessment Exercise (RAE) since 1986. The exercise relied on the &quot;subjective assessment of scientific publications by a panel of experts&quot;. Because of this, the RAE was time-consuming and expensive, costing the UK Government £12 million and universities an additional £47 million (Eyre-Walker &amp; Stoletzki, 2013). In 2014, the RAE will be replaced by the Research Excellence Framework (REF). The REF will controversially provide more focus research impact, with 25% of the final grading going towards measurement of value (Shepherd, 2009). Allen, Jones and Dolby (2009) believe that it is the impact factor&#39;s place as the key indicator of research progression that provides much of the rationale for the move to a more metrics-based successor.</p>
<ul>
<li>h-index &amp; other metrics (?)</li>
<li>Forms of impact (?)</li>
<li>Define &quot;research output&quot;<ul>
<li>Articles/papers</li>
<li>Blogs/Tweets</li>
</ul>
</li>
</ul>
<h4 id="2-1-2-failings-of-traditional-metrics">2.1.2 Failings of Traditional Metrics</h4>
<p>The impact factor has been the subject of much criticism, with many papers reporting on its faults. Increasingly, scientists have been calling for an end to the wide-ranging use of the impact factor. In December 2012, researchers at the annual meeting of the American Society for Cell Biology signed the Declaration on Research Assessment, calling for the end of the use of journal metrics to assess individual articles or authors.</p>
<p>Arnold &amp; Fowler (2010) report that &quot;the allure of the impact factor as a single, readily available number - not requiring complex judgments or expert input, but purporting to represent journal quality - has proven irresistible to many&quot;. It is this inherent simplicity that lead to the rise of the impact factor. As we shall see, this simplicity fails to interpret the scope and complexity of scientific impact.</p>
<p>As explained above, the impact factor has been used as a proxy for other forms of impact - primarily article or author impact. This is problematic, as the impact factor was simply not designed to measure anything other than journal impact (Neylon &amp; Wu, 2009). King &amp; Tenopir (2004) find that only about 15 - 20% of scientists in the United States have authored a refereed article, further supporting the view that journal impact is not representative of author impact. Even Garfield, who proposed the original impact factor, has criticised this usage. In his paper, How to Use Citation Analysis for Faculty Evaluations, and When Is It Relevant? (1983), he states that citation analysis can augment author assessment, but find that it is easily misinterpreted or inadvertently manipulated. Additionally, the current publishers of the impact factor, Thomson Reuters have admitted that it is being used in &quot;many inappropriate ways&quot; (The PLOS Medicine editors, 2006).</p>
<p>Scientific publishing has grown to an incredible rate, with over 800 000 papers published in 2008 in PubMed alone (Neylon &amp; Wu, 2009). This has grown to nearly 950 000 papers in 2013 (&quot;2013 PubMed Search Results&quot;, n.d.). Neylon and Wu (2009) claim this growth has overwhelming for researchers - &quot;It [is] impossible for any scientist to read every paper relevant to their research, and a difficult choice has to be made about which papers to read&quot;. Renear and Palmer (2009) find that scientists read 50% more papers than they did in the 1970s, spending less time on average with each one. Mendeley, a research paper bookmarking service found that researchers spent an average of 1:12 hours per day studying literature (&quot;Global Research Report&quot;, 2009). Choices about where researcher&#39;s time reading papers now must be made, leading to a greater need for filtering the mass of papers. Traditional forms of filtering, namely, peer review and the impact factor, are overwhelmed by the scale of modern research (Priem, Taraborelli, Groth &amp; Neylon; 2010).</p>
<p>The impact factor relies on citations as it&#39;s base measurement, which take time to accumulate. The average paper is not cited for months, at the earliest, but more often until 1 - 2 years after publication (Neylon &amp; Wu, 2009) (Priem, Piwowar, Hemminger, 2012). Potential impact, therefore, takes a long time to accumulate, especially relative to the rapid pace of development. This means that decisions using citation analysis data made soon after publication, may be incomplete, affecting career decisions, decisions on where to publish, funding decisions, and many other decisions. Additionally, real-time ranking of output becomes very difficult, preventing possible filtering applications.</p>
<ul>
<li>Also affects other citation-based metrics (h-index?)</li>
</ul>
<p>Researchers have also pointed out the underlying mathematical issues with the impact factors. In a Journal of Cell Biology Editorial, Rossner, Van Epps and Hill (2007) report that 89% of Nature&#39;s citations are attributable to only 25% of papers. Fundamentally, the impact factor is a mean, so it can be &quot;badly skewed by a &#39;blockbuster&#39; paper&quot;.</p>
<ul>
<li>Only 15% to 20% of scientists in the US have authored a refereed article</li>
</ul>
<p>Allen, Jones &amp; Dolby (2009) compared expert reviews of research articles to the impact factor, and other citation-based metrics. They found that the expert&#39;s score was more strongly correlated to the impact factor than to the number of citations the paper had received. They believe this was a consequence of experts rating papers in high profile (and high impact) journals more highly, rather than an ability of experts to judge the intrinsic metric or likely impact of a paper. This presents the current metric system with a &quot;rich get richer&quot; problem - papers published in journals with existing high impact factors are perceived to be &quot;better&quot;, leading to further high impact factors. This means it is difficult for papers in new journals to break into the system. A metric that considers more diverse communication methods might address this issue.</p>
<p>Measurement of scientific value have attempted to find metrics that measure scientific value outside of traditional scholarly communication for some time (Priem, Piwowar, Hemminger, 2012). The impact factor only measures impact within the scientific community, through it&#39;s sole use of citations. Any attempt by researchers to provide outreach to the public is ignored by the impact factor (The PLOS Medicine editors, 2006). Citation is evolving, and the published scientific article is not always the primary method of communicating scientific thinking (Anderson, 2009). In the current research climate, where the impact factor has power over career paths, outreach is discouraged. The PLOS Medicine editors (2006) write that &quot;[For a journal] which strives to make ... open-access content reach the widest possible audience ... impact factor is a poor measure of overall impact&quot;. They believe that by reaching a wider audience, they can help to set agendas - by publishing policy papers or highlighting neglected issues.</p>
<p>Other areas of scholarly work are also overlooked by the impact factor. The published paper is not necessarily the total output of a researcher, although it is often used to summarise. The Altmetrics manifesto authors (2010) report that there are &quot;new forms [of communication that] reflect and transmit scholarly impact&quot;. Additional work might include data sets, or code, that may go on to be reused by further research, increasing it&#39;s impact (Piwowar &amp; Vision, 2013). Priem &amp; Light Costello (2011) believe that sharing information (in all forms) is a &quot;central component of [researchers] work&quot;. However this potential impact will be not be captured by the impact factor.</p>
<p>Additionally, some forms of scholarly communication that were rarely recorded before are now becoming commonplace in digital formats (Groth &amp; Gurney, 2010) (Priem &amp; Light Costello, 2010) (Zhao and Rosson, 2009) (Letierce, Passant, Decker et al., 2009) (Shema, Bar-Ilan &amp; Thelwall; 2012). Researchers who collaborate well, or can explain concepts well have unpublished, but still valuable impact. Priem, Piwowar &amp; Hemminger (2012) call this &quot;scientific street cred&quot;, the informal and sometimes unintentional credit that scholars receive from peers that is moving online, where it can be tracked and quantified. Again, the impact factor does not take this hidden impact into consideration, by it&#39;s citation-based nature.</p>
<p>Authors have criticised the impact factor&#39;s simplicity, in that it is merely a straight count of citations, and does not take the effect of a network of researchers into account. Bollen, Rodriquez &amp; Van de Sompel (2006) write that it &quot;only represents the popularity factor of status, not its prestige factor&quot;, where popularity is defined as the total number of endorsements, and prestige is the relative prestige of endorsing actors. In their proposed system, a citation from a paper with higher prestige would be weighted more than a citation from a paper with lower prestige. This system is similar to Google&#39;s very successful PageRank algorithm, which allowed large scale ranking of web pages to produce accurate and relevant search engine. Bollen, Rodriquez &amp; Van de Sompel (2006) find that their PageRank-like algorithm &quot;strongly-overlapped&quot; with the impact factor, but it &quot;revealed significant and meaningful discrepancies&quot;. The simplicity of the impact factor does not allow for the concept of network effects, reducing it&#39;s usefulness as a tool for filtering.</p>
<p>The Institute of Scientific Information, founded by Garfield in 1960, was acquired by Thomson Reuters in 1992 (&quot;Thomson Corporation acquired ISI&quot;, 1992), handing over control of the impact factor. At no point have the key details of the impact factor calculation been made public, as they remain proprietary to Thomson Reuters. Some have accused the corporation of misidentifying article types, something that significantly affect the impact factor. The calculation is dependent on which article types are deemed as &quot;citable&quot; - the fewer, the higher the impact factor. Thomson Reuters has not published it&#39;s process of choosing &quot;citable&quot; article types, and The PLOS Medicine editors (2006) report that &quot;it became clear that the process of determining a journal&#39;s impact factor is unscientific and arbitrary&quot;. The editors of the Journal of Cell Biology in a separate report found &quot;numerous incorrect article-type designations&quot; in Thomson-Reuters&#39; data. They also found that the number of citations used in the impact factor was &quot;substantially fewer&quot; than the number published on the <em>Journal Citation Reports</em> website. They attempted to buy data that is used to calculate the impact factor and were refused by Thomson Reuters. They concluded that scientists would reject an article without primary data, so the impact factor should be rejected (Rossner, Van Epps &amp; Hill, 2007) (Galligan &amp; Dyas-Correia, 2013). The impact factor is not reproducible, a damning report of a metric that is supposed to measure scientific information, and that may affect potential career decisions.</p>
<p>Research papers are sometimes retracted because of errors, however these are still counted towards the impact factor. Liu (2007) points to the infamous Hwang stem cell papers that were retracted after it was discovered that they were fabricated. The retracted papers were cited 377 times, and accounted for 39% of Science&#39;s impact factor in 2004 and 2005. Liu also reports that high impact factor journals are based on &quot;useless or misleading citations&quot;. Impact gained from retracted papers does not reflect accurately on the true value, and should not be included in the impact factor.</p>
<p>The impact factor suffers from problems with gaming - where publishers or authors attempt to artificially raise their impact factor. The main approach to this is to garner as many citations as possible, even if this does not accurately reflect on the true number of citations a journal may have received. Certain types of article are used to achieve this, in particular, review articles. These articles that review a journal&#39;s output, by their very nature tend to cite the journal many times (Rossner, Van Epps &amp; Hill, 2007). When indexed by the Journal Citation Reports, journal editors way negotiate with Thomson Reuters to have whole article types removed, thereby potentially increasing their impact factor. One strategy for doing this reducing the number of articles of a certain type, a strategy that cannot be be viewed as good for a healthy science community (The PLOS Medicine Editors, 2006). Arnold &amp; Fowler (2010) also report on another worrying trend from authors of manuscripts under review, who were asked or required by editors to cite other papers from the journal. They describe this practice as &quot;bordering on extortion&quot;.</p>
<p>A core aim of bibliometrics is to find the best scientific articles, however the impact factor can encourage poor scientific practices. The impact factor provides no incentive to publish negative results, considered a good practice for advancing science. Buschman &amp; Michalek (2013) comment that the impact factor &quot;discourages publishing research with negative results&quot;, as negative result papers receive fewer citations, and therefore lower impact factor. They also find that blogs are increasingly being used to document negative results, however this valuable work is not captured by the impact factor.</p>
<p>Some newer approaches to assessing impact, such as the webometric movement are fundamentally limited by terms of use restrictions. Text mining or scraping web sites to count citations to scholarly work is a potentially useful method for measuring impact, however these approaches are prevented by academic publisher&#39;s terms of use. These terms prevent automated text mining, so instead indexes must be build manually, which is not a scalable approach (Priem, Piwowar &amp; Hemminger, 2012).</p>
<ul>
<li>What about alternatives to the JIF - i.e. using the same citation count method, but not controlled by Thomson Reuters<ul>
<li>Not possible because restricted by text-mining/scraping licenses</li>
</ul>
</li>
</ul>
<p>Roemer &amp; Borchardt (2013) note that &quot;it has frequently been noted by both librarians and information scientists that researchers in STEM disciplines tend to emphasize the production and consumption of journal articles more heavily than scholars in the humanities or social sciences&quot;. This has a huge effect on the less article-heavy disciplines, like the humanities, as the impact factor only measures article citations. By de-emphasising this aspect of scholarly communication, the impact factor will be subsequently lower, which is potentially damaging especially in the current climate of measuring all academics using the impact factor. For example, in the UK, the upcoming REF will take impact measurement into account and these disciplines may lose out.</p>
<p><br></p>
<h4 id="2-2-what-is-altmetrics-">2.2 What is Altmetrics?</h4>
<p>As alternative to traditional bibliometrics, the concept of altmetrics was developed in the late 2000s. Although there are many different definitions, altmetrics as a term has evolved over the years to an umbrella term for metrics that measure impact of web-based scholarly communications both qualitatively and quantitatively (Priem, Taraborelli, Groth &amp; Neylon, 2010) (Liu, 2003) (Howard, 2012). Altmetrics, in contrast to the journal impact factor, are primarily measured at the article level, although it has been suggested by some that altmetrics should also measure impact outside of the traditional article (Neylon &amp; Wu, 2009) (Adie, 2013). Article level metrics is a term favoured initially by those looking for an alternative to the impact factor. However article level metrics have started to be incorporated under the banner of altmetrics (Priem, 2010). These metrics attempt to find the &quot;real world impact&quot; of research, measuring sources outside of traditional science, giving a more encompassing view of impact.</p>
<p>Metrics that have been included in altmetrics in the past include article views and downloads, scholarly tweets, bookmarks on bookmarking services like Delicious, saves on social referencing services like Mendeley and even traditional citations (Priem, Piwowar &amp; Hemminger, 2012). There is a consensus that altmetrics are centre around mining web-based resources (Howard, 2012). There exists no definitive list of metric sources, and some disagree on the validity of specifics, but there is some movement towards standardising metrics and how they are measured (Thelwall, Haustein, Lariviere et al, 2013) (Lin &amp; Fenner, 2013).</p>
<p>Kaitlin Thaney, Director of the Mozilla Science Lab, likened altmetrics to tracking &quot;a researcher&#39;s footprints in the community&quot; (Liu, 2003). A good metaphor for describing how altmetrics attempts to capture impact from a wide range of sources. It is this wide range of sources, and their relative ease of access that allows not only the measurement of scholarly communications, but also of the wider general public (Priem &amp; Hemminger, 2010). Although the webometrics measurements have mined the web for impact data before, they have not updated for the rise of the social web, or Web 2.0. This new form of interaction on the web, enables a much larger audience to access publishing tools on the web. Users are using these tools to chat, discuss and share interesting links, activities that could potentially generate impact for researchers by measuring how far their research is spread (Galligan, 2012). Priem &amp; Hemminger (2010) state that &quot;[the] emergence of &#39;Web 2.0&#39; presents a new window through which to view the impact of scholarship&quot;.</p>
<p>The web&#39;s usage by scholars to communicate has been growing, and looks to continue into the future (Priem, Taraborelli, Groth &amp; Neylon, 2010). This usage comes in many forms, with studies finding usage on social media services, Twitter and Facebook (Zhao and Rosson, 2009), on blogs (Groth &amp; Gurney, 2010), on bookmarking services, such as Delicious (Priem, Piwowar &amp; Hemminger, 2012), and social reference managers, such as Mendeley (Yan &amp; Gerstein, 2011). Fenner (2013) found that 93% of PLOS Biology research articles published since June 2012 have been discussed on Twitter, and 63% mentioned on Facebook. Procter, Williams, Stewart et al. (2010) find that usage of &quot;Web 2.0&quot; tools such as blogs, comments and wikis are used for scholarly purposes &quot;frequently&quot; by 13% of their sample and &quot;occasionally&quot; by 45% (Procter, Williams, Stewart et al; 2010).</p>
<p>A key feature of altmetrics is the diversity of data sources - as discussed, the impact factor&#39;s simplistic model does not adequately measure the complex system that is scientific impact (Fenner, 2013). There is no definitive list of data sources that altmetrics practitioners have standardised around, and different studies have used different data sources, and different implementations of retrieving the sources (Mulvany; 2013). Below is a list of data sources that previous studies have used.</p>
<h5 id="2-2-1-twitter">2.2.1 Twitter</h5>
<p>Twitter as a service has grown rapidly in recent years (Priem &amp; Hemminger, 2010). Several studies have looked at the scholarly usage of Twitter, finding that it has been used for a variety of academic purposes (Zhao &amp; Rosson, 2009) (Letierce, 2009) (Shuai, Pepe &amp; Bollen, 2012) (Eysenbach, 2011). Zhao &amp; Rosson (2009) found that 6% of sampled tweets contained a link to a paper, of which, 52% directly linked to a paper and 48% linked to a third-party which then linked to a paper.</p>
<p>Eysenbach (2011) studied Twitter usage after article publication, finding that &quot;the top 20% of the tweet authors accounted for 63.4% of all tweetations&quot;, which roughly follows the 80/20 rule. Finally, he found that tweets can predict highly cited articles within the first 3 days of article publication, and concludes that tweets could be used as an alternative to the impact factor.</p>
<p>Shuai, Pepe &amp; Bollen (2012) studied the usage of Twitter by scholars over time after a new paper has been released. They found that most Twitter mentions of a paper occur one day after publication, and that is the only day the article is mentioned on Twitter. Articles are quickly passed around with little in-depth discussion. They conclude that either early Twitter mentions drive greater download numbers, or alternatively, inherently higher quality articles generate high early Twitter mentions. Eysenbach (2011) found similar usage - a majority of tweets related to a paper (Eysenbach terms them &quot;tweetations&quot;) were sent on the day of article publication, and after 30 days, tweets would move into &quot;sporadic tweetation phase&quot;.</p>
<p>Desai, Shariff, Shariff et al. (2012) found that tweets related to an academic conference had more positive sentiment scores if they were classified as opinion tweets. They also found that &quot;a positive sentiment score is not a prerequisite for message amplification&quot;.</p>
<p>Zhao and Rosson (2009) interviewed scholarly Twitter users and found that they &quot;typically follow people both in and out of their particular subfields&quot;, which gives conversations a &quot;more interdisciplinary perspective&quot;. This broad scope of conversations is interesting to altmetric practitioners, as it affords an opportunity to measure more impact that is more wide ranging. Zhao and Rosson additionally found that participants &quot;emphasized that they saw citing on Twitter as part of a dynamic, ongoing conversation&quot;.</p>
<p>Access to Twitter data primarily comes from the Twitter API, which enables programmatic access to tweets. However, historically Twitter has struggled to provide complete access to all tweets, or tweets older than a few days. Twitter now says that it filters search results for &quot;quality Tweets and accounts&quot; (&quot;Twitter search rules and restrictions&quot;, n.d.). In addition, Twitter has begun rate limiting API requests to 450 per app per hour (&quot;GET search/tweets | Twitter Developers&quot;; n.d.).</p>
<p>Several applications have developed approaches to solve this. The third-party altmetrics service, Altmetric.com, creates it&#39;s own database of tweets by polling the Search API regularly (&quot;Sources of Attention - Twitter&quot;, n.d.), and saving tweets with links to scholarly papers. Another altmetrics service, ImpactStory, uses a third-party Twitter search analysis API called Topsy to retrieve tweets. Topsy enables not just a search for the number of times a link has been tweeted but a search for the number of times &quot;influential&quot; users have tweeted a link (&quot;Twitter Search, Monitoring, &amp; Analytics&quot;; n.d.). In addition, ImpactStory uses the regular Twitter API to retrieve statistics on the number of followers of a particular account (&quot;ImpactStory: FAQ&quot;; n.d.).</p>
<h5 id="2-2-2-social-networks">2.2.2 Social Networks</h5>
<p>Twitter is by far the most studied social network as a data source for altmetrics. However, other social networks have been included in altmetric studies, with Facebook, Reddit, Digg, StumbleUpon and Slashdot being examples (Priem &amp; Hemminger, 2010). Interaction with science in these online arenas is similar to that of Twitter - discussion around a link to a primary or secondary source. These take many forms; Facebook data consists of counts of clicks, likes, shares and comments of a link (Priem, Piwowar, Hemminger, 2012). Reddit shows some criteria that make it potentially valuable as a data source, with Sanderson &amp; Rigby (2013) reporting that Reddit&#39;s code of behaviour, or &quot;reddiquette&quot;, enforces linking to primary sources, and giving credit.</p>
<p>Impact from social networks is likely to be similar to that of Twitter&#39;s, with less relative usage from scholars. This reflects the wider usage of social networks, and represents the diverse impact potential of an article (Sanderson &amp; Rigby, 2013) (Zhao &amp; Rosson, 2009).</p>
<p>Less emphasis has been placed on study of these networks, with only a few articles including them. No articles focus entirely on studying the impact of a particular social network other than Twitter. This may be for a few reasons; as discussed, Twitter&#39;s usage among scholars is growing, so focus on that particular social network has been strong. Some of the social networks have seen declining usage recently, Digg reportedly dropping in visitors since 2010 (Metz, 2010). Additionally, gaining access to other social network&#39;s data is somewhat difficult, as sites like Slashdot do not offer programmatic access to data. RSS feeds or web pages would have to be scraped by a crawler.</p>
<h5 id="2-2-3-blogs">2.2.3 Blogs</h5>
<p>Blogging as a medium started in the late 1990s, but grew more popular in the mid-2000s, unlike most of the tools discussed here, it is not closely associated with a &quot;name-brand&quot; service, perhaps reflecting on the maturity of the medium. This maturity combined with the ease of publication that is possibly the reason for blogging&#39;s popularity among scholars, with Priem &amp; Hemminger (2010) commenting that the literature is much too large to review in its entirety. Scholars use blogs for a variety of reasons, including sharing content, expressing opinions and interacting with others both inside and outside of the author&#39;s discipline (Shema, Bar-Ilan &amp; Thelwall, 2012). Groth and Gurney (2010) argue that scholarly communication is not separated between articles and blogs, but intertwined, with blogs increasingly referring to traditional publications (Shema, Bar-Ilan &amp; Thelwall, 2012). Blogs are examples of participatory journalism, with scientific-focussed blogs addressing topics published in journals but also extending to the general public, often in a formal setting. Most posts describe the implications of science (Groth &amp; Gurney, 2010) (Priem &amp; Hemminger, 2010). This variety makes blogs useful to altmetrics practitioners, as they allow measurement of different forms of impact, both scholarly and non-scholarly.</p>
<p>The demographics of scholarly bloggers are skewed towards males affiliated with academic institutions, according to Shema, Bar-Ilan and Thelwall&#39;s (2012) sample. They found that 67% of the sample reporting to be male, 18% female, 5% male-male co-authors, 4% male-female co-authors, and 6% unknown. They also found 59% were either students or researchers in an academic institute, and less than a third were not affiliated with an institute. The study also looked at blogger&#39;s Twitter accounts, finding that 72% had active Twitter accounts, and they many followed fellow bloggers. They concluding that there is a core of quite well connected  scholarly bloggers who are &quot;information disseminators&quot;.</p>
<p>Unlike Twitter, Facebook and other social media tools, blogging&#39;s culture of linking to primary sources is strong, and is similar to academics culture of citations. This makes links on blogs a strong indicator of impact (Priem &amp; Hemminger, 2010). Blogs tend to reference &quot;high quality science&quot;, with 70.5% of the publications references were in journals with high impact factors (Groth &amp; Gurney, 2010).</p>
<p>Priem &amp; Hemminger (2010) report that mining blogs to spot trends has been an area of active research, with some blog trend detection services created (Glance et al; 2004) (Bansal &amp; Koudas, 2007). These systems scrape the blogosphere to spot emerging trends, which can then be mapped to scholarly impact (Priem &amp; Hemminger, 2010). In addition to text mining, subscriber numbers are used to gain insights on impact. The altmetrics tool, ImpactStory uses an API provided by Wordpress to extract the number of subscribers to a given blog. Using this method, impact of a scholar&#39;s blog can be determined.</p>
<p>As mentioned above, the blogging literature is too large and disparate to measure in it&#39;s entirety so many altmetrics tools use speciality blogging aggregators, such as ResearchBlogging.com, to take a sample of scholarly focussed blogs. These aggregators allow academic blogs to submit new posts and an editor ensures that posts follow guidelines and are of appropriate quality (Shema, Bar-Ilan &amp; Thelwall, 2012).</p>
<p>Groth &amp; Gurney (2010) report that mining blogs provide more immediate scientific discourse, similar to other altmetrics data sources. They also find that newer articles get more blog post page views, but older articles will be referenced and put into context by current articles (Groth &amp; Gurney, 2010).</p>
<h5 id="2-2-4-social-reference-managers">2.2.4 Social Reference Managers</h5>
<p>Social reference managers are a fairly new category, dominated by Mendeley, started in 2008. These services allow researchers to collect papers they find interesting and take notes on them, in an easily accessible place. This is similar to older services, such as Endnote, however they add the ability for a researcher to view a feed of the papers other researchers have saved (Gunn, 2013) (Procter, Williams Stewart et al, 2010). &quot;By broadcasting what papers they think are important, researchers are directly influencing the research community&#39;s choice of reading and discussion material&quot;. In addition, the researcher has the ability to add tags, comments or ratings when saving papers (Neylon &amp; Wu; 2009).</p>
<p>Mendeley is the most popular of these services, with Gunn (2013) reporting broad adoption in the life scientists, chemistry, maths and computer science. Mendeley stores 420 million documents, with half a million new documents added a day. A high percentage of recently released articles are represented in the Mendeley collection of documents (Gunn, 2013) (Priem, Piwowar &amp; Hemminger, 2012). Despite it&#39;s dominance, Mendeley is not the only service in this market, with some competitors such as Zotero (Priem &amp; Hemminger, 2010).</p>
<p>Mendeley offers a public API, or application programming interface, that allows altmetrics practitioners to directly access usage data. The API provides an endpoint for articles that returns the number of users who saved the article. Articles can be referencing using PubMed ID, arXiv ID, DOI ID, ISBN number or an ISSN. The API is updated approximately daily with the latest data (Gunn, 2013) (&quot;Mendeley API Documentation&quot;, n.d.).</p>
<p>As data from social reference managers is generated by scholars its value to altmetrics is high. Practitioners have a high degree of confidence that impact generated from social reference managers originates from trained scholars. In addition, tagging and rating metadata is valuable, providing more insight into scholarly impact. Neylon and Wu (2009) even speculate that reference managers might add future functionality to able to &quot;track the amount of time users spend viewing papers within their interface&quot;.</p>
<h5 id="2-2-5-social-bookmarking">2.2.5 Social Bookmarking</h5>
<p>Bookmarking services are very similar to reference managers, in that they allow users to save links and view other&#39;s saved links, perhaps in a feed of links. Both scholars and non-scholars use social bookmarking services. Bookmarking services also allow users to tag and comment on links, giving valuable metadata (Taraborelli, 2008) (Neylon &amp; Wu, 2009) (Priem &amp; Hemminger, 2010). The major difference between reference managers and bookmarking services is the likelihood that impact is from scholarly sources. Bookmarking services tend to be aimed at the general public, whereas reference managers are focussed on scholars. This difference has advantages for altmetrics practitioners, as it allows measurement of wider impact, distinct from the impact factor (Howard, 2012) (Galligan, 2012).</p>
<p>There are several social bookmarking services, including the widely used Delicious, and the more scholarly focussed CiteULike. Priem and Hemminger (2010) describe bookmarking as &quot;[maybe] the best–developed scholarly Web 2.0 application&quot;, although business focus in this area has dropped off recently.</p>
<h5 id="2-2-6-faculty-of-1000">2.2.6 Faculty of 1000</h5>
<p>The Faculty of 1000 (F1000), is a post-publication peer review and recommendation service, designed for biology and medicine researchers. Articles are recommended by &quot;hand-selected&quot; reviewers, the Faculty, and given a score, the F1000 Factor (&quot;What is F1000?: The recommendations&quot;, n.d.) (&quot;What is F1000?: F1000Prime factors and rankings&quot;, n.d.). Unfortunately, for altmetrics practitioners, F1000 does not plan to provide an API, meaning that F1000 scores have to be manually collected (Priem &amp; Hemminger, 2010).</p>
<p>F1000 has been rapidly growing as a service to provide recommendations (Wardle, 2010). Wets, Weedon &amp; Velterop (2003) report that F1000 was adopted by two-thirds of top institutions worldwide within 18 months. They argue that it provides a &quot;much needed &#39;qualitative&#39; addition to the tool-box&quot; of impact-assessors (Priem &amp; Hemminger, 2010).</p>
<p>Impact extracted from the F1000 is likely to be similar to that of social reference managers, like Mendeley, as the population of recommenders is scholarly. Therefore, articles recommended through the service are likely to have high influence among other scholars.</p>
<h5 id="2-2-7-article-views-and-downloads">2.2.7 Article Views and Downloads</h5>
<p>Out of the data sources discussed here, views and downloads are the most traditional. Lin &amp; Fenner (2013) define this data source as the &quot;activity of users accessing the article online&quot; (Lin &amp; Fenner, 2013). Several article publishers have begun to offer statistics on the number of times an article has been viewed or downloaded including all of the Public Library of Science (PLOS) journals and Nature journals (Patterson, 2009) (Baynes, 2012).</p>
<p>The link to impact is somewhat direct, the higher the download numbers, the higher number of people reading the paper, the higher the impact (Taraborelli, 2008). Yan &amp; Gerstein (2011) reporting that &quot;the spread of a paper will then be reflected at the level of web usage statistics&quot;. They also found that article views follow a &quot;long tail&quot; distribution with a rapid downfall of views in the first month, followed by a more gradual power law decay.</p>
<p>Several studied found that views/downloads had the highest numbers of all of the altmetric data sources, reflecting the ease viewing a paper. Fenner (2013) says that &quot;when readers first see an interesting article, their response is often to view or download it&quot;. However most pointed to the increased engagement of other data sources - it takes a more engaged reader to go to the effort of tweeting, bookmarking, saving or citing the paper. This implies a hierarchy of engagement with articles, something to consider when evaluating altmetric data.</p>
<p>The Social Science Research Network (SSRN) has been tracking per-article views and downloads - bringing impact measurement into non-STEM research, which has traditionally not been measured by the impact factor.</p>
<h5 id="2-2-8-mainstream-media">2.2.8 Mainstream Media</h5>
<p>Like some social networks, mentions in mainstream media are not always included as a data source in altmetrics studies. The mainstream media is usually defined as newspaper articles, usually online. There are two approaches to capturing impact on these sites: through the use of web scrapers or manual searches through a site. The third-party altmetrics service, Altmetric.com, maintains a manually curated list of RSS feeds which it then parses for instances with links to, or mentions of scholarly papers in the body of the news article (&quot;Sources of Attention - Mainstream News Outlets&quot;, n.d.).</p>
<p>The difficulty of obtaining data from mainstream media sites is a likely reason that most studies do not include this data source. In addition, correctly parsing for a citation is very difficult, as formats vary widely, a long standing problem for altmetrics. To get around this, Altmetric.com searches for important information such as journal or article title, which is then used as a basis for a literature search on PubMed. This technique is not 100% reliable, nor is it particularly timely. Impact can take some time for the article parser to find (&quot;Sources of Attention - Mainstream News Outlets&quot;, n.d.).</p>
<h5 id="2-2-9-categorisation-of-metrics">2.2.9 Categorisation of metrics</h5>
<p>Lin and Fenner (2013) describe the emerging altmetrics landscape as &quot;increasingly difficult to manage, understand, and navigate&quot;, due to the growth of altmetrics services. To combat this, there have been some discussions among altmetric scholars recently around classifying data sources into categories and the development of industry best standards. Mulvany (2013) describes on his blog the 2013 NISO Article Level Metrics (ALM) standardisation workshop, where creation of altmetric standards was discussed. He imagines a scenario where the users of altmetric tools would find it difficult to guarantee reliability of a particular tool, unless they &quot;transparently describe&quot; the method for gathering sources.</p>
<p>The goal of this classification is to create an ontology of altmetrics, to understand the intent behind the data source. Lin &amp; Fenner (2013) attempted this for PLOS&#39; ALM data, describing how they &quot;moved from an emphasis on the data source itself to the underlying activity captured by the data source&quot;. Based on Priem, Piwowar and Hemminger&#39;s (2012) work, ImpactStory, an altmetrics provider, breaks impact measurement down into 10 categories. This allows somewhat of a qualitative assessment of impact, along with the pure quantitative measurements (&quot;A new framework for altmetrics&quot;, 2012).</p>
<p>There is no universal agreement around categories, but in general the following are used by many altmetric studies (&quot;A new framework for altmetrics&quot;, 2012) (Lin &amp; Fenner, 2013) (Priem &amp; Hemminger, 2010).</p>
<ul>
<li>Views/downloads<ul>
<li>Activity of users accessing an article online</li>
<li>Example data sources:<ul>
<li>Article views and downloads</li>
</ul>
</li>
</ul>
</li>
<li>Saved/bookmarked<ul>
<li>Activity of a user saving an article link in an online bibliography manager</li>
<li>Example data sources:<ul>
<li>Social bookmarking services, such as Delicious</li>
<li>Social reference managers, such as Mendeley and CiteULike</li>
</ul>
</li>
</ul>
</li>
<li>Shared/Recommended<ul>
<li>Activity of a user endorsing the article</li>
<li>Example data sources:<ul>
<li>F1000</li>
<li>(Mainstream media)</li>
</ul>
</li>
</ul>
</li>
<li>Discussed<ul>
<li>Activity of a user discussing the article with peers</li>
<li>Example data sources:<ul>
<li>Blogs</li>
<li>Twitter</li>
<li>Facebook</li>
</ul>
</li>
</ul>
</li>
<li>Citation<ul>
<li>Activity of a user citing the article in a formal setting</li>
<li>Example data sources:<ul>
<li>Formal citation<ul>
<li>CrossRef</li>
<li>PMC</li>
<li>Web of Science</li>
<li>Scopus</li>
</ul>
</li>
<li>Wikipedia</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Areas of disagreement exist, for example, Priem &amp; Hemminger (2010) include social news sites such as Reddit and Digg in the shared/recommended category, whereas most other studies do not. This may be because the popularity of this market has waned recently. They also include Wikipedia, as do ImpactStory, although in separate categories.</p>
<p>A concept that is revealed through classification of altmetric data sources is a hierarchy of engagement. It is intuitive that formally citing an article shows more engagement with the work than sharing the article on Twitter, and in turn, more engagement than simply reading the article (Lin &amp; Fenner, 2013). Fenner (2013) finds that the average ratio of citations to article views is 1 to 300.</p>
<p>It could be argued that this higher engagement shows higher impact. However, some find that simply classifying impact generates value by putting it into context (&quot;ImpactStory: FAQ&quot;, n.d.). Additionally, by definition, the access to the tools that show higher levels of engagement - not everyone can write a research paper in which to cite. To combat this, ImpactStory splits it&#39;s categories into data sources that affect scholars and the public. They suggest &quot;categorizing metrics along two axis: engagement type and audience&quot; (&quot;A new framework for altmetrics&quot;, 2012).</p>
<p><img src="../../src/img/figure2-2.png" alt="Categorisation of altmetric sources by ImpactStory"></p>
<p>Similarly, Lin &amp; Fenner (2013) distinguish between primary metrics, &quot;the raw counts of activity captured by each source&quot;, and secondary metrics &quot;comprised of descriptive statistics that give context to the primary metrics&quot;, giving an example of average usage of similar papers.</p>
<h5 id="2-2-10-timeline-of-usage">2.2.10 Timeline of Usage</h5>
<p>Yan &amp; Gerstein (2011) find that usage of a scholarly article over time generally follows a &quot;long tail&quot; distribution, where &quot;on average, the older a paper is, the less attention it receives&quot;. The decline in usage drops rapidly in the first few months, after which the pace of decline slows. They further find that once a user has accessed an article, they may spread the information to others, again supporting the hierarchy of data sources model.</p>
<p>Wu &amp; Huberman (2007) find voting statistics on the social news website, Digg.com, follow a &quot;simple stochastic model&quot;, where the general dissemination of information is spread randomly amongst a user&#39;s contacts over time.</p>
<p>There are very few studies into the usage of altmetrics over time, other than to validate short term altmetric experiments against traditional citation rates. These generally attempt to compare the results for an altmetric data source, usually over a fairly short period of time, i.e. predictions using these data sources, against citation rates. This approach, however, gives little insight into how an altmetric data source changes over time, and how this affects the article&#39;s impact. This area of study may prove fertile as further research is required to understand the temporal aspect of altmetrics.</p>
<p><br></p>
<h4 id="2-3-the-case-for-altmetrics">2.3 The Case for Altmetrics</h4>
<p>There are a multitude of reasons for believing that altmetrics offer a viable option when measuring scholarly impact. The capability of altmetrics to measure more diverse forms of impact, with less delay, more transparently and at enormous scales is exciting for those looking to improve scholarly impact measurement. Priem &amp; Hemminger (2010) find that &quot;altmetrics take advantage of the pervasiveness and importance of new tools (Web pages, search engines, e–journals) to inform broader, faster, and more open metrics of impact&quot;. New data sources allow altmetrics practitioners to explore the underlying properties of an article to &quot;measure the distinct concept of social impact&quot; (Eysenbach; 2011).</p>
<h5 id="2-3-1-correlation-with-citations">2.3.1 Correlation with Citations</h5>
<p>Many altmetrics studies have focussed on correlating various data sources with traditional citations as a way of validating their results. Thelwall, Haustein, Lariviere et al. (2013) attempted to correlate 11 different data sources with Web of Science citation counts. They created a simple sign test whether an article&#39;s citations match altmetrics values - if both were higher than the average for articles published at a similar time then the data source was considered successful. They found &quot;clear evidence&quot; that data sources for blogs, Facebook and Twitter show strong correlation with citations - &quot;the success rate of the altmetrics at associating with higher citation significantly exceeded the failure rate at the individual article level&quot;. They found that an additional three data sources correlate with citations, although the correlation is weaker.</p>
<p>Another study found a &quot;moderately strong relationships between citation count and pdf/html download count&quot;, and that &quot;Mendeley and CiteULike displayed the highest correlations to Web of Science counts&quot;. From their original sample, they created a time-restricted sample of recent (2011) papers, finding that correlations between Mendeley and Web of Science citation count &quot;rivaled or surpassed those of Scopus, PubMed, and CrossRef citations&quot; (Priem, Piwowar, Hemminger, 2012).</p>
<p>Some studies focussed specifically on a single data source. Nielsen (2007) finds that the number of links to research papers in Wikipedia correlates with the citation count found in the Journal Citation Report (from which the impact factor is calculated). Similarly, Eysenbach (2011) finds a statistically significant, although weak, correlation between Twitter citations and traditional citation counts.</p>
<h5 id="2-3-2-capturing-the-unseen-conversations">2.3.2 Capturing the Unseen Conversations</h5>
<p>Altmetrics have the potential to capture previously hidden scholarly communications. Conversations between academics in the hallways at their institutions or at conferences are important methods for discussing new and interesting ideas. These discussions have huge value in determining the course of future science, and thus are valuable sources of impact. As academia increasingly moves online, it seems logical that these conversations will also move online. Altmetrics gives us the opportunity to measure this impact.</p>
<p>Priem, Piwowar &amp; Hemminger (2012) claim that &quot;[Blogging and Twitter] facilitate the sort of informal conference chats that have long vivified the academy’s invisible colleges&quot;. They also point out that this form of impact measurement &quot;[facilitates] existing practice&quot;, instead of forcing new behaviour.</p>
<h5 id="2-3-3-diversity">2.3.3 Diversity</h5>
<p>One of the biggest benefits of altmetrics is impact discovery from a much broader and diverse set of data sources. Fenner (2013) describes scientific impact as &quot;multi-dimensional construct that can not be adequately measured by any single indicator&quot;, supporting altmetric&#39;s model of many diverse data sources. By using a wider variety of sources, the impact reflects the real-world &quot;diverse scholarly ecosystem&quot; more accurately (&quot;altmetrics: a manifesto&quot;, 2010). Gibson (2013) examined the altmetric data sources used by the Journal of Ecology finding that &quot;each metric reflected a different form of reader usage&quot;.</p>
<p>As discussed in the History of Bibliometrics section, the Impact Factor fails to measure the general public&#39;s interaction with science. Altmetrics, due to their inherent diversity, allow us to measure this interaction. This can be achieved by data from sources that are used by people outside of science, such as social networks (Priem, Piwowar &amp; Hemminger, 2012). In addition, other under-represented or niche groups can be looked at to discover impact. Thelwell, Haustein &amp; Lariviere (2013) studied the validity of altmetrics using Twitter as a primary data source, by attempting to correlate Twitter metrics with citation counts. They comment that this may be limiting to the scope of altmetrics, where it can capture the &quot;influence of scholarly publications on a wider and different section of their readership than citation counts&quot;.</p>
<p>The benefits of this wider group extend to other areas of scholarship, such humanities and social science. As discussed in the History of Bibliometrics section, these fields miss out on the impact factor as they rely on alternative forms of publication to the journal. Altmetrics gives us the opportunity to fix this imbalance (Roemer &amp; Borchardt, 2013).</p>
<p>The range of altmetrics can be adapted to multiple different purposes and contexts (Neylon &amp; Wu, 2009), allowing us to view impact in novel ways. For example, Priem, Piwowar &amp; Hemminger (2012) propose a set of &quot;impact flavours&quot;, that can be used to describe the impact of papers. This concept and similar have evolved to the categories of data sources discussed in the previous section. Using these categorisations, and applying them to papers - i.e. papers that have relatively high metrics from one category are, by implication, similar to other papers with high metrics in the category - we can view the impact through the lens of these categories. This allows us to look at metrics more nuanced and encompassing way. As Priem, Piwowar &amp; Hemminger (2012) note, &quot;the goal is not to compare flavors: one flavor is not objectively better than another&quot;, which leads to the recognition that &quot;different types of contributions might help us appreciate scholarly products for the particular needs they meet&quot;.</p>
<p>Altmetrics can also be used to generate a more semantic view of influence, by looking at the entire system of sharing and re-use (&quot;altmetrics: a manifesto&quot;, 2010). Google&#39;s PageRank system famously ranks pages on the web by considering which other web pages link to the page in question. Links from trusted sites hold more weight, therefore a page containing a link from these sites is given a higher ranking. This concept can be applied to altmetrics - by considering who or what cites a scholarly article, a weighting can be applied to that citation that will be reflected the article&#39;s impact. Bollen, Van de Sompel, Hagberg et al. (2009) studied this concept using a &quot;clickstream&quot; model to investigate how scientific publications are accessed. A map of how users clicked to move through the system was collected, and modelled. They drew few conclusions on it&#39;s validity as a influence measure, although it was found that &quot;[there is a] distinction between citing behaviour and online information seeking behaviour&quot;, implying that altmetrics can discover impact that is unseen by traditional citation metrics.</p>
<h5 id="2-3-4-speed">2.3.4 Speed</h5>
<p>As discussed in the Failings of Traditional Metrics section, citation analysis can take several months if not years to accumulate. Altmetrics analysis, on the other hand, can gather data in &quot;days or weeks&quot; (&quot;altmetrics: a manifesto&quot;, 2010). By leveraging the power of open APIs, altmetric data sources can be queried immediately after a paper has been published (Chamberlain, 2013). For example, tweets usually occur very soon after publication, almost as the paper is read and shared by other academics. Priem &amp; Light Costello (2011) find that, for Twitter citations &quot;39% ... refer to articles less than one week old, and 15% of citing tweets refer to articles published that same day&quot;. Groth &amp; Gurney (2010) comment that one of blogging&#39;s &quot;major strengths&quot; is &quot;the ability to provide instantaneous commentary on a subject with simultaneous feedback on their own content&quot;. They also comment that blogs allow discussion of older papers, putting current papers into context and reintroducing older ideas.</p>
<p>This much shorter delay between publication and citation presents a great opportunity to altmetrics practitioners, enabling novel uses for altmetrics data. The Altmetrics manifesto (2010) claims that it allows the opportunity to develop &quot;real-time recommendation and collaborative filtering systems&quot;. This would add an additional layer to existing review systems within science, cutting the overwhelming and increasing volume of papers published yearly, and allowing researchers to focus on the most important papers in their field.</p>
<h5 id="2-3-5-transparency">2.3.5 Transparency</h5>
<p>Altmetrics foster a culture of openness that is missing from traditional bibliometrics, especially the impact factor. This approach has been adopted from the beginning, with the Altmetrics manifesto (2010) calling for openness in &quot;not just the data, but the scripts and algorithms that collect and interpret it&quot;. </p>
<p>Most, if not all, altmetric data sources have open or permissive licenses, allowing free re-use, usually with an API key. This is extremely beneficial to the measurement of scientific impact, as it allows the process to follow practices and values long held in science. Open data means that anyone can access and verify the results of a given altmetric system. Chamberlain (2013) concludes that &quot;if data sources are open, conclusions based on article-level metrics can be verified by others and tools can be built on top of the article-level metrics&quot;.</p>
<p>Some altmetric practitioners have extended this openness to their code. For example, the altmetric data gathering and analysis tool, ImpactStory, makes their entire code-base available under a MIT license. This allows anyone to use and extend the code without having to obtain a license. The methods by which they calculate altmetric scores can be viewed by anyone to verify that they are correct. This approach is very similar to the generalised scientific method of publishing your work openly for anyone to inspect and critique.</p>
<h5 id="2-3-6-captures-more-of-the-author-s-work">2.3.6 Captures More of the Author&#39;s Work</h5>
<p>As discussed in the Failings of Traditional Metrics section, increasingly a paper is not the only output from a research project. Altmetrics allows us to find influence in these other forms of output. ImpactStory recently started to track Github repositories to enable researchers to view altmetrics data for their code (&quot;Uncovering the impact of software&quot;, 2013). ImpactStory have additionally implemented features where open data hosted on the scientific data sharing platform, figshare, can be analysed for altmetrics data (&quot;figshare and altmetrics.&quot;, 2012).</p>
<h5 id="2-3-7-web-scale">2.3.7 Web Scale</h5>
<p>Due to the primarily web-based data sources, altmetrics operates at the very large web scale. Whereas traditional citations counts were rarely found above a hundred, some altmetrics can be regularly found on the order of thousands. As these numbers rise, the statistical likelyhood that they reflect real-world impact also rises. Any single altmetric citation may be uninteresting, or even erroneous, but the vast amount of data balances this reflecting the &quot;wisdom of the crowds&quot;, a concept detailed by Surowiecki (2005). Taraborelli (2008) comments that individually metadata is &quot;hardly of any interest, but at a large scale metrics based on these metadata are likely to outperform more traditional evaluation processes in terms of coverage, speed and efficiency&quot;.</p>
<p>Neylon &amp; Wu (2009) compare the large scale of altmetrics to the online advertising industry, stating that altmetrics &quot;may not be completely accurate but they are consistent, comparable, and considered sufficiently immune to cheating to be the basis for a billion dollar Web advertising industry&quot;.</p>
<h5 id="2-3-8-complimentary-to-traditional-metrics">2.3.8 Complimentary to Traditional Metrics</h5>
<p>Finally, it must be noted that altmetrics in no way replaces traditional forms of impact measurement. They can provide additional context or data when making decisions based on impact. Despite it&#39;s failings the impact factor still has value in finding impact within the scientific community. Altmetrics gives us the opportunity to extend this beyond science, for example boosting the impact profile of a research who spends more time performing public outreach through a blog or Twitter account.</p>
<p>McKiernan (2004) notes that usage-based metrics (for example, page views) are increasingly perceived as necessary to complement to traditional peer review as an indicator of scientific significance. This is supported by Priem, Piwowar &amp; Hemminger (2012) who speculate that in the future altmetrics and more traditional impact measurements could be presented together as &quot;complementary tools presenting a nuanced, multidimensional view of multiple research impacts at multiple time scales&quot;.</p>
<ul>
<li>Not relying on a single number<ul>
<li>More nuanced</li>
<li>Reflects the complex system of scientific publishing</li>
<li>&quot;With altmetrics, there is a sense that the users themselves should articulate how the measurements should be applied to specific problems, rather than dictated by an organization that thinks it knows best. This is the main drawback of all altmetric tools - there is as yet no simple way to interpret the data and give clear meaning&quot; (Altmetrics: Rethinking the Way We Measure; Galligan &amp; Dyas-Correia; 2013)</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="2-4-criticism-of-altmetrics">2.4 Criticism of Altmetrics</h4>
<p>Not all believe that altmetrics are a viable solution to the bibliometrics problem. They point to problems with data sources that can be misleading or to the ease with which altmetrics can be &quot;gamed&quot;. These need to be addressed by the altmetrics community before they are accepted by the wider scientific institution.</p>
<h5 id="2-4-1-inadequate-data-sources">2.4.1 Inadequate Data Sources</h5>
<p>Several papers have been published criticising specific altmetric data sources, claiming that they are inherently poor statistics for measuring impact. Taraborelli (2008), when studing usage metrics such as article views, states that &quot;it is debatable whether they will be able to overcome the major issues that afflicted search engine research over the last decade&quot;. These early search engines &quot;relied on raw traffic data&quot;, which is vulnerable to spam or &quot;gaming&quot;, where one attempts to boost views maliciously. Taraborelli draws similarities between scholarly article views and this raw traffic data. However, Taraborelli goes on to offer a potential solution, again drawing from the experience of early web search engines, where &quot;raw traffic data [was abandoned] in favour of more accurate, scalable and spam-resistant criteria for quality assessment&quot;. This points to an exciting new area of altmetrics that, to some extent, has gone unexplored - the usage of the entire network to rank influencers and thus weight individual citations. Neylon &amp; Wu (2009) also find that article views and downloads are a &quot;crude measure of actual use&quot;. They criticise the approach, stating that counting &quot;how many people clicked on the download button thinking they &#39;might read it later&#39;&quot; is not a measure of &quot;how much influence an article has&quot;.</p>
<p>Another data source that has received criticism is the social network, Twitter, with some finding that supposed scholarly tweets were in fact advertisements. Desai, Shariff &amp; Shariff (2012) studied tweets related to an academic conference on the subject of kidney disease, finding that &quot;a large percentage of tweets were advertisements&quot; for treatments. They conclude that the &quot;use of Twitter as a communication tool challenging as advertisers can misuse it under the disguise of education&quot;. However, Desai, Shariff &amp; Shariff also find that few conference attendees composed tweets in their sample.</p>
<p>Eysenbach (2011) finds that directly comparing Twitter citation rates (or, &quot;tweetations&quot;) for two unrelated articles is problematic. The paper finds that &quot;number of tweetations is a function of time since publication&quot;, and as such two papers published at different times would produce different Twitter citation rates. It also reports that a comparison of the &quot;twimpact factor&quot; of two articles &quot;would not be legitimate&quot;, giving an example of an article on social media gaining more Twitter citations than an article on molecular biology. However it could be countered that comparing two fields as in the example is rare and difficult even using more established tools. In addition, this argument does not cover the diversity of altmetrics, which may reflect the likelihood that the molecular biology paper will be cited more by scholars than the social media article.</p>
<h5 id="2-4-2-correlation-with-citations-disputed">2.4.2 Correlation with Citations Disputed</h5>
<p>Evidence that altmetrics correlates with traditional forms of impact measurement are disputed by some. Thelwall, Haustein, Lariviere et al. (2013) find that several data sources &quot;may only be useful to identify the occasional exceptional or above average article rather than as universal sources of evidence&quot;, and Priem, Piwowar &amp; Hemminger (2012) point to further studies that find &quot;weak to moderate correlation&quot; with social reference managers, and weak correlation with tweets.</p>
<p>However, Thelwall, Haustein, Lariviere et al. (2013) further find that the low correlation rate may be affected by the time of publication. Priem, Piwowar &amp; Hemminger (2012) also claim that &quot;altmetric indicators seem mostly orthogonal to citation&quot;, suggesting that altmetrics reflects a different form of impact to traditional citations. Eyre-Walker &amp; Stoletzki (2013) found poor correlation between citation counts and Faculty of 1000 (F1000) scores. They report that &quot;scientists are poor at estimating the merit of a scientific publication&quot;, in terms citation counts, finding articles with high F1000 scores that had low citation counts.</p>
<h5 id="2-4-3-low-coverage">2.4.3 Low Coverage</h5>
<p>Coverage, in this context, is the statistic that defines how well an article is represented by the altmetric data sources that cite it. For example, if an article has very few or no citations for a given data source, then it is considered to have low coverage. This affects the likelihood that the altmetric value accurately represents the true impact of the article. Studies have found that, for some papers and some data sources, coverage is low. Thelwall, Haustein, Lariviere et al (2013) report that, for their sample, &quot;the coverage of the altmetrics, and particularly those other than Twitter, may be low&quot;. They find many instances where articles had no altmetric data sources associated with them. Priem, Piwowar &amp; Hemminger (2012) find that &quot;scholarly use of social media [is] relatively rare&quot;. As discussed in the What Is Altmetrics? section, this is countered by the fact that scholar&#39;s usage of web-based tools is growing rapidly, likely increasing the coverage of many articles.</p>
<h5 id="2-4-4-bias">2.4.4 Bias</h5>
<p>Some have criticised altmetrics, claiming that altmetrics have some inherent bias. Priem &amp; Hemminger (2010) believe that &quot;users of social software probably skew younger, and from more technical and scientific disciplines&quot;. They further claim that a positive feedback loop can be created that is dangerous to measuring the true impact, where &quot;more popular items attract more attention, increasing their popularity still further&quot;. Others believe that altmetrics is too easily affected by &quot;trendy&quot; topics, that are popular in the moment (Eysenbach, 2011). </p>
<p>This younger bias also extends to papers, with more recent articles garnering more attention than older articles (Gunn, 2013). As discussed above Thelwall, Haustein, Lariviere et al. (2013) suggest that older articles are &quot;compensated for lower altmetric scores&quot;. Others find problems with altmetric&#39;s bias toward articles with high traditional citation metrics. Groth &amp; Gurney (2010) find that scholarly blogs focus on papers published in journals with high impact factor. Shema, Bar-Ilan &amp; Thelwall (2012) call this the &quot;rich-get-richer phenomenon&quot;, although they concede that this may be due to the influence of mainstream media leading academic bloggers to &quot;offer their own analysis and interpretation&quot; of popular stories. Eyre-Walker &amp; Stoletzki find a similar bias amongst F1000 reviewers, giving &quot;higher scores to papers in high impact factor journals, independent of merit&quot;. It is possible that altmetrics may be able to avoid this bias as more scholarly processes come online, and thus there is more low level &quot;scientific street cred&quot;, as discussed in The Case for Altmetrics section. </p>
<h5 id="2-4-5-gaming">2.4.5 Gaming</h5>
<p>&quot;Gaming&quot; of altmetrics is a source of much criticism, with critics claiming that it is easier to falsely boost an article&#39;s apparent impact maliciously, and thus cause problems for those assessing impact. Beall (2013) imagines a world where page views are &quot;shamelessly gamed&quot; by low wage workers hired to &quot;reload web pages thousands of times&quot;. Beall goes on to criticise analysis of Twitter, claiming that researchers will &quot;pay companies to add bogus followers to their social media accounts, and these bogus followers will like and share their articles, actions that will be counted as part of the metrics&quot;. A similar conclusion is levelled at Google Scholar citation counts - not traditionally included in altmetrics, but the principle applies - by Delgado López-Cózar, Robinson-García &amp; Torres-Salinas (2013). For this study, they published 6 fake papers to investigate whether they could &quot;game&quot; the Google Scholar citation counts. They find Google Scholar&#39;s main shortcoming is &quot;the ease with which they can be used to manipulate citation counting&quot;. However it could be argued that the publication of 6 fake papers without anyone noticing is a failing of the scientific system, rather than solely Google Scholar&#39;s.</p>
<p>Some in the altmetrics community have answered this criticism by looking to other fields vulnerable to gaming, and adopting their strategies for controlling gaming or limiting it&#39;s influence. Priem &amp; Hemminger (2010) report that &quot;history suggests that while gaming social metrics may not be solved, it can be controlled&quot;. They cite several sources from the search engine and social network fields related to reducing spam and bad actors within their systems. Ntoulas et al. (2006) reports that &quot;advertisers have assaulted Google search results with &#39;black–hat SEO&#39;&quot;, similar to impact gaming, and yet search results remain mostly free from these advertisers. Yardi et al (2009) report on their efforts to detect spam in a Twitter network, finding &quot;the existence of structural network differences between spam accounts and legitimate users&quot;. This suggests that by using the vast amount of data available to altmetrics practitioners, it is possible to detect and isolate gaming attempts within altmetrics.</p>
<h5 id="2-4-6-involving-the-public">2.4.6 Involving the Public</h5>
<p>There are some who have voiced concern that by capturing impact from sources that are not peer-reviewed, and are dominated by the general public, that scientific rigour will be lost. Taraborelli (2008) comments that social reference managers cannot offer the &quot;same guarantees&quot; as expert peer review, and &quot;are less immune to biases and manipulations&quot;. Beall (2013) claims that &quot;the general public lacks the credentials needed to judge or influence the impact of scientific work, and any metric that relies even a little bit on public input will prove invalid&quot;. Liu &amp; Adie (2013) claim that &quot;such concerns are understandable, especially when one examines some of the trending articles that have garnered extremely high scores of online attention&quot;, giving examples of articles that are &quot;humorous, unusual, or even fictitious in nature&quot;.</p>
<p>This argument misses a great benefit of altmetrics, the ability to discover alternative forms of impact, beyond pure science. If all public influence is rejected, science and it&#39;s impact would become insular, a move that has grave consequences for the relevance of science to the general public. Additionally, many altmetric practitioners believe that the data can be adjusted or weighted in favour of academic influence, protecting from the &quot;damaging&quot; impact of the public.</p>
<h5 id="2-4-7-disagreement-about-standardisation">2.4.7 Disagreement About Standardisation</h5>
<p>As discussed in the What is Altmetrics section, altmetric data sources can be categorised, giving a framework for standardisation of data sources. However, not all within the altmetrics field feel that standardisation is warranted currently. Mulvany (2013) notes during the 2013 NISO ALM workshop, finding that much of the &quot;discussion is whether this is the right time to [standardise]&quot;. He also notes that two of the people who voiced dissension against standardisation represented altmetric services, ImpactStory and Plumb Analytics. They believe that &quot;standardization would cause calcification&quot;, also citing that their customers had not asked for it.</p>
<p>Liu &amp; Adie (2013) also note that &quot;there has been no clear consensus on which data sources are most important to measure&quot;, meaning that some data sources and methods have not been &quot;systematically validated&quot;. The lack of this validation may have a large effect on the consistency of data between different altmetrics practitioners. Chamberlain (2013) notes that &quot;when similar data sources are collected by article-level metrics providers, ideally, there should be a way to [compare] data&quot;. A comparison can only take place if provenance of the data and methods used to calculate a metric for the data are provided by the practitioner. The only existing provider that does both is ImpactStory, by providing a <code>provenance_url</code> on each data source, and by publishing their code under an open license.</p>
<h5 id="2-4-8-technical-issues-with-data-sources">2.4.8 Technical Issues with Data Sources</h5>
<p>Chamberlain (2013), comments that &quot;Twitter data is notorious for not being persistent&quot;. Twitter&#39;s API is rate limited, it&#39;s search functionality is inadequate and it is difficult to retrieve tweets older than 30 days. He reports two solutions: &quot;either have to query the Twitter &#39;firehose&#39; constantly and store data, or go through a company like Topsy (which collects Twitter data and charges customers for access) to collect tweets&quot;. Existing altmetrics providers take both approaches. This makes Twitter data very difficult to track over long periods of time, and is one of the main reasons behind the push for standardisation, as described above. He also finds other data sources, such as Google Scholar &quot;totally inaccessible&quot;. Liu &amp; Adie (2013) find problems in tracking impact, finding that &quot;technical limitations currently prevent the tracking of certain sources, such as multimedia files&quot;.</p>
<p>Some existing altmetrics providers do not provide historic data - a breakdown of how metrics change over time. ImpactStory and Plum Analytics only provide metrics that show the current total for each data source. Altmetric.com provides publicly available historical data, while the PLoS ALM API provides full historical data, but only for some of their metrics. Chamberlain (2013) believes that &quot;as more [sources] are tracked, historical data will become expensive to store, so perhaps won’t be emphasized by article-level metrics providers&quot;. However, this would be detrimental to altmetrics, as the subject of how impact changes over time is somewhat unexplored currently.</p>
<p>These problems make finding metrics from these data sources difficult, but also makes finding provenance difficult. Twitter&#39;s API has undergone several changes since it&#39;s original release in 2006, and in particular deprecating it&#39;s older API URL structures. This has resulted in broken links in altmetrics services, where provenance cannot be accurately tracked.</p>
<ul>
<li>How citations are actually tracked<ul>
<li>URLs aren&#39;t reliable</li>
<li>2 order citations<ul>
<li>i.e. someone links to a blog post that links to the article</li>
<li>Somewhat solved by network effects</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="2-5-literature-review-conclusion">2.5 Literature Review Conclusion</h4>
<p>Altmetrics provides a unique opportunity to broaden the scope and reach of the field of scientific impact, in a much expedited and transparent fashion. Furthermore, several studies have shown that altmetrics correlates with traditional citation counts, thereby validating their use. Taylor (2013), says &quot;the potential for what we currently call altmetrics is nothing short of a complete map of scholarly activity and influence&quot;.</p>
<p>As scholarly usage of the web and social media increases, more communication between scholars will take place on mediums that we can measure using altmetrics. As discussed in the Case for Altmetrics section, Priem, Piwowar &amp; Hemminger (2012) believe that these conversations represent those that have been long held in the hallways and conferences, and that altmetrics gives us the opportunity to examine impact generated here. They go on to speculate that as scholarly usage of web tools grows, coverage of data sources will improve. The growth of this online communication would also bring more open APIs and access to altmetrics data.</p>
<p>Altmetrics are more open and transparent than traditional metrics, bringing bibliometrics up to the standards set by the rest of science. Peer review of research is a powerful and vital step in scholarly publication, whereas traditional bibliometrics are surprisingly lacking at publishing their methods. This unscientific approach leads to arbitrary and unfair results. The altmetrics community encourages openness, with several existing providers publishing their codebases under open source licenses. Chamberlain (2013) believes that methods of calculation &quot;should be very clear and accessible&quot;, allowing peers to check these calculations. Transparency among altmetrics may also lead to greater openness in the wider research community. Open access is a publishing model where readers do not have to pay for access to scholarly materials, that has grown in popularity recently with several new journals publishing using this model. Galligan &amp; Dyas-Correia (2013), citing Curry (2012), claim that &quot;the impact factor [is] one of the main culprits in the creation of a roadblock to open access&quot;. He argues that the impact factor&#39;s bias towards prestigious high impact journals prevents newer open access journals from gaining traction. By using a more diverse range of metrics, altmetrics can provide an alternative view of these important publishing models.</p>
<p>Priem, Piwowar &amp; Hemminger (2012) state that &quot;metrics must move beyond simply reporting counts&quot;, moving to a more nuanced lens with which to view impact. The somewhat meaningless number produced by the impact factor can only be put in context by other impact factor values. Several altmetrics practitioners have warned against a move to simplify or &quot;normalise&quot; altmetrics into a single &quot;altmetrics factor&quot;. Chamberlain (2013) says &quot;combining article-level metrics into a single score defeats one of the advantages of article-level metrics over the traditional journal impact factor&quot;. He believes that for altmetrics to &quot;avoid the pitfalls of the Journal Impact Factor&quot;, they should be &quot;important to different stakeholders&quot; and &quot;retain their context&quot;. Galligan &amp; Dyas-Correia (2013) believe that &quot;the tendency to desire one single score to evaluate research is one of laziness&quot;, preventing evaluators from gaining the full analysis of impact.</p>
<p>The same argument can be applied to any individual data source within altmetrics, however, this data source cannot be isolated without losing context. Telling the &quot;story&quot; of how impact was created is more valuable than raw numbers. This is especially true when looking at research products other than the scholarly article - for example, re-use of a dataset may continue well after publication of an associated paper, giving more scope for a &quot;story&quot; that fully describes the it&#39;s influence.</p>
<p>However, as the list of altmetrics data sources grows, analysis of this data would become complex and opaque. For this reason, categorisation of data sources into groups that have inherent properties is required. Lin &amp; Fenner (2013) attempt to describe an ontology for altmetrics that would &quot;establish thoughtful and meaningful ways of grouping similar altmetrics together and distinguishing them from other altmetrics with different meaning&quot;. The altmetrics provider ImpactStory presents users with a set of &quot;tags&quot; describing an article&#39;s impact according to the raw data sources. For example, a paper with a high number of Twitter citations would receive the &quot;Highly discussed by the public&quot; tag. The categorisation provides context, but allows for future data sources with similar properties to Twitter citations to be added. This approach allows a easy to understand, but nuanced view of the impact.</p>
<p>Altmetrics has the potential to uncover impact gained through non-traditional paths, such as those outside of science and in wider society. Currently there is no incentive within the impact assessment structure for outreach - explaining science to the general public through a variety of mediums. And thus, outreach is dominated by a few television stars, and by niche bloggers. However, a much wider range of outreach takes place, the PLoS Medicine Editors (2006) report that &quot;Magazine sections, such as those that we and other medical journals publish, ... &#39;add value&#39; to the research articles by interpreting them for a wider audience&quot;. Altmetrics, enables measurement of impact from these mediums, for the first time giving value to the hugely important task of outreach. Taylor (2013) states that &quot;the increasing strength of altmetrics will be to increase the detail and scope of the description of research in society&quot;. As researchers can demonstrate to assessors the impact of outreach on society, rewards will increase. This incentivises outreach, and provides competition, potentially improving the quality of scientific outreach.</p>
<p>As discussed in the History of Bibliometrics section, the growing flood of research is overwhelming scientists who cannot keep up with the volume of published work. Neylon &amp; Wu (2009) report on possible strategies to combat this: &quot;Our only options are to publish less or to filter more effectively, and any response that favours publishing less doesn&#39;t make sense, either logistically, financially, or ethically&quot;. This is the ultimate purpose of bibliometrics, to filter all possible information to find valuable and trustworthy information. As Clay Shirky (2008) famously said at the Web 2.0 Expo, &quot;it&#39;s not information overload, it&#39;s filter failure&quot;. The impact factor and other traditional metrics calculate article impact using citation counts to test against these criteria. Altmetrics simply offer an alternative filter, that can give a different perspective on impact, and thus on the filter they provide.</p>
<p>The purpose of this review is to present altmetrics as an alternative to the impact factor, however one cannot conclude that altmetrics are a wholesale replacement for traditional bibliometrics. They still offer a useful filter for research, and in some cases citation counts are included in altmetrics services. This is especially true for cases where trustworthy information is required, as it is unlikely that an article with high citation counts is scientifically unsound. This is the basis of the hierarchy of altmetrics categories discussed in the What is Altmetrics section, where data sources that involve a higher level of engagement - such as citing in an academic paper - are expected to create more impact.</p>
<p>Ultimately, altmetrics cannot be used as the definitive answer to the question of impact assessment. Qualitative assessment of the work must be combined with the quantitative data provided by altmetrics. Liu &amp; Adie (2013) believe that &quot;users must frame appropriate questions and decide what information they want the altmetrics data to provide&quot;. To succeed, altmetrics must broaden the scope of bibliometrics, provide data in a more timely manner and provide context for itself.</p>
<h5 id="2-5-1-further-research">2.5.1 Further Research</h5>
<p>Altmetrics is still a field in it&#39;s infancy, with much work to done in various areas. Priem, Piwowar &amp; Hemminger (2012) comment that much work is required around &quot;reducing noise that obscures the impact signal&quot;, that is &quot;crucial to understand what the events informing alternative metrics actually mean&quot;.</p>
<p>One area that is of great interest is a further exploration of altmetrics categorisation. Priem, Piwowar &amp; Hemminger&#39;s (2012) seminal work in this area requires more research to isolate and identify different &quot;different types of impacts on different audiences&quot;. Galligan &amp; Dyas-Correia (2013) also believe that work on &quot;determining meaningful clusters of metrics for particular groups&quot; is needed. They believe that certain categories can be more relevant to different assessors - some metrics may be more suited for librarians, or for the general public, while others are more relevant to researchers within the field.</p>
<p>Another area that has potential is the use of network effects in determining impact with less noise. Priem, Piwowar &amp; Hemminger (2012) also suggest research into this area: &quot;a tweet from a highly-connected, expert scholar should mean something different from one authored by a casual observer&quot;. Bollen, Rodriquez &amp; Van de Sompel (2009) apply this principle to the impact factor, weighting a citation by the prestige of the journal it was published in, finding improved results. This principle could be applied to altmetrics as a way to further reduce the influence of bad actors and noise.</p>
<p>As discussed in the Case for Altmetrics section, altmetrics gives us the opportunity to measure impact for a wider range of scholarly output. Of particular interest are datasets and software, which several recent efforts are looking to investigate for potential impact measurement. Priem, Piwowar &amp; Hemminger (2012) believe this, stating that &quot;investigation should also expand to examine altmetrics for scholarly products other than articles&quot;. In a recent announcement, the altmetrics provider ImpactStory, published their implementation of code impact through the collaborative software hosting service, GitHub. This means that re-use of software can be tracked and analysed for impact.</p>
<p>In most studies, there have been two periods of time covered by the research; between immediately after publication up and about a week after to publication; or over a long period, usually several years, after publication. This is usually performed on articles several years old, to give time for traditional citations to build up. The altmetrics data is then validated against the citation counts. The former approach is too short to consider trends in how the altmetrics data changes over time, and the latter approach also fails to analyse the rate of change, treating data as cumulative over the entire time period. This leaves an interesting area of study open - how do changes in altmetrics data over a period of time affect impact at a later stage. For example, if an article gains 50 Twitter citations within a week of publication, will this result in a higher number of traditional citations than if it gained 50 Twitter citations within a day of publication? Research in this area would provide additional context for altmetrics, again providing a new perspective on impact.</p>
<p>To approach this subject, tools are required to help kick-start research. As discussed, altmetrics can generate large amounts of data that can be difficult to analyse, and therefore visualisation tools are needed to assist researchers. This also true if altmetrics are to be used for qualitative assessment, not just for quantitative assessment, as visual aides help to understand the &quot;story&quot; of impact.</p>
<p><br></p>
<h4 id="2-3-chapter-summary">2.3 Chapter Summary</h4>
<p>In Chapter 1 we proposed ...</p>
<p>In this chapter the state-of-the-art was categorised into ... and ... . Observations were made on the systems reviewed (see section x), and the relevance of the state-of-the-art to ... was summarised (see section y).</p>
<p>The next chapter presents the design of ..., which is a system intended to ...<br></p>
<h3 id="chapter-3-artefact-design">Chapter 3 Artefact Design</h3>
<h4 id="3-1-introduction">3.1 Introduction</h4>
<p>In the previous chapter it was identified that altmetrics are a new approach to the field of measuring scientific impact, using an array of web-based services to discover impact. Altmetrics were found to be an alternative to traditional forms of impact measurement, such as the impact factor, improving the speed, diversity and - in some cases - accuracy of impact measurements. It was found that there is little study of how altmetrics change over time, and tools are required for this analysis.</p>
<p>In this chapter, the design of a system for visualising how altmetric data sources change over time will be discussed. The first section will describe the project&#39;s design methodology (section 3.2), and then the system&#39;s requirement&#39;s will be discussed (section 3.3). Finally, a solution is proposed in section 3.4.</p>
<p><br></p>
<h4 id="3-2-design-methodology">3.2 Design Methodology</h4>
<p>The design and development of the system was supported by the methodology described in this section. The methodology is based on an agile approach, that is similar to the Scrum approach.</p>
<p>However, initially, the project took a Waterfall approach during the research and initial design phase. This methodology takes a sequential approach where tasks are ordered and one task cannot start before another. One of the benefits of this methodology is that design must be done first, leading to greater efficiency later. It is useful in this situation, as there are few specific requirements early in the process, and research is required before design can start. Using this research, decisions about the direction and requirements for the system will be made, decisions that drive the design. The design of the system changed several times during the research phase, as more was discovered about the field of altmetrics. Research continued throughout the project, continually feeding into the design of the system.</p>
<p>After this phase, the agile approach was adopted, where a prioritised list of tasks is created based on the system requirements. These are then split into short &quot;sprint&quot; periods where focussed work on the assigned tasks was completed. During each period, planning for the task, design of the implementation, and finally the actual implementation are performed, creating a working iteration of the final product. This methodology is useful as it is good at adapting to change in the design and requirements. As change is encountered, the list of tasks can be modified, and the currently assigned task can be redesigned to fit with the change.</p>
<p>Within individual sprints of this agile phase, implementation of the final product is completed. To ensure that changes to the code base do not affect previous iterations, testing were performed. A Test Driven Development (TDD) approach was adopted, where unit tests were written before production code was written. This approach drives design before implementation by forcing consideration of the final result and of any edge cases before any implementation is attempted. It also encourages a modular design, as tests can be written for smaller parts easily and reused elsewhere. Finally, TDD encourages refactoring of poor or outdated code, as it greatly reduces the fear that rewriting the old code will introduce new bugs. The approach is increasingly becoming popular because of these attributes.</p>
<p><br></p>
<h4 id="3-3-requirements">3.3 Requirements</h4>
<p>The main purpose of the system is to provide tools for those investigating altmetrics. To achieve this, there are many requirements that are described in this section. Each requirement is numbered, for referencing later.</p>
<p>As described in section 3.2, the requirements changed as research progressed. The project is driven primarily by this research, and therefore system requirements could not be developed until after some study has been completed. It became clear that, as research developed, there is little study of how altmetrics change over time, which lead to focussing the system towards providing a tool that visualises how altmetrics change over time. Similarly, after research was conducted, several APIs were found that provide altmetrics data, thereby removing the requirement for a system to generate altmetrics data. The following requirements are taken from the latest and most up-to-date set of requirements.</p>
<h5 id="requirement-1-visualisation-of-altmetrics-data-sources">Requirement 1. Visualisation of altmetrics data sources</h5>
<p>It was found in section 3.6 that more tools for analysing and visualising altmetrics data are required. In addition, it was found that validation of altmetrics is required for analysis to be accepted more widely. The system, therefore, must provide a mechanism for showing a visualisation of altmetrics data, with some validation of the data. The visualisation must allow for some comparison of article altmetrics.</p>
<p>As discussed in section 3.2, altmetrics assessment is often performed on a body of work, such as an author&#39;s career output. Therefore, the system must be able to provide a visualisation for multiple articles of interest. The system will generate a graph using altmetrics data from these selected articles. This allows the user to compare altmetrics data between these articles.</p>
<p>In addition, altmetrics researchers may only wish to focus on a subset of altmetric data sources (see section 2.6), or they may want to directly compare altmetric data sources. The system must provide a mechanism for this, by allowing the user to switch between altmetric data sources.</p>
<p>The visualisation must also show the total number of scholarly citations for the article, so that the altmetric data sources can be compared to a more traditional measure of impact. As discussed in section 3.6, citation counts are often used as a validation measure to compare against altmetrics data.</p>
<p>Finally, the visualisation must have a method for associating with the original article set, giving each article&#39;s title and final metric values. This allows the user to associate a paper with it&#39;s visualisation.</p>
<h5 id="requirement-2-visualisation-of-altmetrics-changing-over-time">Requirement 2. Visualisation of altmetrics changing over time</h5>
<p>As discussed in section 3.6, there has been little or no study of the temporal aspect of altmetrics. The previous requirement discussed the need for a visualisation of altmetrics data, which can be extended to meet this requirement.</p>
<p>The system will provide a fourth axis for the graph, for the current time, and can be changed by the user. Data on the graph will be shown for the current time. For example, if this time axis shows the year 2009, then the altmetric data for 2009 will be shown on the graph. The current time will be shown on the graph, allowing the user to easily identify what the current time is.</p>
<p>The user will be able to easily change this axis, moving back and forward in time. This allows them to perform a comparison as the data changes over time. In this way, the recommendations in section 3.6 are achieved.</p>
<h5 id="requirement-3-scholarly-article-search">Requirement 3. Scholarly article search</h5>
<p>Users of the system must be able to find articles that they wish to visualise altmetrics for, therefore a search for scholarly papers is required. A search form must be provided for the user, to input parameters for their query. The form must contain fields including article title, author, journal and DOI. Search results must include article metadata, so that users can correctly identify articles of interest.</p>
<p>As discussed in section 2.2, altmetrics assessment is often performed on a body of work, such as an author&#39;s career output. Therefore, users must be able to identify multiple articles of interest, that altmetrics data is to be generated for.</p>
<p>The search should return articles in an identifiable manner. Articles must be uniquely addressable, so that one article can be distinguished from another. In the academic fields this is achieved using the Digital Object Identifier (DOI) System, standardised under ISO 26324. To conform with this standard, the system will return a list of DOIs that match the search parameters.</p>
<p>The search system will be released separately from the main artefact, and therefore must be developed with best practices. Search logic must be encapsulated within the search system, so that third party developers can reuse the code in novel ways.</p>
<h5 id="requirement-4-altmetrics-data-collection">Requirement 4. Altmetrics data collection</h5>
<p>As is evident in the previous requirements, altmetrics data is required for this system. This data could be generated by the system itself, by querying the APIs of the various services described in section 3.3 and building it&#39;s own altmetrics data. However, as described in section 3.4, there are several technical problems with collecting this data. In addition, it would take time and resources to create this data that are unavailable for this project, especially for work that is somewhat out of scope for this project. There are existing altmetrics providers that have built their own data stores which can be queried to collect this data, each of which will be assessed for suitability with this project (see requirement 10).</p>
<p>The system must pass the DOIs of the selected articles to a provider API. The system must then provide a way of accessing data from these providers for the selected articles. Finally, the system must be able to interpret their response, for use in the visualisation.</p>
<p>The data collection system will also be released separately from the main artefact, as a module. Best practices for module development must be adopted. Data collection logic will be encapsulated within the module to create, so that it can be reused in other applications.</p>
<h5 id="requirement-5-storage-of-results">Requirement 5. Storage of results</h5>
<p>The system must provide a way of saving visualisation results so that users can revisit later. If no storage was provided, altmetrics data would need to accessed from the provider every time the visualisation results were viewed. This creates unnecessary network traffic and would significantly slow down the application.</p>
<p>In addition, the system must provide a mechanism for accessing the resulting visualisation after generating it. This will be done using a permalink - a URL that will show the user the same visualisation, using the same altmetrics data every time the URL is visited. This allows users to demonstrate altmetrics impact to assessors without having to regenerate the data every time.</p>
<h5 id="requirement-6-easy-to-use">Requirement 6. Easy to use</h5>
<p>The system must be easily understandable by users who are not familiar with technical altmetrics terms, and those outside of the technology industry. The project is aimed at users in scientific fields, who do not have a background in computer science and therefore cannot be expected to understand complex systems or obscure vocabulary.</p>
<p>As part of this requirement, the system must have good visual design. Scientific software is not known for it&#39;s striking visuals, and therefore a well designed, clean layout will attract users to the system, increasing it&#39;s popularity.</p>
<h5 id="requirement-7-open-source">Requirement 7. Open source</h5>
<p>As discussed in sections 3.4 and 3.6, transparency is a founding principle of altmetrics. The altmetrics community supports the use of permissive open source licenses, as they allow users to inspect how metrics were collected and calculated. Ultimately, this transparency enhances trust in altmetrics. Furthermore, reuse of code could be beneficial to altmetrics researchers investigating how altmetrics data changes over time. An open source license allows researchers to reuse code, and even develop it further by contributing changes back to the source.</p>
<p>For these reasons, source code for the system must be released under an open source license, as defined by the Open Source Initiative (OSI). The OSI has several compatible licenses listed on it&#39;s website.</p>
<h5 id="requirement-8-suitability-of-node-js-for-this-project">Requirement 8. Suitability of Node.js for this project</h5>
<p>As will be discussed in section 4.4, the system will be created using the server-side Javascript platform, Node.js. The project must evaluate whether Node.js is suitable for applications of this nature. Node.js was created in 2009 by Ryan Dahl, using the Google V8 Javascript engine. It is an interesting new technology, but there is some debate about it&#39;s stability and scalability.</p>
<p>For Node.js to be considered suitable, it must provide a platform that can be quick and easily built upon to construct an application such as the one described in the requirements above. The platform must enable applications to be well structured and efficient.</p>
<h5 id="requirement-9-suitability-of-d3-js-for-this-project">Requirement 9. Suitability of D3.js for this project</h5>
<p>Similar to requirement 8, D3.js will be used to provide the visualisation required for this project, and this Javascript library will be evaluated to see whether it is suitable for creating visualisations. As described in requirements 1 and 2, a complex graph will be built using a large amount of altmetrics data. If D3.js can provide a stable and efficient platform for creating such graphs, then it can be considered suitable.</p>
<h5 id="requirement-10-assessment-of-existing-altmetrics-providers">Requirement 10. Assessment of existing altmetrics providers</h5>
<p>As discussed in requirement 4, altmetrics data will be sourced from an altmetrics provider. The PLOS ALM API, the ImpactStory API and the Altmetric.com API all provide access to this data, with varying levels of compatibility for this project. They will be compared to see which is the most suitable for this implementation.</p>
<p>The selected provider must include historical data, where metadata on when altmetric citations occurred. For example, the API would provide a breakdown of each data source, showing the increase in altmetric citations for each year. This breakdown must be no longer than a year, otherwise any analysis would be too general to be useful. The system requires this to be able to show the difference in altmetrics data between time periods, as discussed in requirement 2.</p>
<p>In addition, the provider must be able to provide altmetrics data for a representative sample of articles. Altmetrics data can be time-consuming and resource-heavy to generate, and therefore it can be expected that not every article ever published will be covered by the provider. However, users must be able to access data for a reasonable set of articles.</p>
<p>Finally, the provider must offer data for a useful set of altmetric data sources (see section 3.3). If a provider only tracks altmetric citations from a small group of data sources, then some diversity and context is lost from the analysis. As discussed in section 3.6, this is a core concept within in altmetrics, so if significantly lost, the provider would become unsuitable for this project.</p>
<p><br></p>
<h4 id="3-4-proposed-solution">3.4 Proposed Solution</h4>
<p>The following section describes the proposed solution to the requirements laid out in section 3.3. This solution is to create a web application called Quo that will assist altmetrics researchers in studying how altmetrics change over time. The application will consist of four parts, that are described in detail in the subsections 3.4.5 - 3.4.8.</p>
<p>A web application will be created on the Node.js platform, written in the Javascript language. This application will consist of a website that allows users to search for articles that they wish to view altmetrics data for, visualise this data, and allow them to save it in a database.</p>
<p>The application will provide a search form, with various fields for article metadata. When the search is requested, the server will access the PLOS article search API, to find matching articles. A wrapper around this API will be created as a standalone module, that will abstract the implementation details of API. The module will return a JSON response containing a list of DOIs.</p>
<p>Using this list of DOIs, a request is made to the PLOS ALM (Article Level Metrics) API, to access the altmetrics data for the selected articles. Another wrapper around this API will be created, again as a standalone module. This module encapsulates the API details, and so can be reused in other projects. The module will return a JSON object containing the altmetrics data.</p>
<p>This data is then returned to the client, where it will be used to create the visualisation. The D3.js library will be used to create a bubble chart where each article is represented by a bubble, the bubble&#39;s size corresponds to the total number of citations received, and the x- and y- axis will correspond to altmetrics data sources. A forth &quot;axis&quot; will be represented by the current year, displayed on the background of the chart, and will allow users to control moving back and forward in time. The current values of the altmetric data sources will be calculated using the current year.</p>
<p>When the data is returned to the client, it will also be saved in the application&#39;s database. A Node.js module will be used to interact with the MongoDB database. Application data will be stored in a JSON-like format and associated with a unique identifier. Users can access the visualisation and data later using this unique identifier, through the use of a permalink.</p>
<h5 id="3-4-1-web-application">3.4.1 Web Application</h5>
<p>The project will create a web application to meet the requirements described in 3.3. A web application was chosen as it provides a method for everyone to access the application, quickly, easily and without downloading any large programs.</p>
<p>As discussed in section 2.6, altmetrics are diverse and measure impact from outside science - areas that may not have access to expensive computer equipment. Therefore, a medium that can be accessed by all is suited to this project. No additional programs - other than a web browser, which often come pre-installed on many devices - are required to view a website. In addition, no restrictions are placed on the usage of the web, unlike much of the existing scholarly publishing field where paywalls regularly prevent access.</p>
<p>A website also provides a useful wrapper around the separate parts of the application, as described in sections 3.4.3 - 3.4.6. Without this wrapper, users would have to manually move data between the separate parts, leading to a much more complex and frustrating user experience.</p>
<h5 id="3-4-2-node-js">3.4.2 Node.js</h5>
<p>Node.js was chosen as the server-side framework for this project, and the logic behind this choice is explained in this subsection. Node.js is designed for server-side applications, providing methods for receiving and responding to HTTP requests. Applications for this platform are written in Javascript, taking advantage of the extremely powerful Google V8 Javascript engine that Node.js is based on.</p>
<p>The fact that applications are written in Javascript is one of Node.js&#39; main advantages, as it is the same language that is used on the client-side for web development. Developers do not lose context when switching between languages, such as with a more traditional set-up with PHP programs on the backend, and Javascript on the frontend. In the past, Javascript server-side frameworks have failed, however Node.js is built on the highly optimised V8 Javascript engine, meaning that programs can be executed much faster (&quot;JavaScript Performance Rundown&quot;, 2008).</p>
<p>As a web application is to be constructed for the application, and HTTP server is required. This can be easily achieved using Node.js, creating an application that will listen for HTTP requests and sends responses. The following code snippet is taken from the Node.js website:</p>
<pre><code class="lang-js">var http = require(&#39;http&#39;);

http.createServer(function (req, res) {
    res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/plain&#39;});
    res.end(&#39;Hello World\n&#39;);
}).listen(1337, &#39;127.0.0.1&#39;);

console.log(&#39;Server running at http://127.0.0.1:1337/&#39;);</code></pre>
<p>This would, when run using the Node.js command line interface, create a server that listens on port 1337 and on the 127.0.0.1 host that responds to requests with a plain text &quot;Hello World&quot;.</p>
<p>Modules are core to the Node.js platform, allowing for quick and easy addition and use of code written by yourself or others. This even applies to core parts of Node.js - in the example above, the <code>http</code> module is used and so it has to be <code>require</code>d first. The very popular npm (Node Package Manager) application comes bundled with Node.js, which enables developers to publish modules for others to use.</p>
<p>The modular philosophy that Node.js holds is very beneficial for this project. As described above, there are several sections of the application that can be split from the main project and published as standalone modules for others to use. As discussed in section 2.6, the altmetrics community encourages transparency and reuse, principles that are reflected in the Node.js developer community. These packages can be reused by any Node.js developer easily, or perhaps by a researcher looking to study altmetrics.</p>
<p>The basic server described above only provides a low-level and overly-verbose approach, that is ill-suited for thie project. For a more powerful server, a third-party module will be used. The Express module is one of the most popular modules available on npm, currently listed as the fifth most depended upon package on npm&#39;s package registry. Express offers greater control over common web server tasks, such as routing requests, templating views and generally improving upon the built-in <code>http</code> module. Express will be used as the framework for the application, receiving requests, routing them to the correct controllers and generating responses. The following snippet is taken from the Express website: </p>
<pre><code class="lang-js">var express = require(&#39;express&#39;);
var app = express();

app.get(&#39;/&#39;, function(req, res){
    res.send(&#39;hello world&#39;);
});

app.listen(3000);</code></pre>
<p>This shows the Express equivalent to the previous snippet, however in a much cleaner manner. Requests to port 3000, on the index of the current host, will return a plain text &quot;hello world&quot;. More routes can be added by registering further callbacks on the <code>app.get</code> function.</p>
<p>In the final application, Express will provide the backbone for the server, routing requests to the correct controllers, with any attached request parameters. These controllers will then call methods within the API wrapper modules, before generating responses from templates based on data returned from the modules. </p>
<h5 id="3-4-3-grunt">3.4.3 Grunt</h5>
<p>Grunt is a Javascript task runner that provides automation for many useful functions. There are several uses for this within the project including running the Node.js server, starting the MongoDB database, and process Sass files. Grunt can be configured to run these tasks from the command line, and therefore a useful collection of administration scripts can be created. Many of these tasks are packages published on Node.js&#39; package manager npm (see section 3.4.2), in the form of Grunt plugins. These plugins are designed to work with grunt, and only need to be configured to work with the current project. This configuration can be included in the project repository, to be shared with others so that they can use the administration scripts.</p>
<p>As described in section 3.4.2, Node.js has a web server that must be started from the command line. If changes are made to the application, the server must be restarted for changes to be reflected at runtime. This can become tedious in periods of heavy development. The third-party application, nodemon is design to help with this problem. nodemon will watch application files for changes and automatically restart the server. There is a Grunt plugin (called grunt-nodemon) that will perform this function, but also allow developers to add other Grunt tasks alongside nodemon. This will be used for the project to reduce development times.</p>
<p>Similarly, the database for the application must be started at boot. This is also achieved through Grunt, through the plugin, grunt-shell. This allows Grunt to call command line shell scripts, such as the one used to start the MongoDB database used for this project (see section 3.4.8). The command <code>mongod</code> will start the database, so grunt-shell is configured to run this command when the application is started.</p>
<p>The project will use the pre-processor Sass for creating stylesheets (see section 3.4.4). These must be converted to CSS before they are served to a user. This is achieved using Grunt. A plugin is used that will process the Sass files and convert them to CSS, called grunt-contrib-sass, which needs to be configured to set where the source files are and where the destination files will be. However, this plugin will need to be run every time a change is made to the source Sass files. The grunt-watch plugin will be used to watch for changes in the Sass files and automatically run the conversion scripts.</p>
<p>Finally, styles from the Twitter Bootstrap project will be used in this application (see section 3.4.4). These files do not need to be included in the repository, as they are merely included into the main CSS file without modification. Instead, another command line tool is used to download them from the project&#39;s host. Bower is &quot;front-end&quot; package manager, created by Twitter employees, for downloading open source Javascript and CSS projects. This will be configured to download the Bootstrap files, When the Grunt &quot;build&quot; task is run, the plugin will call Bower to download the files and then move them to a configured directory.</p>
<h5 id="3-4-4-styling">3.4.4 Styling</h5>
<p>Requirement 6 states that the system must be &quot;easy-to-use&quot; and have &quot;good visual design&quot;. To help improve this, the Twitter Bootstrap project will be used to provide a framework for the visual design. The project, created in CSS, allows developers to create stylesheets quickly, by providing a library of pre-constructed components. These components can be created by adding class attributes to HTML documents. In effect, Bootstrap creates a much more appealing default stylesheet for the web. For example, the following code snippet will create a button, that is shown in Figure 3.3.</p>
<pre><code class="lang-html">&lt;button class=&quot;btn btn-primary&quot;&gt;Primary&lt;/button&gt;</code></pre>
<p><img src="../../src/img/figure3-3.png" alt="Figure 3.3: Button created using Bootstrap"></p>
<p>The styles provided by Bootstrap are useful, however, they do require adjustment. This can be a little difficult when using CSS, especially considering the challenges faced in a modern web environment. For example, many browsers use CSS pre-fixes for newer or experimental features. Adding these manually is time-consuming, and so CSS pre-processors have started to fill the gaps left by CSS.</p>
<p>Sass is one such preprocessor, which follows a very similar syntax to CSS but have to passed through a conversion script to be turned into CSS. No browser will interpret a Sass file. Sass offers several advantages over CSS, including variables, a nested syntax, and a function-like syntax called mixins. These allow a developer to spend less time doing functional work to focus on true design work. For example, changing colours in a CSS file can take time to find all instances of the colour, however, with Sass a variable can be used for each of these instances. Changing a colour is as simple as changing that variable.</p>
<p>Another benefit that Sass provides is automatic concatenation, through imports. CSS also uses a <code>import</code> syntax, however this is not considered best practice as it adds unnecessary HTTP requests. One Sass file can import another on the server-side, creating a single concatenated CSS file. This will be used in the project to spilt the stylesheet into smaller, more meaningful modules that are combined into a single CSS file when the &quot;build&quot; script is run (see section 3.4.3).</p>
<h5 id="3-4-5-altmetrics-data-collection">3.4.5 Altmetrics Data Collection</h5>
<p>Requirement 4 describes the need for altmetrics data collection from an external API. As discussed in this requirement, gathering new data for the system is out of scope, as it is time- and resource-heavy, and several different altmetrics providers offer APIs. There are several options available, although not all satisfy the requirements. The following table compares the different altmetrics data APIs:</p>
<ul>
<li>Table comparing altmetrics providers<ul>
<li>Features of the APIs<ul>
<li>History</li>
</ul>
</li>
<li>How data is collected<ul>
<li>Twitter</li>
</ul>
</li>
<li>Rate limits<ul>
<li>Altmetric.com rate limits are lower - &quot;significantly higher if using an api key&quot;</li>
</ul>
</li>
<li>ImpactStory api (does it still exist?)</li>
<li>PLOS ALM API<ul>
<li>Github issue about v3 which will include historical metadata for all data sources</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>From the table it is clear that, for this project, the most appropriate solution is the PLOS Article Level Metrics (ALM) API. Crucially, it provides historical metadata, about when altmetric citations occurred. This is obviously important as the system must be able to show altmetrics changing over time, and that cannot be achieved without this data. The PLOS ALM API also gives a more detailed breakdown of when citation occurred. Citation numbers are given every month (or every year) for each data source, whereas Altmetric.com&#39;s API only gives aggregate numbers for all data sources at set points after publication (each day in the first week, 1 month after, 3 months after, 6 months after and 1 year after). The rate limits when compared with the Altmetric.com API are much higher, beyond the greatest expected usage and so they do not need to be considered.</p>
<p>However the PLOS ALM API has a major drawback - only articles that are published with PLOS journals are tracked by the API. Altmetric data for articles published in other journals cannot be calculated since this data is not gathered. Despite this restriction, the system will still offer a very large sample of articles to users as the PLOS library is so large. In 2013, the PLOS One journal published 31500 articles (&quot;Thanking Our Peer Reviewers&quot;, 2014).</p>
<p>The PLOS ALM API gives access to altmetrics data that PLOS collects regularly. The program that collects this data has been published under an Apache 2.0 License. Requests to the API will be made with a list of DOIs that represent the articles that the user has selected (see section 3.4.4). For this application, the option for historical metadata will be added to the request. Responses are returned in JSON format, the preferred format for this project, as XML format can be difficult to parse. An example request and response are shown below:</p>
<pre><code>http://alm.plos.org/api/v3/articles?api_key={YOUR_API_KEY}&amp;ids=10.1371/journal.pone.0035869&amp;info=history

[
    {
        &quot;doi&quot;: &quot;10.1371/journal.pone.0035869&quot;,
        &quot;title&quot;: &quot;Research Blogs and the Discussion of Scholarly Information&quot;,
        &quot;url&quot;: &quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0035869&quot;,
        ...
        &quot;publication_date&quot;: &quot;2012-05-11T07:00:00Z&quot;,
        ...
        &quot;views&quot;: 21454,
        &quot;shares&quot;: 135,
        &quot;bookmarks&quot;: 135,
        &quot;citations&quot;: 9,
        &quot;sources&quot;: [
            {
                &quot;name&quot;: &quot;citeulike&quot;,
                &quot;display_name&quot;: &quot;CiteULike&quot;,
                &quot;events_url&quot;: &quot;http://www.citeulike.org/doi/10.1371/journal.pone.0035869&quot;,
                &quot;metrics&quot;: {
                    &quot;pdf&quot;: null,
                    &quot;html&quot;: null,
                    &quot;shares&quot;: 25,
                    &quot;groups&quot;: null,
                    &quot;comments&quot;: null,
                    &quot;likes&quot;: null,
                    &quot;citations&quot;: null,
                    &quot;total&quot;: 25
                },
                &quot;update_date&quot;: &quot;2014-03-05T07:55:58Z&quot;,
                ...
                &quot;by_day&quot;: [
                    {
                        &quot;year&quot;: 2012,
                        &quot;month&quot;: 5,
                        &quot;day&quot;: 12,
                        ...
                        &quot;shares&quot;: 5,
                        ...
                        &quot;total&quot;: 5
                    },
                    ...
                ],
                &quot;by_month&quot;: [
                    {
                        &quot;year&quot;: 2012,
                        &quot;month&quot;: 5,
                        ...
                        &quot;shares&quot;: 17,
                        ...
                        &quot;total&quot;: 17
                    },
                    ...
                ],
                &quot;by_year&quot;: [
                    {
                        &quot;year&quot;: 2012,
                        ...
                        &quot;shares&quot;: 22,
                        ...
                        &quot;total&quot;: 22
                    },
                    ...
                ]
            },
        ]
    }
]</code></pre>
<p>This shows the generated response for the article (with DOI 10.1371/journal.pone.0035869). Firstly, the aggregate altmetrics, organised by category are given - 21454 views, 135 shares and bookmarks and 9 citations since publication. Then an array of altmetric data sources is given, each giving the total metrics for the given source. Finally, each data source reports the number of citations each day, month and year that it changes. So in this example, the article gained 5 CiteULike citations on 12th May 2012, given in the <code>by_day</code> array of the CiteULike object. Further documentation of the API is available from the API website (&quot;Public Library of Science (PLOS)&quot;, n.d.).</p>
<p>It should also be noted that the request includes an API key, which can be obtained from PLOS ALM API website. This is used to control usage of the API, however there is no published rate limit. It is not expected that this limit will be reached, and so no mitigation is planned.</p>
<p>A wrapper will be created for around this API, encapsulating it&#39;s functionality into a single library that has a uniform interface. This abstracts implementation details of the API out of other parts of the codebase that do not need to know these details. The uniform interface will make it easier for queries to be made, as only the list of DOIs will need to be provided to create a response. Another advantage of this modular approach is that if changes are made to the API, then only the wrapper will be affected.</p>
<p>The wrapper will accept a single DOI (as a string) or a list of DOIs (as an array). Optionally, configuration can be set on the wrapper, for example, setting the option to request the historical metadata as described above. These are then validated, before the request string is built using the input DOI list. The request is sent to the API, and the response is parsed to ensure no errors have occurred. The wrapper will then return this parsed response.</p>
<p>In above sections, the ability for standalone Node.js packages to be published was described. The PLOS ALM API wrapper will be published in this manner. The package will be available on the module registry, npm, allowing third-party developers to download and use the package in their own systems, encouraging further altmetrics study. The module will be released under a MIT license, giving permission for others to not only use the package but to make changes and contribute back to the source, improving the project. This shows the great benefit of Node.js&#39; modularity and altmetrics culture of openness.</p>
<h5 id="3-4-6-scholarly-article-search">3.4.6 Scholarly Article Search</h5>
<p>Requirement 3 describes the need for a scholarly article search. The proposed solution to this problem is in two parts: the search form, and the PLOS Search API wrapper. The search form will provide the user with a mechanism for interacting with the system to give their search parameters. The search wrapper will execute the search, using these parameters, to the PLOS Search API which will find matching article&#39;s in it&#39;s archive and return a list of matching articles.</p>
<p>The application will serve the search form as a static web page, containing a form. The form is a relatively simple way for users to interact with the system to find &quot;articles of interest&quot;, or articles which they wish to receive altmetrics data for. The form will provide input fields for the following, expanded from requirement 3:</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Keyword</td>
<td>Matches all text in an article</td>
</tr>
<tr>
<td>Author</td>
<td>Matches article&#39;s author name</td>
</tr>
<tr>
<td>Journal</td>
<td>Matches article&#39;s journal name</td>
</tr>
<tr>
<td>Subject area</td>
<td>Matches an article&#39;s subject area (taken from PLOS taxonomy)</td>
</tr>
<tr>
<td>Publication date</td>
<td>Matches article&#39;s publication date</td>
</tr>
<tr>
<td>DOI</td>
<td>Matches an article&#39;s Digital Object Identifier</td>
</tr>
</tbody>
</table>
<p>Once the form is submitted, a request will be made to the application server with the attached search parameters. This request can take the form of a regular POST request, or can be captured, using Javascript, and sent using an AJAX request, meaning that the form follows the convention of progressive enhancement. The parameters are received by the server and passed to the search API wrapper.</p>
<p>The search API wrapper will perform a search for articles that match these search parameters. Unfortunately, there are very few reliable sources for searchable published scholarly material. Initially, the CrossRef API was considered as it provides search for a large number of publishers. Search parameters for all of the fields required for the form are provided by this API. However, and as discussed in section 3.4.3, only articles published by PLOS can be processed by the PLOS ALM API wrapper, and therefore the search part of the system cannot return articles published by other journals. This drawback means that the sensible choice for the search API is the PLOS Search API, as this will only search for PLOS articles, and so solves the problem of users searching for non-PLOS articles. For this reason, the PLOS Search API was chosen as the search API for use in the application.</p>
<p>The PLOS Search API allows access to PLOS&#39; internal search engine, based on the Apache Lucene project, Solr. Accepted search parameters meet those required by requirement 3. Responses are returned in JSON format and an example request and response are shown below:</p>
<pre><code>GET http://api.plos.org/search?q=&quot;altmetrics&quot;&amp;wt=json&amp;api_key={YOUR_API_KEY}

response: {
    numFound: 13,
    start: 0,
    maxScore: 1.8082654,
    docs: [
        {
            id: &quot;10.1371/journal.pone.0064841&quot;,
            journal: &quot;PLoS ONE&quot;,
            eissn: &quot;1932-6203&quot;,
            publication_date: &quot;2013-05-28T00:00:00Z&quot;,
            article_type: &quot;Research Article&quot;,
            author_display: [
                &quot;Mike Thelwall&quot;,
                &quot;Stefanie Haustein&quot;,
                &quot;Vincent Larivière&quot;,
                &quot;Cassidy R. Sugimoto&quot;
            ],
            abstract: [
                ...
            ],
            title_display: &quot;Do Altmetrics Work? Twitter and Ten Other Social Web Services&quot;,
            score: 1.8082654
        },
        ...
    ]
}</code></pre>
<p>This response shows that a simple search for the keyword &quot;altmetrics&quot; will return 13 articles, including each article&#39;s DOI. These DOIs can then be used to find their altmetrics data.</p>
<p>Again, it should be noted that the request includes an API key, which can be obtained from the PLOS Search API website. However, unlike the previous API, a rate limit is enforced. If an API key exceeds 7200 requests a day then the API key will be blocked. However, it is not expected that this limit will be reached.</p>
<p>As described in the introduction for this section, a wrapper around the API will be created. This is to abstract details of the API away from other parts of the codebase, encapsulating it&#39;s functionality into one module. This will also make querying the API easier, as a uniform interface is provided. Responses will be parsed by the wrapper, making returned data more consistent.</p>
<p>The wrapper will accept search parameters as either a string or an object. A string will indicate that a keyword search is to be performed, while an object will contain key/value pairs indicating which field is to be searched with which value. These parameters are then validated, to ensure that errors made by the user will not produce invalid responses from the API. For example, any search parameter that is not used by the PLOS Search API will raise an error, as this may return an error response from the API. The wrapper will then construct a request string using the validated parameters, and send the response to the API. The response received from the API will be parsed before it is returned to the wider system.</p>
<p>Similarly to the PLOS ALM API wrapper described in section 3.4.3, the wrapper will be published separately as a package. The module will be released under a MIT license, a open source license that satisfies requirement 7. This means that others can download the module from npm, and use within their systems without having to obtain permission. This, again, shows the great benefit of Node.js&#39; modular philosophy.</p>
<h5 id="3-4-7-visualisation">3.4.7 Visualisation</h5>
<p>Requirements 1 and 2 state the need for a visualisation of altmetrics data and how it changes over time. The solution for these requirements will be a D3.js &quot;bubble&quot; chart that will show a visualisation of multiple articles with axes for two altmetrics data sources, an axis for the number of citations and an axis for time.</p>
<p>The chart will show two altmetric data sources on the x- and y-axis, allowing the user to switch between data sources of their choice. Articles are represented by a &quot;bubble&quot; on the chart that is centred according to the values of the selected data sources. The size of the bubble represents the total number of traditional scholarly citations the article has received. An example is shown below:</p>
<p><img src="../../src/img/figure3-1.png" alt="Figure 3.1: Example bubble chart"></p>
<p>This will meet some of the requirements described in requirement 1, as it allows the user to compare multiple articles against each other. Two altmetric data sources are shown on the x- and y-axis, and the number of scholarly citations is shown in the size of the bubble. Users can see if a large number of views (as in this example) leads to a large number of citations, and thus greater impact. Requirement 1 also states that the altmetric data sources must be able to change to suit the user, and so a system will be created that will allow the user to switch the x- and y-axis between the available data sources. Finally, to meet all of requirement 1, there must be some interface for associated each bubble to the article it represents. As the chart will be built using Javascript and HTML&#39;s SVG specification, it is relatively easy to add a &quot;tooltip&quot; that will appear when the user hovers over the bubble with the mouse. This will show the article&#39;s title, and current values for the selected data sources.</p>
<p>To meet requirement 2, the chart must be able to respond to users input and change the data on the chart to represent a different time period. Initially, creating a simple line graph that showed each data source against time was considered. However some tools already exist for this purpose, and comparison between data sources is difficult - a key component in requirement 1. A graph that allows comparison of data sources, while also showing changes over time is required. This area of the design was heavily influenced by Hans Rosling&#39;s &quot;Heath &amp; Wealth of Nations&quot; presentation (&quot;Hans Rosling&#39;s 200 Countries, 200 Years, 4 Minutes&quot;, 2010). In this presentation Rosling shows the life expectancy and income of 200 countries over 200 years. By animating through each year and changing the position of the bubble based on data for that year, changes at certain periods of time become evident that would have gone unseen except under close scrutiny. It is this comparison over changes over time that is missing from other visualisation styles that were considered. The concept will be adopted for the project, by animating the bubbles to move as the time changes. In addition to this animation, the user will also be able to control the passage of time manually using the mouse.</p>
<p><img src="../../src/img/figure3-2.png" alt="Figure 3.2: Animated bubble chart"></p>
<p>Much like the wrappers described in sections 3.4.3 and 3.4.4, the visualisation code will be encapsulated into a module. A small library will be created that will create the visualisation, once constructed during the system&#39;s runtime. Prototypical inheritance will be used to structure the library, giving a object-orientated style &quot;class&quot; that has a constructor, class variables and public or private methods. The library will accept basic options in it&#39;s constructor, which will set up where in the DOM the chart is to be added, and where the data can be accessed. The library will then build the chart and append it into the DOM.</p>
<p>The library will be self-contained, following the best practice of not polluting the global namespace in the browser, although it will depend on D3.js. Further, the library will follow the asynchronous module definition (AMD) convention, allowing clients to asynchronously download the library. This is often done using the popular Require.js library.</p>
<p>D3.js is a well known Javascript library used primarily for creating charts and graphs. It was chosen over other similar libraries for this application for several reasons.</p>
<p>Other potential libraries that were considered include Google Charts, and Chart.js. However, both of these libraries are focussed on instantiating a pre-made chart - line graphs, bar charts, pie charts - with a dataset. This approach is more suited to projects with a more traditional visualisation style, where a simple chart is required, and can be quickly created without overly verbose code. D3.js takes a more flexible approach, stating on it&#39;s website &quot;D3 is not a monolithic framework that seeks to provide every conceivable feature&quot; (&quot;D3.js - Data-Driven Documents&quot;, n.d.). This is much more appropriate for this project, where a custom style of chart is created specifically for this purpose.</p>
<p>D3.js is based on manipulating Scalable Vector Graphs (SVG) elements. This relatively modern technique is based on XML, and thus does not rely on raster images, meaning that graphics can be scaled without loss of resolution. This is very powerful for usage on the web, where images can be scaled down for smaller screen devices and scaled up for larger screen devices. The World Wide Web Consortium (W3C) standardised a specification for SVG in 2011, and is now supported in a majority of browsers. The D3.js library provides a cleaner, less verbose interface to the SVG specification. This makes manipulation of SVG elements in the browser relatively simple.</p>
<p>As the library is based on transformation of DOM elements, not on an internal representation of the shapes that make the chart, it can be considered more &quot;future friendly&quot;. As the web changes, and more standards are added, tools that focus on existing standards will require less change. D3.js is able to adapt to potential additions to the web specification, for example, more advanced CSS properties or new HTML elements.</p>
<p>One of D3.js&#39; most powerful features is it&#39;s concept of data &quot;joins&quot;. This functionality allows for actions that are to be taken when new data is added to the visualisation, or existing data is removed. This is extremely powerful on the web, which is increasingly dominated by asynchronous Javascript applications that can fetch new data on the fly. While this functionality is unused for this application there is scope for future work that potentially use it, further validating the choice of D3.js for this system.</p>
<p>Finally, D3.js provides a large amount of useful documentation and example applications that make it easy to learn. As described above, the library does not provide many built-in charts, and leaves much of these decisions to the developer. Therefore, a large gallery of examples helps to inspire as well as learn from.</p>
<h5 id="3-4-8-storage">3.4.8 Storage</h5>
<p>Requirement 5 describes the need for storing altmetrics data for faster and more efficient retrieval and for creating a permalink. Data from the altmetrics data collection process will be in JSON format, and so it is sensible to also store this data in JSON format. The storage process does not require a large degree of complexity, as the data only needs to be stored against a unique key so that it can be retrieved later. A key/value pair database therefore is mostly appropriate for this use case. However, future revisions of the project may require a deeper understanding of the data stored in the database. For this reason, it may be useful to have more structure to data stored in the database.</p>
<p>Data will be stored in the database after it is returned from the altmetrics API wrapper described in section 3.4.3. The application flow starts with the user submitting their search, which will call the search wrapper, returning a list of DOIs. This list of DOIs is passed to the altmetrics API wrapper, the results of which will then be stored in the database. The database will generate a unique key for this data, which is used to create a permalink. The user is then redirected to this link, and the visualisation is generated.</p>
<p>MongoDB was chosen for the application as it satisfies these requirements. Documents in MongoDB are stored in a JSON-like format, that is associated to a unique key. This is much less complex than a relational database, where a schema would have to be constructed. There are many tools for Node.js available for use with MongoDB. The most prominent is the <code>mongodb</code> library on npm. This provides a simple driver for connecting to MonogoDB, querying and updating the store. However, this library does not offer functionality for creating structure in the database. Another potentially useful library is the popular Mongoose library, that is modelled as a Object Relation Map (ORM) for MongoDB. This would give more structure to the database, while still storing data in the JSON-like format, and will likely give the best results.</p>
<h5 id="3-4-9-source-control">3.4.9 Source Control</h5>
<p>The source code of the application will be stored in a git repository, a source control system designed to assist developers write code. Code for the project will be hosted on GitHub, a service that offers code hosting for git repositories. By combining these, the project will have powerful tools for controlling, debugging and distributing code.</p>
<p>git allows developers to track and control changes to a codebase, by &quot;committing&quot; updates regularly. This is achieved by a system of cryptographic hashes generated using the source code itself. These hashes can be used as identifiers for stages in the code which can be moved between easily. This has the added benefit of helping to debug problematic code, by identifying when an error was introduced. &quot;Branches&quot; are concept within git that enable changes to be made in a semi-temporary area where developers can experiment with the codebase without fear of permanently breaking anything. Experiments can be merged into the main branch if found to be successful.</p>
<p>GitHub is a service that can accept updates, allowing code to hosted on their servers. This also gives the benefit of creating a backup. GitHub provides a easy-to-use Graphical User Interface (GUI) to git that is helpful under certain circumstances. The service also provides an issue tracker, for saving bugs and new ideas.</p>
<p>Updates can be shared using git&#39;s protocol with others who can make further changes. This means that code published under a permissive open source license can be easily edited by others before changes are submitted back to the original author. This structure enables much faster and easier open source development, as described in requirement 7.</p>
<h5 id="3-4-10-testing">3.4.10 Testing</h5>
<p>For the application to be considered to be successful, it must contain as few bugs as possible. Section 3.2 describes the project&#39;s design methodology of Test Driven Development (TDD). For this to take place, a testing framework must be included in the project. This will provide an environment for tests to be run and tested against. For this project, two libraries will be used; Mocha for running tests; and Chai for providing assertions.</p>
<p>Mocha is a test runner for Node.js, that enables developers to create tests that will be in it&#39;s environment. If an error is thrown by the test, then Mocha will report this, either on the command line, or in a browser. The example below shows a basic Mocha test (that will pass):</p>
<pre><code class="lang-js">var expect = require(&#39;chai&#39;).expect;
describe(&#39;Array&#39;, function() {
    describe(&#39;#indexOf()&#39;, function({
        it(&#39;should return -1 when the value is not present&#39;, function() {
            expect([1,2,3].indexOf(5)).to.equal(-1);
            expect([1,2,3].indexOf(0)).to.equal(-1);
        });
    });
});</code></pre>
<p>Chai is an assertion library, again for Node.js, that provides assertions that expressions can be tested against. If the assertion fails an error will be thrown. The examples below show some of the Chai assertions:</p>
<pre><code class="lang-js">expect(foo).to.not.equal(&#39;bar&#39;);
expect({ foo: &#39;baz&#39; }).to.have.property(&#39;foo&#39;).and.not.equal(&#39;bar&#39;);
expect(&#39;test&#39;).to.be.a(&#39;string&#39;);
expect(foo).to.exist;</code></pre>
<p>These libraries were chose because of their popularity, flexibility and ease of use. Mocha is widely used among the Node.js developer community. It is popular due to it&#39;s rich feature list, and flexibility. Mocha allows developers to choose which assertion library they wish to test with. This is unlike the also popular Jasmine library, which provides a complete all-in-one framework for tests. The Chai library was chosen for it&#39;s readable tests. As is shown in the above example, Chai tests can be very similar to sentences, an improvement over the &quot;assert&quot; style of other libraries.</p>
<p><br></p>
<h4 id="3-5-summary">3.5 Summary</h4>
<p>This chapter describes the high level requirements and design of the system that will provide tools to assist researchers examine how altmetrics change over time. The first section gave a brief introduction to the design of the system, which is followed by a description of the methodology and approach taken throughout this project. Section 3.3 describes the requirements needed to achieve the goals of this project, and section 3.4 details the proposed solutions to these requirements.</p>
<p>Chapter 4 will look in detail at the implementation phase of the project, describing the problems faced and the solutions to these problems. This closely related to the design described in this chapter, as implementation decisions are driven by the design.<br></p>
<h3 id="chapter-4-system-implementation">Chapter 4 System Implementation</h3>
<h4 id="4-1-introduction">4.1 Introduction</h4>
<p>The previous chapter presented the proposed design solution that will drive the implementation phase of this project. This describes the application that will provide tools for researchers investigating how altmetrics change over time.</p>
<p>This chapter builds on the design by detailing the key aspects of the search, data collection, visualisation, storage of the implementation. In addition, crucial details that make up the web application that combines these elements will be described.<br></p>
<h4 id="4-2-altmetrics-data-collection-implementation">4.2 Altmetrics Data Collection Implementation</h4>
<p>In section 3.4.5, a solution to the requirement for collection of altmetrics data was proposed. This describes a wrapper around the PLOS Article Level Metrics (ALM) API, allowing for cleaner and less verbose usage of the API. This wrapper was to be written in Node.js, and published as a standalone package on Node.js&#39; package manager, npm. The package is called nodealm, and can be downloaded by other developers looking to solve similar problems. This section describes the challenges faced when implementing this solution, and how the problems were addressed.</p>
<h5 id="4-2-1-best-practices">4.2.1 Best Practices</h5>
<p>As the module has been published as a standalone package, Javascript best practices must be followed. This is to assist other developers using the package, giving them greater understanding of how the package functions. One of these best practices involves structuring the module using prototypical inheritance. Widely popularised by Douglas Crockford, this form of object orientated programming takes advantage of Javascript&#39;s prototype chain to create class-like objects containing class-scoped variables and functions. Every object has an internal link to another object (called the prototype). This prototype has it&#39;s own link to another prototype, continuing until this prototype chain reaches a reference to <code>null</code>.</p>
<p>Object inheritance within this module is not required - it lacks the complexity for this. However, method and property inheritance are required. This is achieved in the same way as object inheritance, through the prototype chain. The following example shows a property and a function added to the prototype chain of a object.</p>
<pre><code class="lang-js">var Obj = {};

Obj.prototype.aProperty = &#39;foo&#39;;

Obj.prototype.aFunction = function() {
    ...
};</code></pre>
<p>Now if an instance of the <code>Obj</code> &quot;class&quot; is created - by calling <code>obj = new Obj();</code> - the function <code>aFunction</code> can be executed on the instance. The property <code>aProperty</code> can be accessed through the instance, by using the <code>obj.aProperty</code> statement. This encapsulates these components within the object, preventing them from polluting the global namespace, and other objects within the system.</p>
<p>This can be extended to achieve a &quot;constructor&quot; style class. Functions within Javascript also have a prototype, meaning that the prototype chain can be extended for functions in the same way as above. The following example demonstrates the declaration of class with a contructor.</p>
<pre><code class="lang-js">var Func = function() {
    // Constructor
};

Func.prototype.aProperty = &#39;bar&#39;;

Func.prototype.aFunction = function() {
    // Method
};</code></pre>
<p>This is used within the package to create a class, called <code>Alm</code>, that can be easily instantiated by external code and is completely encapsulated.</p>
<p>A constructor is used within the API wrapper package as a mechanism for setting up crucial data within the package. The following code snippet shows the constructor used in the wrapper module.</p>
<pre><code class="lang-js">var Alm = function(ids, options) {
    config = configs(this.options.mode);

    if (options)
        extend(config, options);

    this.ids = ids;
};</code></pre>
<p>This constructor accepts a list of DOIs - as described in section 3.4.5 - and an object hash. The last line of this constructor sets the list of DOIs as a class property for access by other class methods. The rest of the function will resolve the defaults options with input options. The first line imports the default options hash, which is then passed into the <code>extend</code> function along with the input options. This will iterate through properties in the input options and set properties with matching keys in the default options to be the same. This means that the default options will remain unchanged, unless they are specifically overwritten by input options.</p>
<p>The prototypical inheritance structure is enhanced by Node.js. Scope for a given expression is limited to the current module - in most cases, the current file - and the global scope that is maintained by Node.js. To access anything outside of this scope, it must be imported, using the <code>require()</code> method. This can be used to import from an external package (referenced using the name of the package) or from an internal module (referenced using the relative path). To &quot;export&quot; from a module, the <code>module.exports</code> object must be used. Only data referenced in this object can be accessed outside of the module.</p>
<p>This system of exporting scope can be used to create a equivalence of &quot;public&quot; and &quot;private&quot; methods found in traditional object-orientated languages. Methods that are public are added to the prototype chain, while methods that are private are only declared within the module and therefore are only scoped within this module.</p>
<p>This approach does not solve all problems related to inheritance. The most obvious of these is the <code>this</code> keyword, which holds a reference to the class within class methods. This is very useful, as it allows access to other properties within the class. The following example shows a demonstration where a class property is set in the constructor, then referenced in a class method.</p>
<pre><code class="lang-js">var Foo = function() {
    this.bar = &#39;baz&#39;;
};

Foo.prototype.aFunc = function() {
    return this.bar;
}</code></pre>
<p>However, this can become complicated when using prototyped method and non-prototyped methods. In locally declared functions the <code>this</code> keyword loses it&#39;s reference to the class - as it&#39;s technically not part of the class. Fortunately Javascript provides a way of mitigating this. The <code>bind</code> and <code>call</code> functions provide different systems for ensuring that the <code>this</code> keyword is bound to the desired reference. These are used in the wrapper package to maintain a consistent reference to the <code>this</code> keyword throughout the module.</p>
<ul>
<li>Consistent style<ul>
<li>i.e. all code written in the same style</li>
</ul>
</li>
</ul>
<h5 id="4-2-2-event-based-structure">4.2.2 Event Based Structure</h5>
<p>For the first iteration of the API wrapper, a procedural structure was adopted. For this structure, the module would be called procedurally, using a <code>getAlm</code> method, providing a list of DOIs and callback function. This would then bootstrap the module, call the API, process the results and call the callback when finished. This callback would be populated with the results, or if an error occurred, an error object. The follow example demonstrates this functionality.</p>
<pre><code class="lang-js">alm.getAlm(&#39;10.1371/journal.pbio.1000242&#39;, function(err, result) {
    console.log(result);
});</code></pre>
<p>Using this structure, the callback is handling both an error and successful response. The callback would test for an error response - it would be <code>null</code> if no error occurred - and throw an error. This could then be handled by the wrapper&#39;s callee appropriately. This is an acceptable structure, however an alternative structure proved better.</p>
<p>The wrapper module now follows an event based structure, where a new instance of the <code>Alm</code> class is created and event listeners for success and error responses are created. Finally, the <code>fetch</code> method is called which will fire the request to the API. This is shown in the following code snippet.</p>
<pre><code class="lang-js">var alm = new Alm(&#39;10.1371/journal.pbio.1000242&#39;);

alm.on(&#39;success&#39;, function(result) {
    console.log(result);
});

alm.on(&#39;error&#39;, function(err) {
    console.log(err);
});

alm.fetch();</code></pre>
<p>The class constructor would accept the list of DOIs and options, that were previously passed to the <code>getAlm</code> method. This would bootstrap the class, and make these properties available to the rest of the class. This the first key improvement of the initial structure. Key properties, such as the options hash, are immediately available to the entire class. The constructor performs it&#39;s function and bootstraps the class, instead of delegating this task to another method.</p>
<p>Node.js provides event functionality through it&#39;s <code>events</code> module. These events perform much the same task as events within the browser. Custom events can be created using the <code>EventEmitter</code> class, part of the <code>events</code> module, but only from classes that &quot;inherit&quot; from this emitter class. This is achieved using Node.js&#39; <code>inherits</code> method, part of the <code>util</code> module. Immediately after the wrapper class is declared, it is set to inherit from the <code>EventEmitter</code> class, giving access to the <code>emit</code> function. This has two parameters; the name of the event, and any data that will be attached to the emitted event. To listen to the event, the <code>on</code> method is used, as shown in the example above. This also takes two arguments; the name of the event it listening for, and a callback function. When the event is triggered, the callback will be fired.</p>
<p>These callbacks split the duties performed by the callback for the <code>getAlm</code> method. Instead of having to test for an error every time, the success callback will process successfully returned data, while the error callback will only be fired if an error occurs. This is much cleaner than the previous structure, and removes the need for an error check on every response.</p>
<h5 id="4-2-3-api-problems">4.2.3 API Problems</h5>
<p>The PLOS ALM API is a good base for providing altmetrics data, as discussed in section 3.4.5, however it is not without problems. The main problem that was encountered was when querying for many articles - over 20 -  with historical metadata in a single request. This will sometimes produce an error response, although it may be inconsistent. This may be due to the fact that a very large amount of data needs to be processed and so the request times out. Alternatively, there may be a bug with the API that causes this intermittently.</p>
<p>Fortunately, there does appear to be a work-around - the API allows users restrict the data sources that are included in the response. If the request includes <code>source=twitter</code>, for example, then only altmetric data pertaining to Twitter will be returned. This can be expanded to include multiple data sources, using a comma-separated list. In testing it was found that including a list for all possible data sources would not produce the error described above. This work-around has been implemented in the package as a temporary solution, but will be removed if the problem is resolved. The bug has been reported to the API providers.</p>
<ul>
<li>Best practice<ul>
<li>Library in lib directory, app.js file imports the necessary files</li>
</ul>
</li>
<li>request library<ul>
<li>How requests are actually sent</li>
</ul>
</li>
<li>Mention Cameron Neylon&#39;s pyalm</li>
</ul>
<p><br></p>
<h4 id="4-3-search-implementation">4.3 Search Implementation</h4>
<p>This section will describe the implementation of the search module, as described in section 3.4.6. A wrapper around the PLOS Search API was created, creating a uniform interface to the API. By encapsulating the API logic in a module, the codebase will become cleaner and less verbose.</p>
<h5 id="4-3-1-package-publication">4.3.1 Package Publication</h5>
<p>The search package, like the altmetrics API package discussed in the previous section, was also published to npm, under the name plos-search. This enables other altmetrics researchers or developers to download and include in their work. The npm command line client provides an easy way for packages to be published. Valid packages can be published using the <code>npm publish</code> command. To create a valid package, a <code>package.json</code> file must be created. This holds basic metadata about the package - package name, version number, author, repository - as well as the package&#39;s dependencies. External modules that are required for the package to run are listed, along with the relevant version number in the <code>dependencies</code> section of the <code>package.json</code> file. Modules that are only required for development - usually testing or build tools - are listed in the <code>devDependencies</code> section. npm can automatically construct a <code>package.json</code> file using the <code>npm init</code> command.</p>
<h5 id="4-3-2-similarity-to-altmetrics-module">4.3.2 Similarity to Altmetrics Module</h5>
<p>The altmetrics module discussed in section 4.2 is somewhat similar to the search module described in this section. Both follow the best practices (see section 4.2.1) and both use an event based structure (see section 4.2.2). This approach makes sense as there is some shared functionality between the modules. Both accept some parameters, use these to construct a request to their respective APIs, before parsing and validating the response and returning the data. Therefore, using a similar structure means that, overall, the codebase is cleaner and more readable.</p>
<h5 id="4-3-3-validation-of-search-parameters">4.3.3 Validation of Search Parameters</h5>
<p>In contrast to the altmetrics data package, where the input only consists of a list of DOIs, input for the search module is much more complex. The PLOS Search API will only accept search parameters from a specified list. Using a request that has parameters outside this list will produce a response with no results. To prevent this possibility, the module will throw an error if such parameters are used. This will reduce the number of unnecessary requests, as those which are known to produce an error will never be sent. To achieve this, the package must hold a &quot;whitelist&quot; of permissible parameters and must check current parameters against this whitelist before a request.</p>
<p>Implementing the whitelist is relatively easy. The package has some built in configuration, for holding the root URL to the API, and the API key. This configuration is loaded each time a new instance of the module is created. If this configuration is extended to include an array of permissible terms, the module will gain access to the whitelist.</p>
<p>To test current query parameters against the whitelist, a comparison function was created. The following code snippet shows this function, where the the <code>config.query_params_whitelist</code> contains the whitelist array and the <code>throwError()</code> function will throw an error from the module.</p>
<pre><code class="lang-js">testKey = function(key) {
    if (config.query_params_whitelist.indexOf(key) === -1)
        return self.throwError(&#39;Query param: &#39; + key + &#39; not recognised&#39;);
};</code></pre>
<p>This function will be called from the module&#39;s constructor. This means that an error can be thrown before error event listeners are attached, which would never be triggered and the module would silently fail. Therefore, if the module is likely to encounter parameters not on the whitelist, the instantiation of the module must be wrapped in a try/catch block. This is shown in the following code snippet:</p>
<pre><code class="lang-js">var Search = require(&#39;plos-search&#39;);

try {
    var search = new Search(&#39;&#39;);
}
catch (err) {
    // Handle error
}</code></pre>
<ul>
<li>Search API doesn&#39;t return HTTP error codes<ul>
<li>Just 200 with a message in the body</li>
<li>Have to parse response to check for errors</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="4-4-visualisation-implementation">4.4 Visualisation Implementation</h4>
<p>This section will discuss the implementation of the visualisation of altmetrics data. Section 3.4.7 describes the proposed solution for this requirement, a D3.js bubble chart comparing two altmetrics data sources against the total number of scholarly citations for a set of articles. Each bubble will be animated to show the passage of time, and how the data changed over this time period. To achieve these goals, a Javascript library was created that will perform the chart creation and management process.</p>
<h5 id="4-4-1-asynchronous-module-definition">4.4.1 Asynchronous Module Definition</h5>
<p>This library, called AlmChart, will be published under an open source license. Because of this, the library will follow best practices for browser-focussed Javascript libraries. These are similar to the best practices followed for the search and data collection modules, however there are some key differences.</p>
<p>Unlike Node.js, a library author cannot know what environment the library will be run under. The environment that Node.js maintains (see section 4.2.1) means that modules are separated from each other. To access another module, it must first be imported. This is not the case for browser libraries, where namespace collisions can produce errors if not carefully managed. This has lead to the rise of module definitions, which attempt to provide a standardised structure for Javascript libraries, separating them from each other, while still allowing import-like behaviour. One of the most popular of these is Asynchronous Module Definition (AMD), and this format is used for the AlmChart library. The AMD Wiki (&quot;AMD · amdjs/amdjs-api Wiki&quot;, n.d.) defines a single function, called <code>define</code>, that has the following signature.</p>
<pre><code>define(id?, dependencies?, factory);</code></pre>
<p>This shows how a module using AMD would be defined. The unique (per project) identifier, and the module dependencies are optional. The factory is the function that should be executed to instantiate the module. The AlmChart library uses this function to declare itself to AMD module loaders. However, not all developers use AMD, or module loaders and so some fallback must be provided. The following code snippet shows the full declaration of the AlmChart library.</p>
<pre><code class="lang-js">(function(root, factory) {
    if (typeof define === &#39;function&#39; &amp;&amp; define.amd) {
        // AMD. Register as an anonymous module.
        define(function() {
            // Also create a global in case some scripts
            // that are loaded still are looking for
            // a global even when an AMD loader is in use.
            return (root.AlmChart = factory());
        });
    }
    else {
        // Browser globals
        root.AlmChart = factory();
    }
}(this, function() {
    var AlmChart = function() {
        ...
    };

    ...

    return AlmChart;
}));</code></pre>
<p>This shows how if an AMD module loader is used, then the library is declared using the <code>define</code> function. Otherwise, the library is declared as a single class in the global namespace. This encapsulates chart functions and prevents the global namespace from getting polluted.</p>
<p>The AlmChart class is instantiated using the normal Javascript method - <code>new AlmChart()</code>. The library requires two options to be passed to the constructor. Firstly, the DOM element that the chart is to be appended to is required. Then the URL for the data source is also needed. Once instantiated, the chart will be drawn when the <code>draw()</code> function is called. The default data sources used will be the number of Twitter mentions and the number of views. These can be changed by executing the <code>setConfig()</code> function and passing in the new configuration options. This will change the data accessor functions within the library, and therefore will change the data shown on the chart.</p>
<h5 id="4-4-2-almchart-library">4.4.2 AlmChart Library</h5>
<p>The library performs several steps to set up, create and animate the chart. The chart depends on D3.js, which is used to perform most of the drawing and calculation tasks. However, to begin with including the D3.js library did not work. When including the development version of the library, the browser would throw errors, originating from a pi (π) character that was included in the source. It is suspected that some character encoding problems caused the errors. Fortunately, the compressed and minified version of the library did not include this character, and thus did not throw any errors. This is a practice that should be adopted for production-ready web sites anyway, and so was used in the final application.</p>
<p>When the AlmChart class is instantiated the configuration is set. This includes setting up the data accessors - functions that will retrieve values from the currently selected data sources. There three key data accessors; one for the x-axis, one for the y-axis and one for the radius of the bubble. They are called, with a given article&#39;s data passed into them, when the class requires a value for the article. The following code snippet shows the two of the data accessors.</p>
<pre><code class="lang-js">AlmChart.prototype.x = function(data) { return data.sources[this.config.dataSourceKeys.x]; };
AlmChart.prototype.y = function(data) { return data.sources[this.config.dataSourceKeys.y]; };</code></pre>
<p>By making these functions, the data sources can be changed on the fly. Changing the value of <code>this.config.dataSourceKeys.x</code> will change the calculated values for the x-axis.</p>
<p>As discussed above, the chart will not be drawn until the <code>draw()</code> function is called. This will kick off the internal drawing functions. The first step will create the scales and axes for the chart. Scales are a concept within D3.js for transforming values relative to the chart&#39;s size. This allows the data to be accurately plotted on graphs of any size. A key part of this function will be to find the earliest and latest year in the dataset. This is so that the starting and ending points of the animation can be calculated. The axes will hold all data about a single dimension of the graph, and so one is set up for the x- and y-axes.</p>
<p>Finally in this step, the tooltip will be set up. Tooltip support is provided by the d3-tooltip library, and will show a hovering box over a bubble when a user holds the mouse over the bubble. For the tooltip to be set up, a template for the element needs to be created. This is achieved by cloning a hidden template DOM node.</p>
<p>The next step involves downloading the data asynchronously from the server, before parsing into a Javascript object in memory. D3.js provides a mechanism for achieving this, the <code>.json()</code> function. If the data was in a different file format, for example XML files would be retrieved using the <code>.xml()</code> function. The data must then be filtered (see section 4.4.2), and it is passed to the <code>filterJson</code> function.</p>
<p>The next step will begin to draw to the screen. A container <code>g</code> element (a &quot;group&quot; element, part of the SVG specification) is created, within which all other chart elements will be appended. The axes are then drawn on screen, and transformed into the correct position. They are labelled with the relevant label, according to the currently selected data sources. A problem arose when attempting to draw the time axis on the graph. This axis cannot be shown as part of the x- or y-axes - it represents another dimension of the data and so must be shown in another axis. Inspiration was taken from Hans Rosling&#39;s presentation (see section 3.4.7), and a date was added to the background of the graph. However no compelling method could be found for showing the date axis if daily or monthly breakdowns of the historical metadata were used. Therefore the application uses the yearly breakdowns of the data.</p>
<p>Once this is completed, the bubbles are created. The <code>drawCircles</code> function controls this, creating <code>circle</code> elements (again, part of the SVG specification), which are appended to the container element. The circles are positioned and sized by the <code>position</code> function. This function will take a circle element as an argument and set it&#39;s x and y positions and it&#39;s radius according to the current year. These are calculated by taking the current year&#39;s value and scaling it using the relevant scale that were set up earlier.</p>
<p>Finally, the circles will be animated to move according the changes in altmetrics data as time passes. This is performed by D3.js&#39; <code>transition</code> function, which will calculate a number of &quot;frames&quot; between the beginning and end of the transition, calculating the amount of time passed between each one, and thus a new year for each frame. This new year may be fractional, as there are multiple frames between the different years. This year value is then passed to the circle&#39;s <code>enter</code> function, which will re-calculate data values for the x- and y-axes, and reposition the circles using these new data values.</p>
<p>As discussed above, the amount of time passed between years may be fractional. Clearly there is no data values for such fractional years. There are two approaches to this problem; either round up or round down the current year to make it an integer again; or calculate a interpolated value using the fractional year. The former approach would create a very &quot;jerky&quot; animation where bubbles only move at the end of a year, whereas the latter approach would smooth the movement so movement occurs throughout the year. Therefore, the interpolation approach was taken. Figure 4.1 shows a screenshot of the final visualisation.</p>
<p><img src="../../src/img/figure4-1.png" alt="Figure 4.1: Screenshot of the final visualisation"></p>
<h5 id="4-4-3-problems-with-the-dataset">4.4.3 Problems with the Dataset</h5>
<p>Unfortunately, it was found that the dataset provided by the altmetric API wrapper, and thus the PLOS ALM API, has some significant issues. The main problem is that data for most data sources is sparse - much of the data is not useful. Several of the data sources do not include historical metadata, and so simply return a <code>null</code> value. For example, the number of scholarly citations an article receives is not broken down historically. This is one reason why the radius of the bubble does not change with time. This causes bugs in the calculation of article bubble positions, as performing mathematics on a <code>null</code> value will create a <code>NaN</code> (&quot;not a number&quot;). This problem is addressed in a issue, registered on the PLOS ALM API issue tracker, acknowledging the problem and stating that work towards fixing the problem will be included in version 3.0 of the API (&quot;provide daily/monthly/yearly stats for all sources · Issue #9 · articlemetrics/alm&quot;, n.d.).</p>
<p>Of the data that is useful, the historical data is also in a format that is not conducive for the chart calculations. For each year that is given, the associated value is the number of altmetric citations received within that year. A more useful value would be the cumulative number of citations received up to that point.</p>
<p>Another problem, of lesser significance, is that the data is structured so that each article&#39;s data source list is an array. In Javascript, arrays cannot be associative - values are indexed by order within the array, not by a named key. This is problematic for this application as specific data source values must be correctly selected from the array. This could be achieved using the array&#39;s index number, however this is brittle. The API may expectantly change, reordering the array. In addition, using index numbers is difficult to debug, especially when compared to using named keys.</p>
<p>To counter these problems, filtering of the data is required. The library maps over each article in the data and converts the array to an associative object, using the data source&#39;s name as a key. This solves the second problem, allowing data sources to be selected by name, not index number. The first problem is countered by iterating through each data source and filtering out data sources with <code>null</code> historical data values. If a historical data value is found, a running total is created that converts the values to a cumulative number.</p>
<ul>
<li>Interpolation<ul>
<li>Show the algorithm</li>
<li>Why was it chosen<ul>
<li>No better model exists</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="4-5-storage-implementation">4.5 Storage Implementation</h4>
<p>This section discusses the implementation of the database within the application. Section 3.4.8 describes the proposed solution, a MongoDB database to store data returned by the altmetrics data collection module. This data will be stored against a unique key, so that it can be retrieved without having to fetch the data for a second time.</p>
<h5 id="4-5-1-mongodb">4.5.1 MongoDB</h5>
<p>As discussed in section 3.4.8, MongoDB is well suited to this application, due to it&#39;s JSON-like document format. The application generates JSON data, so it is sensible to storage this in a JSON-like manner. MongoDB is also characterised by it&#39;s relative lack of structure, similar to other so-called NoSQL databases. This is suited to the application, due to the sparsity of data that was discussed in section 4.4.3. A SQL database would struggle to define an adequate structure for data that can often be missing values. In addition, requirement 5 (see section 3.3) does not describe a overly-complex datastore. Data is merely associated with a unique key for retrieval later. This is very well suited to MongoDB.</p>
<p>MongoDB is easy to install on many operating systems, with several implementations for package managers, such as <code>apt-get</code> or <code>homebrew</code>. Unlike SQL databases, there are relatively few good tools for administration of MongoDB databases. The best MacOS client that was found is called Robomongo, and unfortunately suffers from some usability issues. To interoperate with Node.js, one of several packages from npm can be used. The most popular is the official driver <code>mongodb</code>, which provides fully featured API for accessing MongoDB data.</p>
<h5 id="4-5-2-mongoose">4.5.2 Mongoose</h5>
<p>The previous section discussed the requirement&#39;s need for lack of complexity and the suitability of MongoDB for this task. However, it was discovered some structure could be implemented for the MongoDB database in a non-time-consuming manner. The Mongoose object model library for Node.js was found to provide a much cleaner interface to MongoDB and also allow the creation of data structures within the database.</p>
<p>Mongoose is an object model map, the equivalent of an object relational map (ORM) that have been used for many years in other languages such as PHP and Ruby. Mongoose allows the creation of &quot;schemas&quot; within the database that have specified properties, unlike vanilla MongoDB where documents can have any number of properties. These are not unlike a table within a relational database, meaning that Mongoose can act as a &quot;halfway house&quot; between NoSQL and relational databases. These models provide a greater structure for the data, allowing for future work to expand on analysis of stored data.</p>
<p>A nested structure of schemas was created for the application. At the top level is the report schema. This provides a unique key to an entire data entity that was returned from the data collection API. This unique key is what is used to generate the permalink to the resource. The following code snippet shows the report schema.</p>
<pre><code class="lang-js">var ReportSchema = mongoose.Schema({
    articles: [ArticleSchema],
    createdAt: {
        type: Date,
        default: Date.now
    }
});</code></pre>
<p>Although not shown in this snippet, Mongoose will automatically generate a <code>_id</code> property for each schema that is used as the unique key. A creation date is associated with each report, that will be automatically filled with the current time when a new instance is created (called a &quot;model&quot; within Mongoose). Finally, an array of articles will be associated with a report. These articles follow an article schema. This schema holds the article metadata as shown in the following code snippet.</p>
<pre><code class="lang-js">var ArticleSchema = mongoose.Schema({
    doi: String,
    title: String,
    url: String,
    mendeley: String,
    pmid: String,
    pmcid: String,
    publication_date: Date,
    update_date: Date,
    views: Number,
    shares: Number,
    bookmarks: Number,
    citations: Number,
    sources: [SourceSchema]
});</code></pre>
<p>This shows the structuring available in Mongoose. The article&#39;s publication date will be stored as a Javascript <code>Date</code> object, while the total number of views the article receives will be stored as a number, as opposed to a string representation of the number. The sources property holds an array of data sources associated with an article. This source schema is the final layer in the application&#39;s database structure. The following code snippet shows the source schema.</p>
<pre><code class="lang-js">var SourceSchema = mongoose.Schema({
    name: String,
    display_name: String,
    events_url: String,
    metrics: Object,
    update_date: Date,
    by_day: Array,
    by_month: Array,
    by_year: Array,
    histories: Array
});</code></pre>
<p><br></p>
<h4 id="4-6-web-application-implementation">4.6 Web Application Implementation</h4>
<p>This section will discuss the implementation of the web application that was described in sections 3.4.1 - 3.4.4. The web application will combine the individual modules described in sections 4.2 - 4.5, providing a easy to use wrapper for the user. Node.js will provide the infrastructure for the application, running an Express server to serve a search form and results page, and the visualisation once altmetrics data has been collected.</p>
<h5 id="4-6-1-application-flow">4.6.1 Application Flow</h5>
<p>Figure 4.2 shows the flow of the application. Users are presented with the home page, containing the search form. When this is submitted, the search parameters are sent to the search API wrapper. Articles returned from the API are then presented to the user in a list. Users can review this list and select articles that they wish to view altmetrics data for. This list of selected articles is sent to the altmetrics data collection API wrapper. Data returned from this wrapper is stored in the database. This generates a unique key for the data, which is used to create a permalink for this dataset. Users are then redirected to the permalink where the visualisation is shown.</p>
<ul>
<li>Application flow diagram<ul>
<li>How data is moved around in the application</li>
<li>The links that they click to get to permalink</li>
<li><em>Diagram showing application flow</em></li>
<li>Differences in flow for AJAX and non-AJAX users</li>
</ul>
</li>
<li>Other notes<ul>
<li>Express<ul>
<li>Routes</li>
<li>Controllers</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The web site was constructed with a progressive enhancement approach, where pages are constructed in a layered fashion. This means that basic support for all clients is provided by HTML. This is visually enhanced with CSS, and then behaviour is added using Javascript. These enhancement steps are not required for the core functionality to work, allowing the web site to work on any browser. Each page is constructed to be semantic, giving meaning to a page without using visual design.</p>
<p>For this application, the functionality is slightly different depending on whether Javascript is enabled on the browser. For example, if turned off, the browser will make a POST request when the selected articles are submitted. When Javascript is turned on, this submit event is captured using an event listener and an XMLHttpRequest will be sent instead. This allows the client-side code to show a &quot;Loading&quot; view while the data is processed, instead of the default loading behaviour of the browser. This improved usability for clients that can use Javascript while providing the functionality to all clients.</p>
<ul>
<li>Progressive enhancement routes<ul>
<li>API/non-API</li>
</ul>
</li>
<li>Exception<ul>
<li>D3<ul>
<li>Wouldn&#39;t work without JS turned on</li>
<li>Couldn&#39;t think of a way around this</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="4-6-2-search-results-page">4.6.2 Search Results Page</h5>
<p>As described in the previous section, users are presented with a list of articles that match their search query. They can then select articles by clicking the checkboxes. This is an acceptable interface for users to designate a list of articles that they wish to view altmetrics data for. However, it became apparent that this user interaction is problematic. If, for example, a user wished to select articles from one author, then go back and select further articles from another author, their original selections were removed and forgotten by the system. Clearly, a mechanism for saving selections was required.</p>
<p>This mechanism is provided by the relatively new Javascript API, localStorage. localStorage, part of the Web Storage specification (&quot;Web Storage&quot;, n.d.), enables developers to store key/value pairs in the browser itself. The named key must be a string, and the value can be of any type, however it must be converted to a string - usually by converting to JSON - before storage. This can then be parsed to retrieve the value when needed. Storage is segmented so that only data stored on a given domain can be retrieved by the same domain. This prevents pollution by other web sites.</p>
<p>Despite it&#39;s simplicity this storage mechanism is useful for this application. When an article is selected by a user, it&#39;s key metadata is added to an array of selected articles. This is then converted to a JSON-format string and stored in localStorage, associated with the <code>articles</code> key. When the results page is loaded, the articles data is extracted from localStorage, parsed to convert back into an array and iterated through, creating new list elements for each article. This allows the user to select articles, move back to the search form and enter a new search, and retain their previous selections. Clicking the checkbox for a selected article will remove the article from the array, and update localStorage with this new data.</p>
<p>Functionality for localStorage is encapsulated in a class, providing a clean uniform interface for storing data in the browser. The following code snippet shows the Storage class&#39; <code>get()</code> function, used for retrieving data with a known key, along with the <code>parseJson()</code> call that it uses. </p>
<pre><code class="lang-js">var parseJson = function(json) {
    try {
        return JSON.parse(json);
    }
    catch (e) {
        return false;
    }
};

var Storage = {
    get: function(key) {
        return parseJson(localStorage.getItem(key));
    }
}</code></pre>
<p>As is shown in the example, encapsulating functionality in this way reduces complexity and improves readability by providing a uniform interface.</p>
<p>As discussed above, localStorage is part of the Web Storage specification. This is still in the Editor&#39;s Draft phase of World Wide Web Consortium (W3C) specification process, a very early stage. Browser support is very good, with all major modern browsers providing full support, with the exception of the Opera Mini mobile browser (&quot;Can I use...&quot;, n.d.). Despite this, older browsers may not be able to support localStorage, and will throw errors if used. For this reason, a progressive enhancement approach is taken, where basic support is provided for all browsers, and more complex behaviours are layered on top of this for more up-to-date browsers. This approach has been applied to all client-side Javascript in the application, but this is most evident when looking at the localStorage support. The following code snippet shows the <code>isSupported()</code> method within the Storage class.</p>
<pre><code class="lang-js">var Storage = {
    isSupported: function() {
        try {
            localStorage.setItem(&#39;supported-test&#39;, &#39;yes&#39;);
            localStorage.removeItem(&#39;supported-test&#39;);
            return true;
        }
        catch (e) {
            return false;
        }
    },
};</code></pre>
<p>This method is designed to be called when the Storage class is used for the first time. It will test to if localStorage is supported by the user&#39;s browser. By wrapping usage of the Storage class in a test for support, a progressive enhancement approach is taken - the more complex behaviour is only used if the browser supports localStorage. </p>
<h5 id="4-6-3-bower">4.6.3 Bower</h5>
<p>As discussed in section 3.4.3, the package manager Bower will be used to download important stylesheets from the Twitter Bootstrap project. Bower is an increasingly popular tool for developers creating web sites using third-party libraries and stylesheets, such as Bootstrap and jQuery. Through the use of a command line tool, packages can be more easily downloaded and maintained. By running a <code>bower update</code> command, latest versions of the required packages will be downloaded and installed in the project.</p>
<p>Another advantage of Bower is that it removes unnecessary libraries from project repositories, making them somewhat cleaner. Required packages are listed in the <code>bower.json</code> file in the root of the project. This allows the project maintainer to remove the libraries from the repository, and require users of the project to run a Bower command that will download libraries and insert them into the project.</p>
<p>There is, however, a major drawback with Bower. Downloaded packages can only be inserted into a single directory. By default this is set to the <code>bower_components</code> directory, although this can be changed in the <code>.bowerrc</code> configuration file. This prevents developers from having full control over how their project is structured. For example, if Bootstrap and jQuery are required, both a placed in the <code>bower_components</code> directory. They cannot be placed in separate directories - a sensible structure that is used by many is <code>public/css</code> for stylesheets (such as Bootstrap) and <code>public/js</code> for Javascript libraries (such as jQuery). </p>
<p>These files could be moved manually, however the purpose of Bower is to automate such tasks. Instead, a Grunt plugin, called grunt-bower-task, is used that will run Bower commands then move libraries to their desired directories. This plugin can be configured to layout libraries by type (Javascript or CSS), copying downloaded libraries from the <code>bower_components</code> directory to the relevant directory in <code>public</code>. For this application, this plugin will be run when the <code>grunt build</code> command is executed. It is frustrating that Bower does not provide this functionality, and an external plugin must be used to perform tasks that seem somewhat basic.</p>
<ul>
<li>Handlebars</li>
</ul>
<p><br></p>
<h4 id="4-7-summary">4.7 Summary</h4>
<p>This chapter describes the implementation of an application to examine how altmetrics change over time, which is based on the design discussed in Chapter 3. Implementation of the altmetrics data collection API wrapper is described in section 4.2, and the search API wrapper is described in section 4.3. The application&#39;s visualisation and storage implementations were discussed in sections 4.4 and 4.5 respectively. Finally, implementation details of the web application were described in section 4.6.</p>
<p>Chapter 5 will look at testing performed on the application, and the results of these tests. In addition, the final application will be evaluated against the project&#39;s requirements. Implementation details discussed in this chapter will have a large effect on the outcome of this evaluation.<br></p>
<h3 id="chapter-5-testing-and-evaluation">Chapter 5 Testing and Evaluation</h3>
<h4 id="5-1-introduction">5.1 Introduction</h4>
<p>Chapters 3 and 4 describe the design and implementation of an application to assist researchers investigate how altmetrics change over time. In this chapter, the methods used for testing the application are presented in section 5.2. The results of these tests show that the application has mixed performance for the project requirements.</p>
<p>Section 5.3 will evaluate the application using the test results. This evaluation will discuss how well the application performs against the requirements.<br></p>
<h4 id="5-2-testing-summary">5.2 Testing summary</h4>
<p>In Chapters 3 and 4, an application was described that is designed to assist researchers examine how altmetrics change over time. In this section, the methods by which this application was tested are described. Three major tests were performed; section 5.2.1 describes the unit tests that were created during the design and implementation phase. Section 5.2.2 describes the usability tests that were performed after the application was completed. Finally, section 5.2.3 discusses the methods and results of further usability tests and acceptance tests from a sample of altmetrics researchers.</p>
<h5 id="5-2-1-unit-testing">5.2.1 Unit Testing</h5>
<p>Section 3.2 describes the Test Driven Development (TDD) approach taken throughout the project. This approach focusses development on testing, creating a through design mentality. The approach involves designing and creating a test for each individual feature of the code before the code is implemented. The focus on testing each individual feature of the code is called unit testing. Unit testing, therefore can be considered a form of functional testing where verification of specific actions is performed. In some extreme cases, each line of production code is tested using a unit test, although this is obviously impractical for most applications.</p>
<p>Once the test is created, the test is run, to ensure that the test will fail. If the test does not fail, then the feature already exists, or alternatively, the test is defective. The feature is then implemented, according to the parameters set by the test. The test is run again, and the feature&#39;s implementation must pass the test. </p>
<p>This approach forces the developer to consider the design of the implementation, as well as any edge cases that could break this design before any production code is written. This design-heavy mentality was found to be extremely beneficial when creating production code, as potential problems are considered ahead of time and factored into the implementation.</p>
<p>Another benefit is that once tests are created, refactoring poorly written or inelegant code becomes much easier. Without unit tests, refactoring was dangerous and time-consuming as newly written code had to be tested manually to ensure that it still produces the expected behaviour, and no new bugs were introduced. This often takes time and is prone to error, as it is easy to miss implementation details when performing tests manually. With unit tests however, the &quot;fear factor&quot; of potentially breaking working code is removed, leaving the developer to focus on improving the code during a refactor. For this reason, TDD reduces the imperative that production code must be perfect first time. Revisiting the code later is acceptable as long as tests are available. This is one of the main reasons why TDD has been adopted into the agile development movement.</p>
<p>However, unit testing is not without it&#39;s problems. The main issue encountered when following the TDD approach is that, while refactoring production code is encouraged through the use of tests, refactoring tests, or the refactoring output that the tests are validating against remains potentially dangerous and time-consuming. Making large changes to the underlying concept of the feature or changing the output of a feature still need to be tested manually, as the tests will need to be rewritten to accommodate the change. For example, the altmetrics data collection API wrapper was modified after tests were written, entirely changing the structure of how the wrapper returned results (see section 4.2.2). This required each test to be converted to the new structure, potentially losing some test coverage. Unfortunately, there is no good solution to this problem. Restructuring a feature entirely should be treated as a rewrite, and thus, tests should also be rewritten. Maintaining support for previous details of the implementation between these modifications remains part of the developer&#39;s duties.</p>
<p>For TDD to take place, a testing infrastructure must be created. For this project, an assertion library, that will allow assertions about output to be validated within a test, and a test runner, that will run the supplied tests within it&#39;s environment. Section 3.4.10 discusses the usage of the Mocha and Chai libraries for this purpose.</p>
<p>The Mocha library is a test runner that will run tests within an environment, reporting the results back to the user. These results can be in the form of printing to the command line (by running the <code>mocha</code> command) or creating a web page showing the results. A test is considered to have failed if an error is thrown at any point during the test. The following code snippet shows a Mocha test used in the altmetrics data collection wrapper.</p>
<pre><code class="lang-js">describe(&#39;ALM&#39;, function() {
    describe(&#39;General fetch&#39;, function() {
        it(&#39;should take a DOI as the first argument&#39;, function() {
            // Test code
        });
    });
});</code></pre>
<p>Mocha tests are structured so that tests can be grouped according to what they are testing. The <code>describe()</code> method is used to group tests in this manner. Blocks can be nests up to two layers, each layer increasing in specificity. In the above example, the first <code>describe</code> block will group all tests for the altmetrics data wrapper. The second block will group all tests that relate to the fetching functionality of the module. Other tests at this same level include tests for accepting options and error handling tests.</p>
<p>Within each of these blocks a series of tests can be run. These are declared as callbacks to the <code>it()</code> function. This function takes a human readable description of the test as the first argument, which will shown to the user if a test fails. If set up or tear down is required, Mocha provides <code>before()</code>, <code>beforeEach()</code>, <code>after()</code> and <code>afterEach()</code> functions for this purpose.</p>
<p>Some tests require the environment to handle asynchronous behaviour. Both the API wrappers use asynchronous calls, and would fail their tests as Mocha will time-out the tests after 200 milliseconds. Responses from the respective APIs take much longer than this. Tests for these libraries must therefore must use the <code>done()</code> callback that is provided by Mocha. Once a test is considered to have been completed asynchronously, the <code>done()</code> callback should be called, which will end the test.</p>
<p>The Chai assertion library provides assertions to the testing environment. Assertions are the core of testing logic.They provide a mechanism for the developer to add predicate (true or false) statements that should always be true. If not true, an error will be raised. Chai&#39;s API is very readable, often considered a best practice for testing, as discussed in section 3.4.10. Sentence-like assertions can be created using the <code>expect()</code> method and the &quot;language chains&quot;.</p>
<p>Coverage, a term often used in relation to testing, refers to the amount of production code that is &quot;covered&quot; or tested by test code. It is used as a metric for judging the quality and extensiveness of test code. Coverage of the altmetrics data collection and search wrappers is very good, with a large percentage of production code tested. The code coverage tool Istanbul was used to calculate coverage metrics for both modules.</p>
<table>
<thead>
<tr>
<th>Module</th>
<th>Statements</th>
<th>Branches</th>
<th>Functions</th>
<th>Lines</th>
</tr>
</thead>
<tbody>
<tr>
<td>Altmetrics data collection</td>
<td>93.85%</td>
<td>84.85%</td>
<td>100%</td>
<td>93.85%</td>
</tr>
<tr>
<td>Search</td>
<td>96.97%</td>
<td>90.48%</td>
<td>100%</td>
<td>96.97%</td>
</tr>
</tbody>
</table>
<p>Coverage of the web application part of the code base is poor. Time restrictions meant that few tests were written while developing this part of the application. However, this may not seriously affect the project, as much of the functionality of the web application extends the well tested Express framework, or Mongoose object-model map.</p>
<p>One area that may be affected is the visualisation library, which was not extensively tested. This is because the library developed from experimentation with the D3.js library, rather than as a specific project designed to meet the requirements. Testing after a large amount of production code has been created takes time, and is more prone to errors. However, if the library is to be released for usage by third parties, unit tests should have been created.</p>
<p>An analysis of the results of unit testing is difficult, as the tests were developed to assist during the implementation phase. They are not designed to evaluate the application against the requirements (see section 3.3). By following the TDD approach, tests were written before implementation, so production code can be created to pass the tests. Production code is not considered to be complete until the tests pass. Unless tests are incomplete - see above for discussion of code coverage - no unit tests will fail - the TDD approach guarantees this.</p>
<h5 id="5-2-2-usability-testing">5.2.2 Usability Testing</h5>
<p>A usability test was performed to analyse how well the application meets requirement 6 - the application must be easy to use (see section 3.3). A small sample of users were given a task to perform using the application. The application was running on a locally installed server, using a development machine. They were then observed during the completion of the task. Task performance was evaluated for speed, &quot;easiness&quot; of the task and comprehension of steps taken to complete the task. After observation was completed, feedback was gathered. This gives a qualitative assessment of the application.</p>
<p>Volunteers were sourced from friends and family, to create a sample of 10 users. This is a fairly small sample size, and the sample was not randomised, meaning that some bias may be introduced. To mitigate this risk, users were shown the application for the first time during the test, and were instructed to give objective feedback. Observers who were familiar with the system were discouraged from interfering with the users. This prevents biased results, caused by a &quot;guided walkthrough&quot; of the system, rather than a true usability test. None of the users in the sample are involved in altmetrics, and therefore the result produced by the visualisation could not be analysed. Instead, usability of the application was assessed.</p>
<p>The task set for each user was to create a visualisation for a given set of articles. This task was chosen because it requires the use of every part of the application, from the use of the web application to use the search from, then the search module to find the articles in question, the data collection module to fetch altmetrics data, the database to store this data, and finally, the AlmChart library to view the visualisation. This task also closely matches the ultimate goal of the project - to create tools that for altmetrics visualisation - and therefore provides a useful test. To complete the task users were given a list of article authors and titles that were to be included in the visualisation. Most of these articles could be found in a single search, however, some required the user to make usage of the storage of articles (see section 4.6.2), and create a new search to add articles to the list of selected articles.</p>
<p>After the task users were asked several short questions to determine whether they understood how the application worked. They were asked a range of questions including if they understood that articles were stored when selected and that the generated URL would act as a permalink for the current visualisation. In addition, they were asked whether they understood the temporal aspect of the visualisation and if they were able to compare different altmetric data sources using the selection boxes. Finally, they were asked if they had any suggestions for improving the application.</p>
<p>Results of this test were mixed. All 10 users in the sample were able to complete the task relatively quickly. However, most users were confused by the storage of selected articles.</p>
<p>The average time taken to complete the task was 2 minutes, 11 seconds. The fastest time was 1 minute 39 seconds, and the slowest 3 minutes 40 seconds. These results show that the application can be used reasonably quickly to produce a visualisation. Most users found that searching for an article and generating it&#39;s visualisation was simple. 7 of the sample responded that they understood that the generated URL would act as a permalink for the current visualisation. These results are encouraging, as they suggest that the core usability of the system is good.</p>
<p>However, nearly all users struggled to understand the article storage concept. 2 of the sample asked the observer for assistance in completing this part of the task. All users undertaking this task took longer to complete than those completing the simpler task. This suggests that the interface for selecting articles must be improved to reduce confusion.</p>
<h5 id="5-2-3-acceptance-test">5.2.3 Acceptance Test</h5>
<p>In addition to the usability test described in section 5.2.2, a acceptance test was performed on a number of members of the altmetrics research community. This test involved setting up the system as a live web application and letting researchers perform their own tests. Researchers were then contacted by email to discuss the artefact and gather feedback. This approach cannot accurately be termed a &quot;test&quot;, as the researcher&#39;s testing was unstructured and unsupervised. However, this was unavoidable, due to time and distance constraints.</p>
<p>The sample size for this test is very small, with only 3 researchers participating. There are several reasons for this; the first being the overall size of the altmetrics research community. Altmetrics is a relatively new field of study, and has yet to gain widespread interest from the larger population of researchers. In addition, spreading word to other altmetrics researchers is somewhat difficult, especially from a previously unknown and untrusted source. Despite these difficulties, three respected scholars were able to provide feedback.</p>
<p>Unfortunately, the results of this test were poor. The researchers found that the visualisation was a good method for comparing altmetrics data sources against each other and against traditional citation metrics. However, all three researchers criticised the use of interpolation on the visualisation, stating that the model used to calculate interpolated values is assumed and has little evidence to support it. They pointed out that no research has been done that validates the linear interpolation model used by the application, and thus the visualisation has reduced scientific value. This result means that the application fails to reach it&#39;s primary goal - to assist in developing the study of altmetrics and how they change over time. However, it is possible to reach some positive conclusions of this test. The researchers suggested that the visualisation tool may be useful as a starting point for this research. A lack of evidence supporting the interpolation model does not necessarily disprove the model, it only requires more study. </p>
<p>There are other positives to take from this test. The application was commended for it&#39;s ease of use, and openness. Several comments believe that the system&#39;s simplicity will be beneficial to the altmetrics movement, as anybody with an interest in the subject can get an understanding very quickly. They found that it was useful for comparing different data sources, if the interpolation was ignored.</p>
<p>In addition, the open nature of the project was praised, with all three researchers commenting that the approach is well suited to the altmetrics community. The permissive license used for the application means that the problems raised above could potentially be fixed, through research, and patches contributed back to the project. In addition, the modularly of the application means that individual parts, such as the altmetrics data collection and search API wrappers, could be reused in other altmetrics research. </p>
<p><br></p>
<h4 id="5-3-evaluation">5.3 Evaluation</h4>
<p>Chapter 1 introduced the concept of altmetrics, and pointed the to problem of a lack of research into how altmetrics changes over time. Chapter 2 gave a deeper look at why altmetrics are needed, what altmetrics are and potential problems with altmetrics. The chapter concluded that tools are needed to assist researchers in studying how altmetrics change over time. Chapter 3 described the design of such a tool, detailing the methodology and requirements of the project needed to develop this tool. A proposed solution to the problem was also described in this chapter. Chapter 4 described the implementation of this proposed solution, giving development details of how the artefact was created. Problems encountered during the implementation were discussed, and the solutions to these problems described.</p>
<p>Testing for this project was discussed in section 5.2, which demonstrated that the artefact meets some but not all of the requirements. Testing with altmetrics researchers found that the application fails to meet the main goal of the project, however with more research it could be improved or adapted to reach this goal. The application also has problems with some confusing user interfaces, but is overall effective at creating altmetrics visualisations and is easy to use. In this section, the implementation will be evaluated and issues in the underlying technologies that the implementation has highlighted will be discussed.</p>
<h5 id="5-3-1-requirements-review">5.3.1 Requirements Review</h5>
<p>In this section, the implementation will be evaluated against the requirements (see section 3.3). Each requirement will be examined to identify the extend to which each has been achieved. In addition, a reflection on each requirement will be made, looking at their relevance for future work. Each of the requirements is reintroduced and discussed in turn.</p>
<h6 id="requirement-1-visualisation-of-altmetrics-data-sources">Requirement 1. Visualisation of altmetrics data sources</h6>
<p>This requirement describes the need for a visualisation that will allow for comparison of altmetrics data sources for a set of scholarly articles by altmetrics researchers. Section 2.6 found that research is often performed by comparing altmetrics data sources against each other and against traditional citation counts. This must be facilitated by the artefact. The visualisation must have some mechanism for associating an article with it&#39;s visual data.</p>
<p>The application achieves all significant details of this requirement. The AlmChart library produces a bubble chart visualisation that enables researchers to compare altmetrics data sources for a number of different articles quickly (see section 4.4). The data sources used to generate the chart can be switched at any time, so that different sources can be compared. The chart can show a large number of articles simultaneously, again aiding comparison. Finally, the tooltips shown when the user hovers their mouse over an article&#39;s bubble satisfy the requirement for a mechanism for associating visual data with the original article.</p>
<p>These features mean that altmetrics data can be compared quickly and easily, meeting the needs of this requirement. This finding is supported by the evidence gathering during the testing phase. Both the usability test and the acceptance test with researchers found that the visualisation meets requirement 1.</p>
<h6 id="requirement-2-visualisation-of-altmetrics-changing-over-time">Requirement 2. Visualisation of altmetrics changing over time</h6>
<p>For this requirement, the application must provide some way for the visualisation described in requirement 1 to additionally analyse how altmetrics data changes over time. This requirement is a significant part of the overall goal of the project - to create tools to aide researchers examining how altmetrics change over time. The requirement specifies that the chart must use a time dimension, for which altmetrics data is taken at different time periods. The user must be able to control the passage of this time dimension. </p>
<p>For the application these time periods were chosen to be every year, as other time periods were found to be either unreliable or overly complex to visualise (see section 4.4). A animation was created that will start with the altmetrics data shown for the earliest year, and will move the article bubbles between each year, until reaching the latest year. An interpolation algorithm was created to calculate values between years, smoothing the animation&#39;s movement. The user is able to control this animation by hovering over a year label that was added to the chart. Further to left of the label, the earlier the data, and further to the right, the later the data.</p>
<p>The visualisation library mostly fails to meet this requirement. The interpolation algorithm was criticised by altmetrics researchers in the acceptance test. The approach to this part of the visualisation library was based on a unsupported assumption, and therefore cannot be found to have meet the requirement. Researchers believe that more study of how altmetrics changes over time is required before modelling these relationships can be accurately achieved. However, because of the open source license applied to the visualisation library, it is believed that once this research has been completed, the source can be modified to reflect the model and the application will meet the requirement and become more useful. The usability testing supports this, as non-researchers found the temporal aspect of the chart easy to understand, suggesting that the visualisation may be useful in areas outside of science.</p>
<h6 id="requirement-3-scholarly-article-search">Requirement 3. Scholarly article search</h6>
<p>The visualisation described in requirements 1 and 2 shows altmetrics data for multiple articles, and this requirement states the user&#39;s need to be able to identify which articles are to be visualised. The requirement states that a search form must be implemented that allows users to search for articles by title, author and other common article metadata. The requirement states the need for a mechanism for uniquely identifying articles within the application, suggesting that the DOI system could be used.</p>
<p>The application provides search functionality through the PLOS Search API, allowing search for any articles in the PLOS library (see section 4.3). This restriction was implemented due to choices made for requirement 4. A uniform interface for the API was provided, through the creation of a search wrapper. This takes search parameters from the search form, and creates a request to the search API. This significantly cleans up the codebase, encapsulating search API logic into one library.</p>
<p>This requirement is fully met by the artefact. The search wrapper allows the application to take search parameters from the user, process them and return a list of matching articles, including their metadata. The system can handle single articles or a set of articles, where each article is uniquely identifiable through it&#39;s DOI. This evaluation is supported by the test results. Users in the usability and acceptance tests were able to search for and find articles of interest. The search module was released separately and can be used outside of the main artefact, satisfying the requirement. Unit tests for the module were also released, allowing others to verify the application&#39;s functionality.</p>
<h6 id="requirement-4-altmetrics-data-collection">Requirement 4. Altmetrics data collection</h6>
<p>Similar to the previous requirement, the visualisation system within the application requires altmetrics data to generate the chart and show how data changes over time. The mechanism for collection of this data is described by this requirement. The requirement states that data collection from the original source is out of scope, and a altmetrics data API provider should be used. The system must take a list of selected article DOIs, request altmetrics data related to these articles from the API, and return the relevant data.</p>
<p>A altmetrics data collection API wrapper was developed to meet this requirement. This, much like the search wrapper, creates a uniform interface for the API. The PLOS Article Level Metrics (ALM) API was chosen for the application for several reasons, discussed in detail in section 3.4.5. Requirement 10 will evaluate this choice. This API can only provide altmetrics data for articles in PLOS journal, and thus only PLOS articles can be used for the application. This is the reason for the restriction on search APIs, discussed above. The wrapper takes a set of article DOIs, creates a request to the API, and processes and parses the response before returning the altmetrics data. Again, similarities with the search wrapper can be drawn. The module uses best practices to encapsulate data collection logic within itself.</p>
<p>This requirement is also fully met by the application. The data collection wrapper allows the application to perform the tasks laid out in the requirement, creating a cleaner, less verbose codebase. This is supported by the test results. Both the usability and acceptance tests found that users were able to collect altmetrics data for articles of interest. This functionality is easy to use, by altmetrics researchers and non-scholars. The wrapper module was released separately so it can be reused by others, along with unit tests. This satisfies all parts of the requirement.</p>
<h6 id="requirement-5-storage-of-results">Requirement 5. Storage of results</h6>
<p>This requirement states that the system must provide some way of storing altmetrics data so that users can revisit the generated visualisation later. This is to prevent unnecessary requests to the data collection API to retrieve data for a second time. The requirement states that a unique identifier must be generated when the data is stored, that is used to create a permalink to the resulting visualisation.</p>
<p>The application uses a MongoDB database to store altmetrics data after retrieval from the data collection wrapper. MongoDB stores &quot;documents&quot; in a JSON-like format - well suited to this application as the data collection API returns a JSON response. The Mongoose library is used to provide some database structure and handling querying and updating functionality. When the data is stored, MongoDB will generate a unique key for the data - similar to a primary key in SQL databases. This is used to create a permalink for the data and it&#39;s resulting visualisation.</p>
<p>This requirement is fully met by the application. The data is stored after it is retrieved from the data collection API, generating a unique identifier. This is used to create a permalink for the data. This is, again, supported by test results, with both tests finding that the generated permalink was a useful feature for the system to have.</p>
<h6 id="requirement-6-easy-to-use">Requirement 6. Easy to use</h6>
<p>This requirement states the artefact must be easy to use by users who are not familiar with altmetrics terms as well as those not in the technology industry. Therefore, a understandable user interface must be developed, that does not use obscure altmetrics-related vocabulary. A good visual design will help to differentiate the project from other scientific software.</p>
<p>The application uses stylesheets from the Twitter Bootstrap project to achieve a simple, well designed visual design. Attempts were made to improve the user interface, for example by storing selected article locally in the browser. This allows users to select articles of interest, then perform another search while maintaining the selection of previous articles.</p>
<p>The application does not meet all of this requirement. Testing found that users were confused by the selected article storage implementation. It took longer for users to complete tasks when using this functionality. It is possible that an improved interface could be created, by providing more affordances - inherent properties - that explain how article storage works. However, some of this requirement was met by the application. Researchers in the acceptance test commented on the clean visual design used for the artefact.</p>
<h6 id="requirement-7-open-source">Requirement 7. Open source</h6>
<p>For this requirement, the application code must be published under a permissive open source license. The requirement states the need for transparency in the calculation of altmetrics. The requirement defines a open source license as one certified by the Open Source Initiative (OSI). </p>
<p>As described in sections 4.2 and 4.3, the search and altmetrics data collection functionality of the application would be developed as separate modules. This means that they can be reused by external developers in other projects, encouraging the development of altmetrics research. For this to be achieved without unnecessary legal difficulties, the modules would be released under an open source license. The MIT license was chosen, allowing developers to reuse module code in proprietary and non-proprietary software, provided copies include the text of the license. This means that original development credit is retained, but permissive reuse is allowed.</p>
<p>Along with the modules described above, the entire artefact will be published under the MIT license. This is of less use to altmetrics developers as this represents an entire project. It is inherently harder to reuse a fully complete package than a separated module. However, as is stated by the requirement, transparency is a core feature of altmetrics. Therefore the methods taken to generate the visualisation should be published.</p>
<p>In addition, source for both the modules and the final artefact is hosted on GitHub, meaning that patches can be accepted. This will encourage further development to improve the software.</p>
<p>The project meets all points of the requirement, and some cases exceeds it. Source code for the application is published under a OSI approved open source license, and will accept further development patches. By separating the modules from the core project, reuse is further encouraged, as the inherent structure of a module allows it to be &quot;plugged in&quot; anywhere. This is supported by evidence from the acceptance, where altmetrics researchers showed some interest in reusing the data collection module in a future project.</p>
<h6 id="requirement-8-suitability-of-node-js-for-this-project">Requirement 8. Suitability of Node.js for this project</h6>
<p>For this requirement, the project will assess whether Node.js was the correct choice for this application. Node.js was chosen as the server side platform to control data flow between the modules, execute the modules themselves and serve the web application. The requirement states that the application must be built quickly and easily using this platform. In addition, Node.js must encourage the creation of a well structured and efficient artefact.</p>
<p>As described in section 4.6, Node.js was used for the server side implementation of the web application. This section of the project combines the other sections together, controlling their interactions and providing an interface between these sections and the user. The search and data collection modules were constructed as Node.js packages, and published on the built-in package repository, npm. In addition, Node.js acted the web server for the application, serving web pages and accepting HTTP requests.</p>
<p>The project meets this requirement fully, finding that Node.js is a suitable platform. Node.js&#39; focus on modularity is crucial to the approach taken by the project. Without the ability to include modules quickly and without difficulty, the project may not have been able to release the search and data collection packages for reuse by altmetrics researchers - a key part of this project (see requirement 7). The server created for this project was able to effectively link the disparate sections that make up the application, although the asynchronous nature of Node.js did lead to some difficulties in creating clean code. It was also found that, unlike frameworks on other languages, Node.js take a un-opinionated approach to the application structure. This has benefits and drawbacks - it is useful to develop custom structures, however in some circumstances, more boilerplate code is required to create a basic MVC structure. Despite this, the project found that Node.js provides a platform for creating well structured and efficient servers.</p>
<h6 id="requirement-9-suitability-of-d3-js-for-this-project">Requirement 9. Suitability of D3.js for this project</h6>
<p>This requirement, similar to the previous requirement will assess whether the D3.js library is suitable for this project. D3.js was used as the framework for creating the visualisation (see section 4.4). It allows manipulation of the browser DOM through the use of data. The requirement states that for this library to be considered successful, it must provide a stable and efficient platform for creating visualisations such as the one described in requirements 1 and 2.</p>
<p>As discussed in section 4.4, D3.js was used as the framework for creating the visualisation. The AlmChart library depends on D3.js, using it to create the SVG elements that make up the chart. The framework requests data from the server asynchronously, which is then transformed to clean the data, before being used to set up axes and finally draw the bubbles of the visualisation.</p>
<p>This requirement is met fully by the project, finding that D3.js is a suitable framework for creating complex visualisations. D3.js was found to be able to efficiently create the visualisation, even with large amounts of data. The visualisation is a relatively complex custom chart, that can be easily created using D3.js.</p>
<h6 id="requirement-10-assessment-of-existing-altmetrics-providers">Requirement 10. Assessment of existing altmetrics providers</h6>
<p>For this requirement, the project will compare altmetrics data APIs, to find the most suitable for this project. The requirement states that the API must provide historical metadata to allow the visualisation to show how altmetrics change over time. The API must also be able to provide data for a reasonable number of articles and altmetric data sources. Without these, it would be difficult to generate a visualisation with much relevance and would be difficult to compare data sources.</p>
<p>The PLOS Article Level Metrics (ALM) API, the ImpactStory API and the Altmetric.com API were researched to assess which would be the most suitable. Section 3.4.5 includes a table that breaks down the key features of the three APIs that were considered. As shown in this table, the PLOS ALM API was chosen as it meets all three of the key requirements. Crucially, the historical metadata that is included in the API response is well suited to a series analysis, such as one used in the visualisation. The Altmetric.com API also provides historical data, but it is structured in such a way that is not conducive to a yearly analysis. All three APIs provide a large number of altmetric data sources, so this was not a factor in the decision. Finally, the PLOS API does not perform as well as the other APIs at providing data for a large number of articles - it can only provide data for articles in PLOS journals. However, PLOS journals are very prolific, and so the pool of potential papers remains high.</p>
<p>The project meets all parts of this requirement, finding that the PLOS ALM API is the most suitable API for providing altmetrics data. All three of the major altmetrics providers were considered in this analysis, and so it is likely that the best possible solution was found.</p>
<p><br></p>
<h5 id="5-3-2-artefact-review">5.3.2 Artefact Review</h5>
<p>This section will evaluate the application and review the important issues highlighted in during the implementation phase. Each major section from chapter 4 will be evaluated to review whether good decisions were made in this phase.</p>
<h6 id="5-3-2-1-altmetrics-data-collection-api-module">5.3.2.1 Altmetrics Data Collection API Module</h6>
<p>The data collection API wrapper (see section 4.2), was implemented as a package for the Node.js platform. The package acts as a wrapper around the PLOS Article Level Metrics (ALM) API, providing a uniform interface to the API and encapsulating logic within itself.</p>
<p>The best practices for creating a Node.js package were followed when creating the data collection wrapper. This produces a good internal structure for the module, where all related logic is encapsulated within the module. Prototypical inheritance was used to create a class with methods and properties. Instantiating this class would set up the module, and listening to events emitted from the module would return results or errors. Handling the creation and sending of the request, parsing response and returning results is performed within the class. This results in a cleaner, more readable and less verbose codebase, as this logic does not need to be recreated multiple times. This has additional benefits - an update to the API will only affect code within this package. This also means that others can reuse the package without making changes to the core. The package can be found on the Node.js package manager, npm, under the name nodealm. Ultimately, the decision to adopt a prototypical inheritance style structure benefited the project as the code was cleaner and the ability to redistribute the package fits with the goals of the project.</p>
<p>When following these best practices, a lot was learned about creating modules with this structure. This involved completely rewriting the data collection wrapper with a improved implementation, after it was realised that using a constructor to create configuration for the rest of the module was useful, and that a cleaner callback could be constructed. An event based structure was adopted (see section 4.2.2), implementing these solutions. The prototypical inheritance style also meant that deeper understanding of the <code>this</code> keyword was acquired, as well as the important <code>bind()</code> and <code>call()</code> functions.</p>
<p>An area of improvement that could be tackled in future versions of the package would be validation of input. The module accepts a list of Digital Object Identifiers (DOIs) that represent articles for which data is to be found. The application currently does not validate this input to check if the list contains invalid DOIs. If this was implemented, an error would be thrown, to be handled by the developer.</p>
<p>As discussed in section 4.2.3, during the implementation of the wrapper, it was discovered that the API can produce errors in certain circumstances. If querying the API for many articles including their historical metadata, a error response will be returned, however this is sometimes inconsistent. A workaround was found that involves appending a list of required data sources to the request. This would consistently produce the desired results, however it may somewhat inconsiderate to the maintainers of the API. Producing a large amount of data, such as in this request, may tax the servers and increase expenses for the providers. To mitigate this, a bug report was issued to the API maintainers, to make them aware of the situation. In addition, the problem may not be relevant, as it not expected that the application will receive heavy usage, and so few of these requests will be sent. Despite these problems, the PLOS ALM API is the best choice for the application, as was assessed in section 3.4.5. These findings mean that no other provider currently gives historical metadata of the type that is used by the visualisation, and therefore switching based on a inconsistent bug that can be easily worked around is a foolish choice.</p>
<p>Overall, the implementation of this module reaches expectations. Desired functionality is provided in a clean, encapsulated manner. This allows the package to be extracted from the main application for reuse in other projects. The package meets two goals of the project - to be able to collect historical altmetrics data for visualisation, and to provide tools for researchers for examining the temporal aspect of altmetrics.</p>
<h6 id="5-3-2-2-search-api-module">5.3.2.2 Search API Module</h6>
<p>The search module (see section 4.2), was implemented as a Node.js platform, in the same way as the data collection module evaluated above. The module provides an interface to the PLOS Search API, giving a uniform set of methods for searching for articles.</p>
<p>The search wrapper shares many characteristics with the data collection wrapper, as the functionality provided by both is to some extent similar. The search module follows best practices, uses a prototypical inheritance approach to creating a class and uses a event based structure. The major difference is the search module will perform validation on the search parameters that are input to the module. The validation checks to ensure that no unknown parameters are passed in the request to the API. As discussed in section 4.2, this means that errors can be thrown by the constructor, and thus must be wrapped in a try/catch block.</p>
<p>As discussed in section 3.4.6, the PLOS Search API was chosen due to the decision to use the PLOS ALM API for altmetrics data collection. The latter API can only provide altmetrics data for articles in PLOS journals, therefore a search engine that will only provide results from PLOS journals would seem to be a practical choice.</p>
<p>There are some further arguments against this decision. All articles published in PLOS journals are under a open access model - the article is free for anyone to access and read. This may introduce some bias into analysis of altmetrics data, as open access articles often show much higher altmetrics performance than closed models. However, previously, several respected altmetrics studies have solely used open access articles. In addition, as all articles visualised by the system will be open access, this bias can be accounted for in analysis. </p>
<p>Another argument against the PLOS Search API is that it does not correctly implement the HTTP protocol. A key component of this protocol is that responses are given codes that identify the type of response. For example, successful responses have the status code &quot;200 OK&quot;. There are several common error responses, including &quot;404 Not Found&quot; and &quot;400 Bad Request&quot;, which indicate that the requested resource cannot be found by the server and that the request is badly constructed respectively. These should be used if an error processing the response is raised. Unfortunately, the Search API does not this, returning a 200 response even if an error is generated. The only way to distinguish successful and error responses is the response body, which contains an error message. This means that message bodies must be parsed by the search module to check for potential errors.</p>
<p>Despite these criticisms, the PLOS Search API still reaches the requirements of the application - PLOS journals publish a large volume of papers, providing a large sample, and search results include article DOIs (see section 5.3.1, requirement 3). For this reason, the decision to use the Search API can be considered the most optimal. This, along with the well structured and efficient wrapper, mean that the implementation of this functionality reaches expectations. Similar to the data collection module, this module meets two goals of the project - to be able to collect altmetrics data for visualisation, and to be able to release tools for researchers to further investigate altmetrics.</p>
<h6 id="5-3-2-3-visualisation">5.3.2.3 Visualisation</h6>
<p>The visualisation of altmetrics data was implemented as a library that depends on the D3.js framework (see section 4.4). The library creates a bubble chart using SVG elements, with 4 axes, each article is represented by a circle. The x- and y-axes represent selected altmetric data sources that can be controlled by the user. For example, if the Twitter data source is selected, then it&#39;s distance along an axis represents how many citations the article has received on Twitter. The size of the circle represents the total number of scholarly citations the article has received. Finally, the fourth axis represents the current year. As this axis advances, the data used for the x- and y-axes is transformed based on the current year. Therefore, as time advances, the bubbles on the graph move based on how much they changed over time.</p>
<p>This part of the codebase was also developed using a prototypical inheritance architecture. An AlmChart class was created that encapsulates the methods required to set up, construct and animate the chart. The library uses the Asynchronous Module Definition (AMD) to define itself as a module. This means that the library can be redistributed as a standalone Javascript library, for reuse in other projects. This approach is similar to that taken by the data collection and search modules, except without the module functionality provided by Node.js, which is instead provided by the AMD structure.</p>
<p>Results of the usability test found that the visualisation is easy to use. This is a key factor in the successful implementation, as the underlying altmetrics data is voluminous and complex. Creating a chart that can draw knowledge from this data is somewhat difficult. Therefore, test results that show that the visualisation is easy to use and understand can be considered a success.</p>
<p>Section 5.2.3 details the acceptance test that was performed on the application. This test found that altmetrics researchers criticised the use of an assumed interpolation model in the visualisation. This allowed for calculation of data values between given years, smoothing the animation. This means that the visualisation has much reduced value to altmetrics researchers. With hindsight, it seems obvious that implementing such a model into scientific software without research was a poor decision. </p>
<p>However, there are some positives to take from this part of the project. The acceptance test found that the comparison aspect of the visualisation is still useful. Being able to quickly compare altmetric data sources has value to researchers. By removing the smooth animation, it would be possible to retain this aspect of the visualisation. Alternatively, the project could implement a interpolation model that better represents altmetrics research. Unfortunately, this research is not yet complete - of the main goals of this project is to encourage this area of study. When complete, the application could easily be updated to use this model as it is released under an open source license.</p>
<p>Overall, evaluation of the visualisation implementation finds mixed results. The usability test found that there is some underlying value in the visualisation, that must be brought forward. Altmetrics visualisations are still rare, especially one that can be dynamically changed for comparison purposes. As discussed, the interpolation issue drastically reduces the usefulness of the application, however there are potential strategies to mitigate this. The open source nature of the project means that making such changes is relatively easy.</p>
<h6 id="5-3-2-4-storage">5.3.2.4 Storage</h6>
<p>After altmetrics data is retrieved by the data collection module, it is stored in a MongoDB database (see section 4.5). The database stores documents in a JSON-like format, very similar to the format received by the data collection module. The application enforces a lightweight structure for the database, using the Mongoose library. When the data is stored, a unique key is generated, which is used to create a permalink for the data. This can be visited later by the user without having to retrieve data for a second time.</p>
<p>As discussed in section 4.5, the proposed solution for this implementation did not require a structured database. The requirement states that data must simply be stored against a unique key for retrieval later. A defined structure is not required, as data would be treated as a single entity. However, this changed as the implementation progressed. The Mongoose library was found to provide a good interface to MongoDB, while also allowing the creation of a simple database schema. This may prove useful in future development of the project, as aspects of the data can be queried separately, as opposed to key/value store. This implementation choice can be considered a good, forward thinking decision, as it meets the requirements of the system while allowing for further extension.</p>
<h6 id="5-3-2-5-web-application">5.3.2.5 Web Application</h6>
<p>The parts of the application described above need to be combined and controlled to provide an interface between them and the user. This is provided by the web application, controlling the flow of data between the individual modules and the web pages that allow the user to interact with the system. The implementation is constructed on the Node.js platform, using the Express web application framework. The application provides a system of routes and controllers that listen for HTTP requests and respond accordingly. Web pages are served on appropriate endpoints, which are constructed from templates so that they contain the correct information.</p>
<p>The web site was constructed with a progressive enhancement approach, where pages are constructed in a layered fashion. The server implementation therefore had to handle requests from &quot;regular&quot; HTTP requests and asynchronous requests (via XMLHttpRequest). These were given separate routes as they could potentially return data in different formats. Requests to the <code>/api</code> routes always return data in JSON format. This means that the application can very effectively take a progressive enhancement approach. The Express framework makes this relatively easy. The one exception to this rule is the visualisation page, which requires Javascript and SVG support. Unfortunately, D3.js depends on being able to use these relatively new features of the web, and so a true progressive enhancement approach could not be taken. It is possible that, given more time, an alternative view of the altmetrics data could be created, however this was considered mostly out of scope.</p>
<ul>
<li>Web application<ul>
<li>Async<ul>
<li>Difficult</li>
<li>&quot;Pyramid of doom&quot;</li>
</ul>
</li>
<li>Accessibility</li>
<li>Search page<ul>
<li>localStorage can be confusing<ul>
<li>Especially when going back and searching for different articles once some articles are selected<ul>
<li>On second search, it would appear that previously searched articles would be returned by the search results, when they&#39;re not they are just appended because they are stored</li>
</ul>
</li>
<li>Usability test</li>
<li>Users not familiar with localStorage concepts - usually have to submit something for storage to take place</li>
<li>Should have split stored articles into a separate area - less confusing</li>
</ul>
</li>
<li>Could be improved by having visualisation build on side of page, when articles are selected<ul>
<li>Then change submit button to save button, to save data to db</li>
<li>Improves on confusing UI</li>
</ul>
</li>
<li>Search results page<ul>
<li>Search results not paginated<ul>
<li>What happens if you put in something really generic &amp; get lots of results</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Styles<ul>
<li>Bootstrap</li>
</ul>
</li>
<li>Overall a reasonable implementation</li>
</ul>
</li>
<li>More time required<ul>
<li>Not fully baked</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="5-4-testing-and-evaluation-summary">5.4 Testing and Evaluation Summary</h4>
<p>This chapter introduced ... . In Section x, a series of tests were described which demonstrated ...</p>
<p>An evaluation of ... was then presented in Section y. Section z revisited the requirements described in Chapter 3 and identified that ... . Finally in Section a, the aspects of ... were discussed.<br></p>
<h3 id="chapter-6-conclusion">Chapter 6 Conclusion</h3>
<h4 id="6-1-introduction">6.1 Introduction</h4>
<p>In this Chapter, we first summarise the work described in this report (Section x). Then we draw a number of conclusions about key parts of the work undertaken in Section y, and finally in Section z we discuss future work and how we see ... helping support projects such as this one.</p>
<p><br></p>
<h4 id="6-2-summary">6.2 Summary</h4>
<!-- This is a summary of each chapter intro and summary -->

<p>Chapter 1 introduced ...</p>
<p>Chapter 2 reviewed the state-of-the-art in ... . ... Was introduced and ... described. The potential for ... was highlighted.</p>
<p>Chapter 3 describes the design of ... . The separate functions of ... that support the requirements were then described in more detail, including ...</p>
<p>Chapter 4 described the implementation ...</p>
<p>Chapter 5 presented a series of test that demonstrate ...</p>
<p><br></p>
<h4 id="6-3-conclusions">6.3 Conclusions</h4>
<p>The aim of this project was to ... . We chose to focus on ... </p>
<p>We then designed and implemented a system that could:</p>
<ul>
<li>foo</li>
<li>bar</li>
<li>baz</li>
</ul>
<p>These combined capabilites ...</p>
<p>In Chapter 1 we state the general hypothesis that ... . We have tested this thesis by ...</p>
<h5 id="6-3-1-key-points">6.3.1 Key Points</h5>
<!-- Discuss future work as you go -->

<p><strong>First key point</strong></p>
<p><strong>Second key point</strong></p>
<ul>
<li>Visualisation<ul>
<li>Further work<ul>
<li>Interpolation<ul>
<li>Improving the model used to calculate interpolated values</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h3 id="chapter-7-references">Chapter 7 References</h3>
<!-- Boakes R J, Thesis Template (delete this and replace with your own). -->
<!-- Remember: Harvard APA 6th ed. -->

<p><br></p>
<h3 id="appendices">Appendices</h3>
<ul>
<li>Screencast/video of visualisation?</li>
</ul>
<p><strong>Something interesting</strong></p>
<p><strong>Something else</strong></p>

  </body>
</html>
