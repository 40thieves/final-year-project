#### 2.5 Literature Review Conclusion

Altmetrics provides a unique opportunity to broaden the scope and reach of the field of scientific impact, in a much expedited and transparent fashion. Furthermore, several studies have shown that altmetrics correlates with traditional citation counts, thereby validating their use. Taylor (2013), says "the potential for what we currently call altmetrics is nothing short of a complete map of scholarly activity and influence".

As scholarly usage of the web and social media increases, more communication between scholars will take place on mediums that we can measure using altmetrics. As discussed in the Case for Altmetrics section, Priem, Piwowar & Hemminger (2012) believe that these conversations represent those that have been long held in the hallways and conferences, and that altmetrics gives us the opportunity to examine impact generated here. They go on to speculate that as scholarly usage of web tools grows, coverage of data sources will improve. The growth of this online communication would also bring more open APIs and access to altmetrics data.

Altmetrics are more open and transparent than traditional metrics, bringing bibliometrics up to the standards set by the rest of science. Peer review of research is a powerful and vital step in scholarly publication, whereas traditional bibliometrics are surprisingly lacking at publishing their methods. This unscientific approach leads to arbitrary and unfair results. The altmetrics community encourages openness, with several existing providers publishing their codebases under open source licenses. Chamberlain (2013) believes that methods of calculation "should be very clear and accessible", allowing peers to check these calculations. Transparency among altmetrics may also lead to greater openness in the wider research community. Open access is a publishing model where readers do not have to pay for access to scholarly materials, that has grown in popularity recently with several new journals publishing using this model. Galligan & Dyas-Correia (2013), citing Curry (2012), claim that "the impact factor [is] one of the main culprits in the creation of a roadblock to open access". He argues that the impact factor's bias towards prestigious high impact journals prevents newer open access journals from gaining traction. By using a more diverse range of metrics, altmetrics can provide an alternative view of these important publishing models.

Priem, Piwowar & Hemminger (2012) state that "metrics must move beyond simply reporting counts", moving to a more nuanced lens with which to view impact. The somewhat meaningless number produced by the impact factor can only be put in context by other impact factor values. Several altmetrics practitioners have warned against a move to simplify or "normalise" altmetrics into a single "altmetrics factor". Chamberlain (2013) says "combining article-level metrics into a single score defeats one of the advantages of article-level metrics over the traditional journal impact factor". He believes that for altmetrics to "avoid the pitfalls of the Journal Impact Factor", they should be "important to different stakeholders" and "retain their context". Galligan & Dyas-Correia (2013) believe that "the tendency to desire one single score to evaluate research is one of laziness", preventing evaluators from gaining the full analysis of impact.

The same argument can be applied to any individual data source within altmetrics, however, this data source cannot be isolated without losing context. Telling the "story" of how impact was created is more valuable than raw numbers. This is especially true when looking at research products other than the scholarly article - for example, re-use of a dataset may continue well after publication of an associated paper, giving more scope for a "story" that fully describes the it's influence.

However, as the list of altmetrics data sources grows, analysis of this data would become complex and opaque. For this reason, categorisation of data sources into groups that have inherent properties is required. Lin & Fenner (2013) attempt to describe an ontology for altmetrics that would "establish thoughtful and meaningful ways of grouping similar altmetrics together and distinguishing them from other altmetrics with different meaning". The altmetrics provider ImpactStory presents users with a set of "tags" describing an article's impact according to the raw data sources. For example, a paper with a high number of Twitter citations would receive the "Highly discussed by the public" tag. The categorisation provides context, but allows for future data sources with similar properties to Twitter citations to be added. This approach allows a easy to understand, but nuanced view of the impact.

Altmetrics has the potential to uncover impact gained through non-traditional paths, such as those outside of science and in wider society. Currently there is no incentive within the impact assessment structure for outreach - explaining science to the general public through a variety of mediums. And thus, outreach is dominated by a few television stars, and by niche bloggers. However, a much wider range of outreach takes place, the PLoS Medicine Editors (2006) report that "Magazine sections, such as those that we and other medical journals publish, ... 'add value' to the research articles by interpreting them for a wider audience". Altmetrics, enables measurement of impact from these mediums, for the first time giving value to the hugely important task of outreach. Taylor (2013) states that "the increasing strength of altmetrics will be to increase the detail and scope of the description of research in society". As researchers can demonstrate to assessors the impact of outreach on society, rewards will increase. This incentivises outreach, and provides competition, potentially improving the quality of scientific outreach.

As discussed in the History of Bibliometrics section, the growing flood of research is overwhelming scientists who cannot keep up with the volume of published work. Neylon & Wu (2009) report on possible strategies to combat this: "Our only options are to publish less or to filter more effectively, and any response that favours publishing less doesn't make sense, either logistically, financially, or ethically". This is the ultimate purpose of bibliometrics, to filter all possible information to find valuable and trustworthy information. As Clay Shirky (2008) famously said at the Web 2.0 Expo, "it's not information overload, it's filter failure". The impact factor and other traditional metrics calculate article impact using citation counts to test against these criteria. Altmetrics simply offer an alternative filter, that can give a different perspective on impact, and thus on the filter they provide.

The purpose of this review is to present altmetrics as an alternative to the impact factor, however one cannot conclude that altmetrics are a wholesale replacement for traditional bibliometrics. They still offer a useful filter for research, and in some cases citation counts are included in altmetrics services. This is especially true for cases where trustworthy information is required, as it is unlikely that an article with high citation counts is scientifically unsound. This is the basis of the hierarchy of altmetrics categories discussed in the What is Altmetrics section, where data sources that involve a higher level of engagement - such as citing in an academic paper - are expected to create more impact.

Ultimately, altmetrics cannot be used as the definitive answer to the question of impact assessment. Qualitative assessment of the work must be combined with the quantitative data provided by altmetrics. Liu & Adie (2013) believe that "users must frame appropriate questions and decide what information they want the altmetrics data to provide". To succeed, altmetrics must broaden the scope of bibliometrics, provide data in a more timely manner and provide context for itself.

##### 2.5.1 Further Research

Altmetrics is still a field in it's infancy, with much work to done in various areas. Priem, Piwowar & Hemminger (2012) comment that much work is required around "reducing noise that obscures the impact signal", that is "crucial to understand what the events informing alternative metrics actually mean".

One area that is of great interest is a further exploration of altmetrics categorisation. Priem, Piwowar & Hemminger's (2012) seminal work in this area requires more research to isolate and identify different "different types of impacts on different audiences". Galligan & Dyas-Correia (2013) also believe that work on "determining meaningful clusters of metrics for particular groups" is needed. They believe that certain categories can be more relevant to different assessors - some metrics may be more suited for librarians, or for the general public, while others are more relevant to researchers within the field.

Another area that has potential is the use of network effects in determining impact with less noise. Priem, Piwowar & Hemminger (2012) also suggest research into this area: "a tweet from a highly-connected, expert scholar should mean something different from one authored by a casual observer". Bollen, Rodriquez & Van de Sompel (2009) apply this principle to the impact factor, weighting a citation by the prestige of the journal it was published in, finding improved results. This principle could be applied to altmetrics as a way to further reduce the influence of bad actors and noise.

As discussed in the Case for Altmetrics section, altmetrics gives us the opportunity to measure impact for a wider range of scholarly output. Of particular interest are datasets and software, which several recent efforts are looking to investigate for potential impact measurement. Priem, Piwowar & Hemminger (2012) believe this, stating that "investigation should also expand to examine altmetrics for scholarly products other than articles". In a recent announcement, the altmetrics provider ImpactStory, published their implementation of code impact through the collaborative software hosting service, GitHub. This means that re-use of software can be tracked and analysed for impact.

In most studies, there have been two periods of time covered by the research; between immediately after publication up and about a week after to publication; or over a long period, usually several years, after publication. This is usually performed on articles several years old, to give time for traditional citations to build up. The altmetrics data is then validated against the citation counts. The former approach is too short to consider trends in how the altmetrics data changes over time, and the latter approach also fails to analyse the rate of change, treating data as cumulative over the entire time period. This leaves an interesting area of study open - how do changes in altmetrics data over a period of time affect impact at a later stage. For example, if an article gains 50 Twitter citations within a week of publication, will this result in a higher number of traditional citations than if it gained 50 Twitter citations within a day of publication? Research in this area would provide additional context for altmetrics, again providing a new perspective on impact.

To approach this subject, tools are required to help kick-start research. As discussed, altmetrics can generate large amounts of data that can be difficult to analyse, and therefore visualisation tools are needed to assist researchers. This also true if altmetrics are to be used for qualitative assessment, not just for quantitative assessment, as visual aides help to understand the "story" of impact.

* Exception to "little or no study of altmetrics over time"
	* Shuai, Pepe & Bollen (2012) studied the usage of Twitter by scholars over time after a new paper has been released
		* Limited to only a few days after publication and only to Twitter as a data source
	* Also Eysenbach (2011)
		* Again limited to short time after publication & only Twitter
	* Yan & Gerstein (2011) show "long tail" distribution of article views/downloads
