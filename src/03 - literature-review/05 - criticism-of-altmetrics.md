<div class="page-break-avoid">

#### 2.5 Criticism of Altmetrics

Not all believe that altmetrics are a viable solution to the bibliometrics problem. They point to problems with data sources that can be misleading or to the ease with which altmetrics can be "gamed". These need to be addressed by the altmetrics community before they are accepted by the wider scientific institution.

</div>

<div class="page-break-avoid">

##### 2.5.1 Inadequate Data Sources

Several papers have criticising specific altmetric data sources, claiming that they are inherently poor statistics for measuring impact. Taraborelli (2008), when studying usage metrics, such as article views, states that "it is debatable whether [altmetrics] will be able to overcome the major issues that afflicted search engine research over the last decade". These early search engines "relied on raw traffic data", which is vulnerable to spam or "gaming", where one attempts to boost views maliciously. Taraborelli draws similarities between scholarly article views and this raw traffic data. However, Taraborelli goes on to offer a potential solution, again drawing from the experience of early web search engines, where "raw traffic data [was abandoned] in favour of more accurate, scalable and spam-resistant criteria for quality assessment". This points to an exciting new area of altmetrics that, to some extent, has gone unexplored - the usage of network effects to rank influencers and thus weight individual citations. Neylon & Wu (2009) also find that article views and downloads are a "crude measure of actual use". They criticise the approach, stating that counting "how many people clicked on the download button thinking they 'might read it later'" is not a measure of "how much influence an article has".

</div>

Another data source that has received criticism is the social network, Twitter, with some studies finding that supposed scholarly tweets were in fact advertisements. Desai, Shariff & Shariff (2012) studied tweets related to an academic conference, finding that "a large percentage of tweets were advertisements". They conclude that the "use of Twitter as a communication tool challenging as advertisers can misuse it under the disguise of education".

Eysenbach (2011) finds that directly comparing Twitter citation rates (or "tweetations") for two unrelated articles is problematic. The paper finds that "number of tweetations is a function of time since publication", and as such two papers published at different times would produce different Twitter citation rates. He also reports that a comparison of the "twimpact factor" of two articles "would not be legitimate", giving an example of an article on social media gaining more Twitter citations than an article on molecular biology. However it could be countered that comparing two fields as in the example is rare and difficult even using more established tools. In addition, this argument does not cover the hierarchy of altmetrics, which may reflect the likelihood that the molecular biology paper will be cited more by scholars than the social media article.

<div class="page-break-avoid">

##### 2.5.2 Correlation with Citations Disputed

Evidence that altmetrics correlates with traditional forms of impact measurement are disputed by some. Thelwall, Haustein, Lariviere et al. (2013) find that several data sources "may only be useful to identify the occasional exceptional or above average article rather than as universal sources of evidence", and Priem, Piwowar & Hemminger (2012) point to further studies that find "weak to moderate correlation" with social reference managers, and weak correlation with tweets.

</div>

However, Thelwall, Haustein, Lariviere et al. (2013) further find that the low correlation rate may be affected by the time of publication. Priem, Piwowar & Hemminger (2012) claim that "altmetric indicators seem mostly orthogonal to citation", suggesting that altmetrics reflects a different form of impact to traditional citations. Eyre-Walker & Stoletzki (2013) found poor correlation between citation counts and Faculty of 1000 (F1000) scores. They report that "scientists are poor at estimating the merit of a scientific publication", in terms citation counts, finding articles with high F1000 scores that had low citation counts.

<div class="page-break-avoid">

##### 2.5.3 Low Coverage

Coverage, in this context, is the statistic that defines how well an article is represented by the altmetric data sources that cite it. For example, if an article has very few or no citations for a given data source, then it is considered to have low coverage. This affects the likelihood that the altmetric value accurately represents the true impact of the article. Studies have found that, for some papers and some data sources, coverage is low. Thelwall, Haustein, Lariviere et al (2013) report that, for their sample, "the coverage of the altmetrics, and particularly [data sources] other than Twitter, may be low". They find many instances where articles had no altmetric data associated with them. This may be explained by Priem, Piwowar & Hemminger (2012), who find that "scholarly use of social media [is] relatively rare". As discussed in section 2.3, scholar's usage of web-based tools is growing rapidly, likely increasing the coverage of many articles.

</div>

<div class="page-break-avoid">

##### 2.5.4 Bias

Some have criticised altmetrics, claiming that altmetrics have some inherent bias. Priem & Hemminger (2010) believe that "users of social software probably skew younger, and from more technical and scientific disciplines". They further claim that a positive feedback loop can be created that is dangerous to measuring the true impact, where "more popular items attract more attention, increasing their popularity still further". Others believe that altmetrics is too easily affected by "trendy" topics, that are popular in the moment (Eysenbach, 2011).

</div>

This younger bias also extends to papers, with more recent articles garnering more attention than older articles (Gunn, 2013). As discussed above, Thelwall, Haustein, Lariviere et al. (2013) suggest that older articles are "compensated for lower altmetric scores". Others find problems with altmetric's bias toward articles with high traditional citation metrics. Groth & Gurney (2010) find that scholarly blogs focus on papers published in journals with high impact factor. Shema, Bar-Ilan & Thelwall (2012) call this the "rich-get-richer phenomenon", although they concede that this may be due to the influence of mainstream media leading academic bloggers to "offer their own analysis and interpretation" of popular stories. Eyre-Walker & Stoletzki find a similar bias amongst F1000 reviewers, giving "higher scores to papers in high impact factor journals, independent of merit". It is possible that altmetrics may be able to avoid this bias as more scholarly processes come online, and thus there is more low level "scientific street cred", as discussed in section 2.4.2.

<div class="page-break-avoid">

##### 2.5.5 Gaming

"Gaming" of altmetrics is a source of much criticism, with critics claiming that it is easier to falsely boost an article's apparent impact maliciously, and thus cause problems for those assessing impact. Beall (2013) imagines a world where page views are "shamelessly gamed" by low wage workers hired to "reload web pages thousands of times". Beall goes on to criticise analysis of Twitter, claiming that researchers will "pay companies to add bogus followers to their social media accounts, and these bogus followers will like and share their articles, actions that will be counted as part of the metrics". A similar conclusion is levelled at Google Scholar citation counts - not traditionally included in altmetrics, but the principle applies - by Delgado López-Cózar, Robinson-García & Torres-Salinas (2013). For this study, they published 6 fake papers to investigate whether they could "game" the Google Scholar citation counts. They find Google Scholar's main shortcoming is "the ease with which they can be used to manipulate citation counting". However, it could be argued that the publication of 6 fake papers without anyone noticing is a failing of the scientific system, rather than solely Google Scholar's.

</div>

Some in the altmetrics community have answered this criticism by looking to other fields vulnerable to gaming, and adopting their strategies for controlling gaming or limiting its influence. Priem & Hemminger (2010) report that "history suggests that while gaming social metrics may not be solved, it can be controlled". They cite several sources from the search engine and social network fields related to reducing spam and bad actors within their systems. Ntoulas et al. (2006) reports that "advertisers have assaulted Google search results with 'black–hat SEO'", similar to impact gaming, and yet search results remain mostly free from these advertisers. Yardi et al (2009) report on their efforts to detect spam in a Twitter network, finding "the existence of structural network differences between spam accounts and legitimate users". This suggests that by using the vast amount of data available to altmetrics practitioners, it is possible to detect and isolate gaming attempts within altmetrics.

<div class="page-break-avoid">

##### 2.5.6 Involving the Public

There are some who have voiced concern that by capturing impact from sources that are not peer-reviewed, and are dominated by the general public, that scientific rigour will be lost. Taraborelli (2008) comments that social reference managers cannot offer the "same guarantees" as expert peer review, and "are less immune to biases and manipulations". Beall (2013) claims that "the general public lacks the credentials needed to judge or influence the impact of scientific work, and any metric that relies even a little bit on public input will prove invalid". Liu & Adie (2013) claim that "such concerns are understandable, especially when one examines some of the trending articles that have garnered extremely high scores of online attention", giving examples of articles that are "humorous, unusual, or even fictitious in nature".

</div>

This argument misses a great benefit of altmetrics, the ability to discover alternative forms of impact, beyond pure science. If all public influence is rejected, science and its impact would become insular, a move that has grave consequences for the relevance of science to the general public. Additionally, many altmetric practitioners believe that the data can be adjusted or weighted in favour of academic influence, protecting science from the "damaging" influence of the public.

<div class="page-break-avoid">

##### 2.5.7 Disagreement About Standardisation

As discussed in section 2.3.9, altmetric data sources can be categorised, giving a framework for standardisation of data sources. However, not all within the altmetrics field feel that standardisation is warranted currently. Mulvany (2013) notes that during the 2013 NISO ALM workshop, much of the "discussion is whether this is the right time to [standardise]". He also notes that two of the people who voiced dissension against standardisation represented altmetric services, ImpactStory and Plumb Analytics. They believe that "standardization would cause calcification", also citing that their customers had not asked for it.

</div>

Liu & Adie (2013) also note that "there has been no clear consensus on which data sources are most important to measure", meaning that some data sources and methods have not been "systematically validated". The lack of this validation may have a large effect on the consistency of data between different altmetrics practitioners. Chamberlain (2013) notes that "when similar data sources are collected by article-level metrics providers, ideally, there should be a way to [compare] data". A comparison can only take place if provenance of the data and methods used to calculate a metric for the data are provided by the practitioner. The only existing provider that does both is ImpactStory, by providing a `provenance_url` on each data source, and by publishing their code under an open license.

<div class="page-break-avoid">

##### 2.5.8 Technical Issues with Data Sources

Chamberlain (2013) comments that "Twitter data is notorious for not being persistent". Twitter's API is rate limited, its search functionality is inadequate and it is difficult to retrieve tweets older than 30 days. He reports two solutions: "either have to query the Twitter 'firehose' constantly and store data, or go through a company like Topsy (which collects Twitter data and charges customers for access) to collect tweets". Existing altmetrics providers take both approaches. This makes Twitter data very difficult to track over long periods of time, and is one of the main reasons behind the push for standardisation, as described above. He also finds other data sources, such as Google Scholar to be "totally inaccessible". Liu & Adie (2013) find problems in tracking impact, finding that "technical limitations currently prevent the tracking of certain sources, such as multimedia files".

</div>

Some existing altmetrics providers do not provide historic data - a breakdown of how metrics change over time - which is problematic for some analyses. ImpactStory and Plum Analytics only provide metrics that show the current total for each data source. Altmetric.com provides publicly available historical data over given intervals, while the PLOS ALM API provides full historical data, but only for some of their metrics. Chamberlain (2013) believes that "as more [sources] are tracked, historical data will become expensive to store, so perhaps won’t be emphasized by article-level metrics providers". However, this would be detrimental to altmetrics, as the subject of how impact changes over time is somewhat unexplored currently.

These problems make finding metrics from these data sources difficult, but also makes finding provenance difficult. Twitter's API has undergone several changes since its original release in 2006, and in particular deprecating its older API URL structures. This has resulted in broken links in altmetrics services, where provenance cannot be accurately tracked.

<div class="page-break-avoid">

##### 2.5.9 Second Order Citations

A long standing problem in the altmetrics research community is that second order citations occur when credit for a citation is buried underneath another citation. For example, if someone tweeted a link to a blog post that, in turn, contains a citation to an article, only one citation will be found using current altmetrics techniques. The tweet is considered a second order citation, as it is citing a third-party rather than directly citing the article. Unfortunately, there has been little study of the scale of this problem, but it seems intuitive to assume that many altmetric citations are occurring in this manner. Few people would go out of their way to find and include the first order citation when discussing an article online in a casual environment such as a social network.

</div>

This problem is mitigated, to some extent, by the vast amount of data that altmetrics can generate. The "wisdom of the crowds" can be leveraged to discover some fraction of the total impact. However, it is difficult currently to speculate whether it is feasible to project this data up to a true impact. This effect is also felt equally across all types of articles, and therefore comparisons between articles using altmetric data are still valid.

