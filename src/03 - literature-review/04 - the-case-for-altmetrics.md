#### 2.3 The Case for Altmetrics

There are a multitude of reasons for believing that altmetrics offer a viable option when measuring scholarly impact. The capability of altmetrics to measure more diverse forms of impact, with less delay, more transparently and at enormous scales is exciting for those looking to improve scholarly impact measurement. Priem & Hemminger (2010) find that "altmetrics take advantage of the pervasiveness and importance of new tools (Web pages, search engines, e–journals) to inform broader, faster, and more open metrics of impact". New data sources allow altmetrics practitioners to explore the underlying properties of an article to "measure the distinct concept of social impact" (Eysenbach; 2011).

##### 2.3.1 Correlation with Citations

Many altmetrics studies have focussed on correlating various data sources with traditional citations as a way of validating their results. Thelwall, Haustein, Lariviere et al. (2013) attempted to correlate 11 different data sources with Web of Science citation counts. They created a simple sign test whether an article's citations match altmetrics values - if both were higher than the average for articles published at a similar time then the data source was considered successful. They found "clear evidence" that data sources for blogs, Facebook and Twitter show strong correlation with citations - "the success rate of the altmetrics at associating with higher citation significantly exceeded the failure rate at the individual article level". They found that an additional three data sources correlate with citations, although the correlation is weaker.

Another study found a "moderately strong relationships between citation count and pdf/html download count", and that "Mendeley and CiteULike displayed the highest correlations to Web of Science counts". From their original sample, they created a time-restricted sample of recent (2011) papers, finding that correlations between Mendeley and Web of Science citation count "rivaled or surpassed those of Scopus, PubMed, and CrossRef citations" (Priem, Piwowar, Hemminger, 2012).

Some studies focussed specifically on a single data source. Nielsen (2007) finds that the number of links to research papers in Wikipedia correlates with the citation count found in the Journal Citation Report (from which the impact factor is calculated). Similarly, Eysenbach (2011) finds a statistically significant, although weak, correlation between Twitter citations and traditional citation counts.

##### 2.3.2 Capturing the Unseen Conversations

Altmetrics have the potential to capture previously hidden scholarly communications. Conversations between academics in the hallways at their institutions or at conferences are important methods for discussing new and interesting ideas. These discussions have huge value in determining the course of future science, and thus are valuable sources of impact. As academia increasingly moves online, it seems logical that these conversations will also move online. Altmetrics gives us the opportunity to measure this impact.

Priem, Piwowar & Hemminger (2012) claim that "[Blogging and Twitter] facilitate the sort of informal conference chats that have long vivified the academy’s invisible colleges". They also point out that this form of impact measurement "[facilitates] existing practice", instead of forcing new behaviour.

##### 2.3.3 Diversity

One of the biggest benefits of altmetrics is impact discovery from a much broader and diverse set of data sources. Fenner (2013) describes scientific impact as "multi-dimensional construct that can not be adequately measured by any single indicator", supporting altmetric's model of many diverse data sources. By using a wider variety of sources, the impact reflects the real-world "diverse scholarly ecosystem" more accurately ("altmetrics: a manifesto", 2010). Gibson (2013) examined the altmetric data sources used by the Journal of Ecology finding that "each metric reflected a different form of reader usage".

As discussed in the History of Bibliometrics section, the Impact Factor fails to measure the general public's interaction with science. Altmetrics, due to their inherent diversity, allow us to measure this interaction. This can be achieved by data from sources that are used by people outside of science, such as social networks (Priem, Piwowar & Hemminger, 2012). In addition, other under-represented or niche groups can be looked at to discover impact. Thelwell, Haustein & Lariviere (2013) studied the validity of altmetrics using Twitter as a primary data source, by attempting to correlate Twitter metrics with citation counts. They comment that this may be limiting to the scope of altmetrics, where it can capture the "influence of scholarly publications on a wider and different section of their readership than citation counts".

The benefits of this wider group extend to other areas of scholarship, such humanities and social science. As discussed in the History of Bibliometrics section, these fields miss out on the impact factor as they rely on alternative forms of publication to the journal. Altmetrics gives us the opportunity to fix this imbalance (Roemer & Borchardt, 2013).

The range of altmetrics can be adapted to multiple different purposes and contexts (Neylon & Wu, 2009), allowing us to view impact in novel ways. For example, Priem, Piwowar & Hemminger (2012) propose a set of "impact flavours", that can be used to describe the impact of papers. This concept and similar have evolved to the categories of data sources discussed in the previous section. Using these categorisations, and applying them to papers - i.e. papers that have relatively high metrics from one category are, by implication, similar to other papers with high metrics in the category - we can view the impact through the lens of these categories. This allows us to look at metrics more nuanced and encompassing way. As Priem, Piwowar & Hemminger (2012) note, "the goal is not to compare flavors: one flavor is not objectively better than another", which leads to the recognition that "different types of contributions might help us appreciate scholarly products for the particular needs they meet".

Altmetrics can also be used to generate a more semantic view of influence, by looking at the entire system of sharing and re-use ("altmetrics: a manifesto", 2010). Google's PageRank system famously ranks pages on the web by considering which other web pages link to the page in question. Links from trusted sites hold more weight, therefore a page containing a link from these sites is given a higher ranking. This concept can be applied to altmetrics - by considering who or what cites a scholarly article, a weighting can be applied to that citation that will be reflected the article's impact. Bollen, Van de Sompel, Hagberg et al. (2009) studied this concept using a "clickstream" model to investigate how scientific publications are accessed. A map of how users clicked to move through the system was collected, and modelled. They drew few conclusions on it's validity as a influence measure, although it was found that "[there is a] distinction between citing behaviour and online information seeking behaviour", implying that altmetrics can discover impact that is unseen by traditional citation metrics.

##### 2.3.4 Speed

As discussed in the Failings of Traditional Metrics section, citation analysis can take several months if not years to accumulate. Altmetrics analysis, on the other hand, can gather data in "days or weeks" ("altmetrics: a manifesto", 2010). By leveraging the power of open APIs, altmetric data sources can be queried immediately after a paper has been published (Chamberlain, 2013). For example, tweets usually occur very soon after publication, almost as the paper is read and shared by other academics. Priem & Light Costello (2011) find that, for Twitter citations "39% ... refer to articles less than one week old, and 15% of citing tweets refer to articles published that same day". Groth & Gurney (2010) comment that one of blogging's "major strengths" is "the ability to provide instantaneous commentary on a subject with simultaneous feedback on their own content". They also comment that blogs allow discussion of older papers, putting current papers into context and reintroducing older ideas.

This much shorter delay between publication and citation presents a great opportunity to altmetrics practitioners, enabling novel uses for altmetrics data. The Altmetrics manifesto (2010) claims that it allows the opportunity to develop "real-time recommendation and collaborative filtering systems". This would add an additional layer to existing review systems within science, cutting the overwhelming and increasing volume of papers published yearly, and allowing researchers to focus on the most important papers in their field.

##### 2.3.5 Transparency

Altmetrics foster a culture of openness that is missing from traditional bibliometrics, especially the impact factor. This approach has been adopted from the beginning, with the Altmetrics manifesto (2010) calling for openness in "not just the data, but the scripts and algorithms that collect and interpret it". 

Most, if not all, altmetric data sources have open or permissive licenses, allowing free re-use, usually with an API key. This is extremely beneficial to the measurement of scientific impact, as it allows the process to follow practices and values long held in science. Open data means that anyone can access and verify the results of a given altmetric system. Chamberlain (2013) concludes that "if data sources are open, conclusions based on article-level metrics can be verified by others and tools can be built on top of the article-level metrics".

Some altmetric practitioners have extended this openness to their code. For example, the altmetric data gathering and analysis tool, ImpactStory, makes their entire code-base available under a MIT license. This allows anyone to use and extend the code without having to obtain a license. The methods by which they calculate altmetric scores can be viewed by anyone to verify that they are correct. This approach is very similar to the generalised scientific method of publishing your work openly for anyone to inspect and critique.

##### 2.3.6 Captures More of the Author's Work

As discussed in the Failings of Traditional Metrics section, increasingly a paper is not the only output from a research project. Altmetrics allows us to find influence in these other forms of output. ImpactStory recently started to track Github repositories to enable researchers to view altmetrics data for their code ("Uncovering the impact of software", 2013). ImpactStory have additionally implemented features where open data hosted on the scientific data sharing platform, figshare, can be analysed for altmetrics data ("figshare and altmetrics.", 2012).

##### 2.3.7 Web Scale

Due to the primarily web-based data sources, altmetrics operates at the very large web scale. Whereas traditional citations counts were rarely found above a hundred, some altmetrics can be regularly found on the order of thousands. As these numbers rise, the statistical likelyhood that they reflect real-world impact also rises. Any single altmetric citation may be uninteresting, or even erroneous, but the vast amount of data balances this reflecting the "wisdom of the crowds", a concept detailed by Surowiecki (2005). Taraborelli (2008) comments that individually metadata is "hardly of any interest, but at a large scale metrics based on these metadata are likely to outperform more traditional evaluation processes in terms of coverage, speed and efficiency".

Neylon & Wu (2009) compare the large scale of altmetrics to the online advertising industry, stating that altmetrics "may not be completely accurate but they are consistent, comparable, and considered sufficiently immune to cheating to be the basis for a billion dollar Web advertising industry".

##### 2.3.8 Complimentary to Traditional Metrics

Finally, it must be noted that altmetrics in no way replaces traditional forms of impact measurement. They can provide additional context or data when making decisions based on impact. Despite it's failings the impact factor still has value in finding impact within the scientific community. Altmetrics gives us the opportunity to extend this beyond science, for example boosting the impact profile of a research who spends more time performing public outreach through a blog or Twitter account.

McKiernan (2004) notes that usage-based metrics (for example, page views) are increasingly perceived as necessary to complement to traditional peer review as an indicator of scientific significance. This is supported by Priem, Piwowar & Hemminger (2012) who speculate that in the future altmetrics and more traditional impact measurements could be presented together as "complementary tools presenting a nuanced, multidimensional view of multiple research impacts at multiple time scales".

* Not relying on a single number
	* More nuanced
	* Reflects the complex system of scientific publishing
	* "With altmetrics, there is a sense that the users themselves should articulate how the measurements should be applied to specific problems, rather than dictated by an organization that thinks it knows best. This is the main drawback of all altmetric tools - there is as yet no simple way to interpret the data and give clear meaning" (Altmetrics: Rethinking the Way We Measure; Galligan & Dyas-Correia; 2013)

